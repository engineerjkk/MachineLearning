{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def load_data(path, dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test=load_data(path=\"./cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
      "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
      "       values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
      "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 0,  ..., 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])\n",
      "tensor([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
      "        256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
      "        396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
      "        410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
      "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
      "        438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
      "        452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
      "        466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479,\n",
      "        480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "        494, 495, 496, 497, 498, 499])\n",
      "tensor([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,  510,  511,\n",
      "         512,  513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
      "         524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,  535,\n",
      "         536,  537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
      "         548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,  559,\n",
      "         560,  561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
      "         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
      "         584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
      "         596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
      "         608,  609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
      "         620,  621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
      "         632,  633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
      "         644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
      "         656,  657,  658,  659,  660,  661,  662,  663,  664,  665,  666,  667,\n",
      "         668,  669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
      "         680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,  691,\n",
      "         692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
      "         704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
      "         716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
      "         728,  729,  730,  731,  732,  733,  734,  735,  736,  737,  738,  739,\n",
      "         740,  741,  742,  743,  744,  745,  746,  747,  748,  749,  750,  751,\n",
      "         752,  753,  754,  755,  756,  757,  758,  759,  760,  761,  762,  763,\n",
      "         764,  765,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
      "         776,  777,  778,  779,  780,  781,  782,  783,  784,  785,  786,  787,\n",
      "         788,  789,  790,  791,  792,  793,  794,  795,  796,  797,  798,  799,\n",
      "         800,  801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
      "         812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
      "         824,  825,  826,  827,  828,  829,  830,  831,  832,  833,  834,  835,\n",
      "         836,  837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "         848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
      "         860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,  871,\n",
      "         872,  873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "         884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
      "         896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
      "         908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
      "         920,  921,  922,  923,  924,  925,  926,  927,  928,  929,  930,  931,\n",
      "         932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
      "         944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,  955,\n",
      "         956,  957,  958,  959,  960,  961,  962,  963,  964,  965,  966,  967,\n",
      "         968,  969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "         980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,  991,\n",
      "         992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
      "        1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "        1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
      "        1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039,\n",
      "        1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
      "        1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
      "        1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "        1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087,\n",
      "        1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
      "        1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111,\n",
      "        1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
      "        1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
      "        1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "        1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
      "        1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171,\n",
      "        1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183,\n",
      "        1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
      "        1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
      "        1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219,\n",
      "        1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231,\n",
      "        1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243,\n",
      "        1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255,\n",
      "        1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267,\n",
      "        1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,\n",
      "        1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291,\n",
      "        1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "        1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315,\n",
      "        1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327,\n",
      "        1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339,\n",
      "        1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351,\n",
      "        1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363,\n",
      "        1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375,\n",
      "        1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387,\n",
      "        1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399,\n",
      "        1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411,\n",
      "        1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423,\n",
      "        1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "        1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
      "        1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
      "        1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
      "        1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483,\n",
      "        1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495,\n",
      "        1496, 1497, 1498, 1499])\n"
     ]
    }
   ],
   "source": [
    "print(idx_train)\n",
    "print(idx_val)\n",
    "print(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GCNN import NodeClassificationGCNN\n",
    "\n",
    "model = NodeClassificationGCNN(features.shape[1], 256, np.max(labels.detach().numpy())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,label):\n",
    "    oneHotCodded = out.max(1)[1].type_as(label)\n",
    "    return oneHotCodded.eq(label).double().sum()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ; accuracy: 0.9; loss: 0.19724716246128082\n",
      "Validation epoch 0 ; accuracy: 0.7666666666666667; loss: 0.9506338238716125\n",
      "Training epoch 1 ; accuracy: 0.9; loss: 0.1967720240354538\n",
      "Validation epoch 1 ; accuracy: 0.7633333333333333; loss: 0.937579870223999\n",
      "Training epoch 2 ; accuracy: 0.9; loss: 0.19625000655651093\n",
      "Validation epoch 2 ; accuracy: 0.7633333333333333; loss: 0.9434000849723816\n",
      "Training epoch 3 ; accuracy: 0.9; loss: 0.1968870908021927\n",
      "Validation epoch 3 ; accuracy: 0.7666666666666667; loss: 0.9557613134384155\n",
      "Training epoch 4 ; accuracy: 0.9; loss: 0.1962631791830063\n",
      "Validation epoch 4 ; accuracy: 0.76; loss: 0.9684122800827026\n",
      "Training epoch 5 ; accuracy: 0.9; loss: 0.19604450464248657\n",
      "Validation epoch 5 ; accuracy: 0.76; loss: 0.9858748316764832\n",
      "Training epoch 6 ; accuracy: 0.9; loss: 0.19563186168670654\n",
      "Validation epoch 6 ; accuracy: 0.7633333333333333; loss: 1.0061109066009521\n",
      "Training epoch 7 ; accuracy: 0.9; loss: 0.195437490940094\n",
      "Validation epoch 7 ; accuracy: 0.7633333333333333; loss: 1.0257076025009155\n",
      "Training epoch 8 ; accuracy: 0.9; loss: 0.19546198844909668\n",
      "Validation epoch 8 ; accuracy: 0.7633333333333333; loss: 1.0399434566497803\n",
      "Training epoch 9 ; accuracy: 0.9; loss: 0.19534510374069214\n",
      "Validation epoch 9 ; accuracy: 0.7633333333333333; loss: 1.057680606842041\n",
      "Training epoch 10 ; accuracy: 0.9; loss: 0.19523103535175323\n",
      "Validation epoch 10 ; accuracy: 0.7633333333333333; loss: 1.0777478218078613\n",
      "Training epoch 11 ; accuracy: 0.9; loss: 0.19521093368530273\n",
      "Validation epoch 11 ; accuracy: 0.76; loss: 1.0989991426467896\n",
      "Training epoch 12 ; accuracy: 0.9; loss: 0.1952187567949295\n",
      "Validation epoch 12 ; accuracy: 0.76; loss: 1.114132046699524\n",
      "Training epoch 13 ; accuracy: 0.9; loss: 0.1960573047399521\n",
      "Validation epoch 13 ; accuracy: 0.76; loss: 1.1087788343429565\n",
      "Training epoch 14 ; accuracy: 0.9; loss: 0.19522292912006378\n",
      "Validation epoch 14 ; accuracy: 0.7566666666666667; loss: 1.1047942638397217\n",
      "Training epoch 15 ; accuracy: 0.9; loss: 0.194999560713768\n",
      "Validation epoch 15 ; accuracy: 0.75; loss: 1.1042174100875854\n",
      "Training epoch 16 ; accuracy: 0.9; loss: 0.19499020278453827\n",
      "Validation epoch 16 ; accuracy: 0.75; loss: 1.106203556060791\n",
      "Training epoch 17 ; accuracy: 0.9; loss: 0.1949572116136551\n",
      "Validation epoch 17 ; accuracy: 0.7533333333333333; loss: 1.1099988222122192\n",
      "Training epoch 18 ; accuracy: 0.9; loss: 0.194951593875885\n",
      "Validation epoch 18 ; accuracy: 0.7533333333333333; loss: 1.1156044006347656\n",
      "Training epoch 19 ; accuracy: 0.9; loss: 0.1951666623353958\n",
      "Validation epoch 19 ; accuracy: 0.7533333333333333; loss: 1.1261898279190063\n",
      "Training epoch 20 ; accuracy: 0.9; loss: 0.19488625228405\n",
      "Validation epoch 20 ; accuracy: 0.7533333333333333; loss: 1.1370104551315308\n",
      "Training epoch 21 ; accuracy: 0.9; loss: 0.19496473670005798\n",
      "Validation epoch 21 ; accuracy: 0.7466666666666667; loss: 1.1481659412384033\n",
      "Training epoch 22 ; accuracy: 0.9; loss: 0.1948961317539215\n",
      "Validation epoch 22 ; accuracy: 0.7466666666666667; loss: 1.1591256856918335\n",
      "Training epoch 23 ; accuracy: 0.9; loss: 0.19484403729438782\n",
      "Validation epoch 23 ; accuracy: 0.7466666666666667; loss: 1.1693387031555176\n",
      "Training epoch 24 ; accuracy: 0.9; loss: 0.1948627233505249\n",
      "Validation epoch 24 ; accuracy: 0.7466666666666667; loss: 1.1788716316223145\n",
      "Training epoch 25 ; accuracy: 0.9; loss: 0.19492679834365845\n",
      "Validation epoch 25 ; accuracy: 0.7466666666666667; loss: 1.1851688623428345\n",
      "Training epoch 26 ; accuracy: 0.9; loss: 0.19484761357307434\n",
      "Validation epoch 26 ; accuracy: 0.7466666666666667; loss: 1.1903831958770752\n",
      "Training epoch 27 ; accuracy: 0.9; loss: 0.1948128193616867\n",
      "Validation epoch 27 ; accuracy: 0.7466666666666667; loss: 1.1951311826705933\n",
      "Training epoch 28 ; accuracy: 0.9; loss: 0.19477419555187225\n",
      "Validation epoch 28 ; accuracy: 0.7533333333333333; loss: 1.1994105577468872\n",
      "Training epoch 29 ; accuracy: 0.9; loss: 0.19478535652160645\n",
      "Validation epoch 29 ; accuracy: 0.7533333333333333; loss: 1.2033584117889404\n",
      "Training epoch 30 ; accuracy: 0.9; loss: 0.1947692483663559\n",
      "Validation epoch 30 ; accuracy: 0.7566666666666667; loss: 1.2069449424743652\n",
      "Training epoch 31 ; accuracy: 0.9; loss: 0.19472579658031464\n",
      "Validation epoch 31 ; accuracy: 0.7566666666666667; loss: 1.2105686664581299\n",
      "Training epoch 32 ; accuracy: 0.9; loss: 0.19504979252815247\n",
      "Validation epoch 32 ; accuracy: 0.7533333333333333; loss: 1.2109631299972534\n",
      "Training epoch 33 ; accuracy: 0.9; loss: 0.1947447508573532\n",
      "Validation epoch 33 ; accuracy: 0.75; loss: 1.211651086807251\n",
      "Training epoch 34 ; accuracy: 0.9; loss: 0.1947331428527832\n",
      "Validation epoch 34 ; accuracy: 0.7466666666666667; loss: 1.2131166458129883\n",
      "Training epoch 35 ; accuracy: 0.9; loss: 0.19470520317554474\n",
      "Validation epoch 35 ; accuracy: 0.7466666666666667; loss: 1.2151106595993042\n",
      "Training epoch 36 ; accuracy: 0.9; loss: 0.19477717578411102\n",
      "Validation epoch 36 ; accuracy: 0.75; loss: 1.2137726545333862\n",
      "Training epoch 37 ; accuracy: 0.9; loss: 0.19471091032028198\n",
      "Validation epoch 37 ; accuracy: 0.75; loss: 1.2132083177566528\n",
      "Training epoch 38 ; accuracy: 0.9; loss: 0.1947210729122162\n",
      "Validation epoch 38 ; accuracy: 0.75; loss: 1.2132387161254883\n",
      "Training epoch 39 ; accuracy: 0.9; loss: 0.19470228254795074\n",
      "Validation epoch 39 ; accuracy: 0.75; loss: 1.2139958143234253\n",
      "Training epoch 40 ; accuracy: 0.9; loss: 0.19473032653331757\n",
      "Validation epoch 40 ; accuracy: 0.75; loss: 1.215410590171814\n",
      "Training epoch 41 ; accuracy: 0.9; loss: 0.194711372256279\n",
      "Validation epoch 41 ; accuracy: 0.75; loss: 1.2174116373062134\n",
      "Training epoch 42 ; accuracy: 0.9; loss: 0.19472919404506683\n",
      "Validation epoch 42 ; accuracy: 0.75; loss: 1.2199759483337402\n",
      "Training epoch 43 ; accuracy: 0.9; loss: 0.1947120875120163\n",
      "Validation epoch 43 ; accuracy: 0.75; loss: 1.2228753566741943\n",
      "Training epoch 44 ; accuracy: 0.9; loss: 0.19470076262950897\n",
      "Validation epoch 44 ; accuracy: 0.75; loss: 1.226166009902954\n",
      "Training epoch 45 ; accuracy: 0.9; loss: 0.19469857215881348\n",
      "Validation epoch 45 ; accuracy: 0.75; loss: 1.2297800779342651\n",
      "Training epoch 46 ; accuracy: 0.9; loss: 0.19470277428627014\n",
      "Validation epoch 46 ; accuracy: 0.75; loss: 1.2338309288024902\n",
      "Training epoch 47 ; accuracy: 0.9; loss: 0.1947021484375\n",
      "Validation epoch 47 ; accuracy: 0.75; loss: 1.2381465435028076\n",
      "Training epoch 48 ; accuracy: 0.9; loss: 0.19471375644207\n",
      "Validation epoch 48 ; accuracy: 0.7466666666666667; loss: 1.242895245552063\n",
      "Training epoch 49 ; accuracy: 0.9; loss: 0.19469240307807922\n",
      "Validation epoch 49 ; accuracy: 0.7466666666666667; loss: 1.2478965520858765\n",
      "Training epoch 50 ; accuracy: 0.9; loss: 0.1946789175271988\n",
      "Validation epoch 50 ; accuracy: 0.7466666666666667; loss: 1.2529106140136719\n",
      "Training epoch 51 ; accuracy: 0.9; loss: 0.1947060227394104\n",
      "Validation epoch 51 ; accuracy: 0.7433333333333333; loss: 1.258205533027649\n",
      "Training epoch 52 ; accuracy: 0.9; loss: 0.19468066096305847\n",
      "Validation epoch 52 ; accuracy: 0.7433333333333333; loss: 1.2635362148284912\n",
      "Training epoch 53 ; accuracy: 0.9; loss: 0.19466547667980194\n",
      "Validation epoch 53 ; accuracy: 0.7433333333333333; loss: 1.2687581777572632\n",
      "Training epoch 54 ; accuracy: 0.9; loss: 0.1946696639060974\n",
      "Validation epoch 54 ; accuracy: 0.7433333333333333; loss: 1.273991346359253\n",
      "Training epoch 55 ; accuracy: 0.9; loss: 0.19467025995254517\n",
      "Validation epoch 55 ; accuracy: 0.7433333333333333; loss: 1.2792037725448608\n",
      "Training epoch 56 ; accuracy: 0.9; loss: 0.19467756152153015\n",
      "Validation epoch 56 ; accuracy: 0.7433333333333333; loss: 1.28444242477417\n",
      "Training epoch 57 ; accuracy: 0.9; loss: 0.19466377794742584\n",
      "Validation epoch 57 ; accuracy: 0.7433333333333333; loss: 1.2895525693893433\n",
      "Training epoch 58 ; accuracy: 0.9; loss: 0.19465374946594238\n",
      "Validation epoch 58 ; accuracy: 0.7433333333333333; loss: 1.294618010520935\n",
      "Training epoch 59 ; accuracy: 0.9; loss: 0.19466441869735718\n",
      "Validation epoch 59 ; accuracy: 0.7433333333333333; loss: 1.2994569540023804\n",
      "Training epoch 60 ; accuracy: 0.9; loss: 0.19465941190719604\n",
      "Validation epoch 60 ; accuracy: 0.7433333333333333; loss: 1.3042341470718384\n",
      "Training epoch 61 ; accuracy: 0.9; loss: 0.19465403258800507\n",
      "Validation epoch 61 ; accuracy: 0.7433333333333333; loss: 1.3088650703430176\n",
      "Training epoch 62 ; accuracy: 0.9; loss: 0.19464904069900513\n",
      "Validation epoch 62 ; accuracy: 0.7433333333333333; loss: 1.313287377357483\n",
      "Training epoch 63 ; accuracy: 0.9; loss: 0.19464431703090668\n",
      "Validation epoch 63 ; accuracy: 0.7433333333333333; loss: 1.3176031112670898\n",
      "Training epoch 64 ; accuracy: 0.9; loss: 0.19464260339736938\n",
      "Validation epoch 64 ; accuracy: 0.7433333333333333; loss: 1.3218504190444946\n",
      "Training epoch 65 ; accuracy: 0.9; loss: 0.19463875889778137\n",
      "Validation epoch 65 ; accuracy: 0.7433333333333333; loss: 1.3259354829788208\n",
      "Training epoch 66 ; accuracy: 0.9; loss: 0.19464994966983795\n",
      "Validation epoch 66 ; accuracy: 0.7433333333333333; loss: 1.3298779726028442\n",
      "Training epoch 67 ; accuracy: 0.9; loss: 0.19465312361717224\n",
      "Validation epoch 67 ; accuracy: 0.7433333333333333; loss: 1.3338271379470825\n",
      "Training epoch 68 ; accuracy: 0.9; loss: 0.19495552778244019\n",
      "Validation epoch 68 ; accuracy: 0.7466666666666667; loss: 1.3238139152526855\n",
      "Training epoch 69 ; accuracy: 0.9; loss: 0.19464093446731567\n",
      "Validation epoch 69 ; accuracy: 0.7466666666666667; loss: 1.3185940980911255\n",
      "Training epoch 70 ; accuracy: 0.9; loss: 0.19464723765850067\n",
      "Validation epoch 70 ; accuracy: 0.7433333333333333; loss: 1.316881537437439\n",
      "Training epoch 71 ; accuracy: 0.9; loss: 0.19467778503894806\n",
      "Validation epoch 71 ; accuracy: 0.7466666666666667; loss: 1.3171958923339844\n",
      "Training epoch 72 ; accuracy: 0.9; loss: 0.19467735290527344\n",
      "Validation epoch 72 ; accuracy: 0.7466666666666667; loss: 1.3185416460037231\n",
      "Training epoch 73 ; accuracy: 0.9; loss: 0.19473376870155334\n",
      "Validation epoch 73 ; accuracy: 0.7466666666666667; loss: 1.3200024366378784\n",
      "Training epoch 74 ; accuracy: 0.9; loss: 0.1947384923696518\n",
      "Validation epoch 74 ; accuracy: 0.7433333333333333; loss: 1.3206387758255005\n",
      "Training epoch 75 ; accuracy: 0.9; loss: 0.19474302232265472\n",
      "Validation epoch 75 ; accuracy: 0.7433333333333333; loss: 1.321010708808899\n",
      "Training epoch 76 ; accuracy: 0.9; loss: 0.19480440020561218\n",
      "Validation epoch 76 ; accuracy: 0.7433333333333333; loss: 1.3200149536132812\n",
      "Training epoch 77 ; accuracy: 0.9; loss: 0.19480307400226593\n",
      "Validation epoch 77 ; accuracy: 0.7466666666666667; loss: 1.318400502204895\n",
      "Training epoch 78 ; accuracy: 0.9; loss: 0.19475512206554413\n",
      "Validation epoch 78 ; accuracy: 0.7466666666666667; loss: 1.3168766498565674\n",
      "Training epoch 79 ; accuracy: 0.9; loss: 0.19490809738636017\n",
      "Validation epoch 79 ; accuracy: 0.7466666666666667; loss: 1.3142470121383667\n",
      "Training epoch 80 ; accuracy: 0.9; loss: 0.19471733272075653\n",
      "Validation epoch 80 ; accuracy: 0.7466666666666667; loss: 1.3128399848937988\n",
      "Training epoch 81 ; accuracy: 0.9; loss: 0.19470062851905823\n",
      "Validation epoch 81 ; accuracy: 0.7533333333333333; loss: 1.312843680381775\n",
      "Training epoch 82 ; accuracy: 0.9; loss: 0.1947077214717865\n",
      "Validation epoch 82 ; accuracy: 0.7566666666666667; loss: 1.314383625984192\n",
      "Training epoch 83 ; accuracy: 0.9; loss: 0.19468945264816284\n",
      "Validation epoch 83 ; accuracy: 0.7533333333333333; loss: 1.3172465562820435\n",
      "Training epoch 84 ; accuracy: 0.9; loss: 0.1946677565574646\n",
      "Validation epoch 84 ; accuracy: 0.7466666666666667; loss: 1.3213777542114258\n",
      "Training epoch 85 ; accuracy: 0.9; loss: 0.1947554647922516\n",
      "Validation epoch 85 ; accuracy: 0.7466666666666667; loss: 1.336387276649475\n",
      "Training epoch 86 ; accuracy: 0.9; loss: 0.19465824961662292\n",
      "Validation epoch 86 ; accuracy: 0.7433333333333333; loss: 1.3516932725906372\n",
      "Training epoch 87 ; accuracy: 0.9; loss: 0.19465143978595734\n",
      "Validation epoch 87 ; accuracy: 0.7366666666666667; loss: 1.3666331768035889\n",
      "Training epoch 88 ; accuracy: 0.9; loss: 0.1946551501750946\n",
      "Validation epoch 88 ; accuracy: 0.7333333333333333; loss: 1.382564902305603\n",
      "Training epoch 89 ; accuracy: 0.9; loss: 0.19465094804763794\n",
      "Validation epoch 89 ; accuracy: 0.7333333333333333; loss: 1.3975248336791992\n",
      "Training epoch 90 ; accuracy: 0.9; loss: 0.19464987516403198\n",
      "Validation epoch 90 ; accuracy: 0.73; loss: 1.411785364151001\n",
      "Training epoch 91 ; accuracy: 0.9; loss: 0.19466513395309448\n",
      "Validation epoch 91 ; accuracy: 0.7233333333333334; loss: 1.425142765045166\n",
      "Training epoch 92 ; accuracy: 0.9; loss: 0.19466720521450043\n",
      "Validation epoch 92 ; accuracy: 0.7233333333333334; loss: 1.4364625215530396\n",
      "Training epoch 93 ; accuracy: 0.9; loss: 0.19465386867523193\n",
      "Validation epoch 93 ; accuracy: 0.72; loss: 1.4461990594863892\n",
      "Training epoch 94 ; accuracy: 0.9; loss: 0.19466044008731842\n",
      "Validation epoch 94 ; accuracy: 0.72; loss: 1.454382061958313\n",
      "Training epoch 95 ; accuracy: 0.9; loss: 0.19465520977973938\n",
      "Validation epoch 95 ; accuracy: 0.72; loss: 1.4606006145477295\n",
      "Training epoch 96 ; accuracy: 0.9; loss: 0.19466231763362885\n",
      "Validation epoch 96 ; accuracy: 0.7233333333333334; loss: 1.4652705192565918\n",
      "Training epoch 97 ; accuracy: 0.9; loss: 0.19465097784996033\n",
      "Validation epoch 97 ; accuracy: 0.7266666666666667; loss: 1.4684832096099854\n",
      "Training epoch 98 ; accuracy: 0.9; loss: 0.19464460015296936\n",
      "Validation epoch 98 ; accuracy: 0.73; loss: 1.4703185558319092\n",
      "Training epoch 99 ; accuracy: 0.9; loss: 0.19464942812919617\n",
      "Validation epoch 99 ; accuracy: 0.7266666666666667; loss: 1.4712737798690796\n",
      "Training epoch 100 ; accuracy: 0.9; loss: 0.19464436173439026\n",
      "Validation epoch 100 ; accuracy: 0.7266666666666667; loss: 1.4710474014282227\n",
      "Training epoch 101 ; accuracy: 0.9; loss: 0.19464156031608582\n",
      "Validation epoch 101 ; accuracy: 0.7266666666666667; loss: 1.4699994325637817\n",
      "Training epoch 102 ; accuracy: 0.9; loss: 0.19463568925857544\n",
      "Validation epoch 102 ; accuracy: 0.7266666666666667; loss: 1.468388557434082\n",
      "Training epoch 103 ; accuracy: 0.9; loss: 0.19463714957237244\n",
      "Validation epoch 103 ; accuracy: 0.7266666666666667; loss: 1.4662885665893555\n",
      "Training epoch 104 ; accuracy: 0.9; loss: 0.19463051855564117\n",
      "Validation epoch 104 ; accuracy: 0.73; loss: 1.4640952348709106\n",
      "Training epoch 105 ; accuracy: 0.9; loss: 0.19463129341602325\n",
      "Validation epoch 105 ; accuracy: 0.7333333333333333; loss: 1.4617735147476196\n",
      "Training epoch 106 ; accuracy: 0.9; loss: 0.19463294744491577\n",
      "Validation epoch 106 ; accuracy: 0.7333333333333333; loss: 1.4594858884811401\n",
      "Training epoch 107 ; accuracy: 0.9; loss: 0.19462692737579346\n",
      "Validation epoch 107 ; accuracy: 0.7366666666666667; loss: 1.4571497440338135\n",
      "Training epoch 108 ; accuracy: 0.9; loss: 0.19462382793426514\n",
      "Validation epoch 108 ; accuracy: 0.7366666666666667; loss: 1.4549858570098877\n",
      "Training epoch 109 ; accuracy: 0.9; loss: 0.19464972615242004\n",
      "Validation epoch 109 ; accuracy: 0.7433333333333333; loss: 1.4375388622283936\n",
      "Training epoch 110 ; accuracy: 0.9; loss: 0.19462275505065918\n",
      "Validation epoch 110 ; accuracy: 0.7466666666666667; loss: 1.424314260482788\n",
      "Training epoch 111 ; accuracy: 0.9; loss: 0.19462229311466217\n",
      "Validation epoch 111 ; accuracy: 0.7466666666666667; loss: 1.4146655797958374\n",
      "Training epoch 112 ; accuracy: 0.9; loss: 0.1946282535791397\n",
      "Validation epoch 112 ; accuracy: 0.7466666666666667; loss: 1.407759189605713\n",
      "Training epoch 113 ; accuracy: 0.9; loss: 0.19462259113788605\n",
      "Validation epoch 113 ; accuracy: 0.75; loss: 1.4028340578079224\n",
      "Training epoch 114 ; accuracy: 0.9; loss: 0.19461795687675476\n",
      "Validation epoch 114 ; accuracy: 0.7466666666666667; loss: 1.3994417190551758\n",
      "Training epoch 115 ; accuracy: 0.9; loss: 0.19462569057941437\n",
      "Validation epoch 115 ; accuracy: 0.7466666666666667; loss: 1.3971171379089355\n",
      "Training epoch 116 ; accuracy: 0.9; loss: 0.1946231573820114\n",
      "Validation epoch 116 ; accuracy: 0.7466666666666667; loss: 1.3956753015518188\n",
      "Training epoch 117 ; accuracy: 0.9; loss: 0.19462375342845917\n",
      "Validation epoch 117 ; accuracy: 0.7466666666666667; loss: 1.394888162612915\n",
      "Training epoch 118 ; accuracy: 0.9; loss: 0.1946319043636322\n",
      "Validation epoch 118 ; accuracy: 0.7466666666666667; loss: 1.3947482109069824\n",
      "Training epoch 119 ; accuracy: 0.9; loss: 0.19462262094020844\n",
      "Validation epoch 119 ; accuracy: 0.7466666666666667; loss: 1.3948018550872803\n",
      "Training epoch 120 ; accuracy: 0.9; loss: 0.19463086128234863\n",
      "Validation epoch 120 ; accuracy: 0.7466666666666667; loss: 1.3950971364974976\n",
      "Training epoch 121 ; accuracy: 0.9; loss: 0.19462750852108002\n",
      "Validation epoch 121 ; accuracy: 0.7466666666666667; loss: 1.3956972360610962\n",
      "Training epoch 122 ; accuracy: 0.9; loss: 0.19463026523590088\n",
      "Validation epoch 122 ; accuracy: 0.7466666666666667; loss: 1.3966425657272339\n",
      "Training epoch 123 ; accuracy: 0.9; loss: 0.19462943077087402\n",
      "Validation epoch 123 ; accuracy: 0.7466666666666667; loss: 1.397727608680725\n",
      "Training epoch 124 ; accuracy: 0.9; loss: 0.19463396072387695\n",
      "Validation epoch 124 ; accuracy: 0.7466666666666667; loss: 1.3988995552062988\n",
      "Training epoch 125 ; accuracy: 0.9; loss: 0.1948578804731369\n",
      "Validation epoch 125 ; accuracy: 0.7466666666666667; loss: 1.3898075819015503\n",
      "Training epoch 126 ; accuracy: 0.9; loss: 0.19462841749191284\n",
      "Validation epoch 126 ; accuracy: 0.7466666666666667; loss: 1.3824485540390015\n",
      "Training epoch 127 ; accuracy: 0.9; loss: 0.19462864100933075\n",
      "Validation epoch 127 ; accuracy: 0.7433333333333333; loss: 1.3765228986740112\n",
      "Training epoch 128 ; accuracy: 0.9; loss: 0.1946258246898651\n",
      "Validation epoch 128 ; accuracy: 0.7433333333333333; loss: 1.3718079328536987\n",
      "Training epoch 129 ; accuracy: 0.9; loss: 0.19462993741035461\n",
      "Validation epoch 129 ; accuracy: 0.7466666666666667; loss: 1.368102788925171\n",
      "Training epoch 130 ; accuracy: 0.9; loss: 0.1946340650320053\n",
      "Validation epoch 130 ; accuracy: 0.7466666666666667; loss: 1.3654627799987793\n",
      "Training epoch 131 ; accuracy: 0.9; loss: 0.19462692737579346\n",
      "Validation epoch 131 ; accuracy: 0.7466666666666667; loss: 1.3633852005004883\n",
      "Training epoch 132 ; accuracy: 0.9; loss: 0.194650337100029\n",
      "Validation epoch 132 ; accuracy: 0.7466666666666667; loss: 1.3621784448623657\n",
      "Training epoch 133 ; accuracy: 0.9; loss: 0.19463951885700226\n",
      "Validation epoch 133 ; accuracy: 0.7466666666666667; loss: 1.3617050647735596\n",
      "Training epoch 134 ; accuracy: 0.9; loss: 0.1946389377117157\n",
      "Validation epoch 134 ; accuracy: 0.7466666666666667; loss: 1.3617911338806152\n",
      "Training epoch 135 ; accuracy: 0.9; loss: 0.1946367472410202\n",
      "Validation epoch 135 ; accuracy: 0.7466666666666667; loss: 1.362297773361206\n",
      "Training epoch 136 ; accuracy: 0.9; loss: 0.19464877247810364\n",
      "Validation epoch 136 ; accuracy: 0.7466666666666667; loss: 1.3633613586425781\n",
      "Training epoch 137 ; accuracy: 0.9; loss: 0.1946321725845337\n",
      "Validation epoch 137 ; accuracy: 0.7466666666666667; loss: 1.3647441864013672\n",
      "Training epoch 138 ; accuracy: 0.9; loss: 0.1946537345647812\n",
      "Validation epoch 138 ; accuracy: 0.7466666666666667; loss: 1.3667107820510864\n",
      "Training epoch 139 ; accuracy: 0.9; loss: 0.19465719163417816\n",
      "Validation epoch 139 ; accuracy: 0.7466666666666667; loss: 1.3691716194152832\n",
      "Training epoch 140 ; accuracy: 0.9; loss: 0.1946515589952469\n",
      "Validation epoch 140 ; accuracy: 0.7466666666666667; loss: 1.372113823890686\n",
      "Training epoch 141 ; accuracy: 0.9; loss: 0.19464656710624695\n",
      "Validation epoch 141 ; accuracy: 0.7433333333333333; loss: 1.3753526210784912\n",
      "Training epoch 142 ; accuracy: 0.9; loss: 0.19465269148349762\n",
      "Validation epoch 142 ; accuracy: 0.7433333333333333; loss: 1.3789137601852417\n",
      "Training epoch 143 ; accuracy: 0.9; loss: 0.19465161859989166\n",
      "Validation epoch 143 ; accuracy: 0.7433333333333333; loss: 1.3827496767044067\n",
      "Training epoch 144 ; accuracy: 0.9; loss: 0.1946439892053604\n",
      "Validation epoch 144 ; accuracy: 0.7433333333333333; loss: 1.3867981433868408\n",
      "Training epoch 145 ; accuracy: 0.9; loss: 0.1946360021829605\n",
      "Validation epoch 145 ; accuracy: 0.7433333333333333; loss: 1.3909929990768433\n",
      "Training epoch 146 ; accuracy: 0.9; loss: 0.1946302205324173\n",
      "Validation epoch 146 ; accuracy: 0.7433333333333333; loss: 1.3950655460357666\n",
      "Training epoch 147 ; accuracy: 0.9; loss: 0.19462957978248596\n",
      "Validation epoch 147 ; accuracy: 0.7466666666666667; loss: 1.3991022109985352\n",
      "Training epoch 148 ; accuracy: 0.9; loss: 0.19463133811950684\n",
      "Validation epoch 148 ; accuracy: 0.7466666666666667; loss: 1.4032154083251953\n",
      "Training epoch 149 ; accuracy: 0.9; loss: 0.194629967212677\n",
      "Validation epoch 149 ; accuracy: 0.7433333333333333; loss: 1.4074218273162842\n",
      "Training epoch 150 ; accuracy: 0.9; loss: 0.19462421536445618\n",
      "Validation epoch 150 ; accuracy: 0.7433333333333333; loss: 1.4115922451019287\n",
      "Training epoch 151 ; accuracy: 0.9; loss: 0.19461655616760254\n",
      "Validation epoch 151 ; accuracy: 0.7433333333333333; loss: 1.4156557321548462\n",
      "Training epoch 152 ; accuracy: 0.9; loss: 0.1946282535791397\n",
      "Validation epoch 152 ; accuracy: 0.7433333333333333; loss: 1.4195263385772705\n",
      "Training epoch 153 ; accuracy: 0.9; loss: 0.1946219652891159\n",
      "Validation epoch 153 ; accuracy: 0.7433333333333333; loss: 1.4233583211898804\n",
      "Training epoch 154 ; accuracy: 0.9; loss: 0.19461862742900848\n",
      "Validation epoch 154 ; accuracy: 0.7433333333333333; loss: 1.4271512031555176\n",
      "Training epoch 155 ; accuracy: 0.9; loss: 0.19461874663829803\n",
      "Validation epoch 155 ; accuracy: 0.7433333333333333; loss: 1.4307975769042969\n",
      "Training epoch 156 ; accuracy: 0.9; loss: 0.19461283087730408\n",
      "Validation epoch 156 ; accuracy: 0.7466666666666667; loss: 1.4343401193618774\n",
      "Training epoch 157 ; accuracy: 0.9; loss: 0.1946159154176712\n",
      "Validation epoch 157 ; accuracy: 0.7466666666666667; loss: 1.4378010034561157\n",
      "Training epoch 158 ; accuracy: 0.9; loss: 0.19461499154567719\n",
      "Validation epoch 158 ; accuracy: 0.7466666666666667; loss: 1.4412165880203247\n",
      "Training epoch 159 ; accuracy: 0.9; loss: 0.1946154683828354\n",
      "Validation epoch 159 ; accuracy: 0.7466666666666667; loss: 1.4445738792419434\n",
      "Training epoch 160 ; accuracy: 0.9; loss: 0.1946086585521698\n",
      "Validation epoch 160 ; accuracy: 0.7466666666666667; loss: 1.447803020477295\n",
      "Training epoch 161 ; accuracy: 0.9; loss: 0.19460946321487427\n",
      "Validation epoch 161 ; accuracy: 0.7466666666666667; loss: 1.4508955478668213\n",
      "Training epoch 162 ; accuracy: 0.9; loss: 0.1946156769990921\n",
      "Validation epoch 162 ; accuracy: 0.7466666666666667; loss: 1.4539604187011719\n",
      "Training epoch 163 ; accuracy: 0.9; loss: 0.19461211562156677\n",
      "Validation epoch 163 ; accuracy: 0.7466666666666667; loss: 1.456913948059082\n",
      "Training epoch 164 ; accuracy: 0.9; loss: 0.19460996985435486\n",
      "Validation epoch 164 ; accuracy: 0.7466666666666667; loss: 1.4596885442733765\n",
      "Training epoch 165 ; accuracy: 0.9; loss: 0.1946112960577011\n",
      "Validation epoch 165 ; accuracy: 0.7466666666666667; loss: 1.4623852968215942\n",
      "Training epoch 166 ; accuracy: 0.9; loss: 0.19461145997047424\n",
      "Validation epoch 166 ; accuracy: 0.7466666666666667; loss: 1.4649924039840698\n",
      "Training epoch 167 ; accuracy: 0.9; loss: 0.19460956752300262\n",
      "Validation epoch 167 ; accuracy: 0.7466666666666667; loss: 1.4675441980361938\n",
      "Training epoch 168 ; accuracy: 0.9; loss: 0.19461454451084137\n",
      "Validation epoch 168 ; accuracy: 0.7466666666666667; loss: 1.4701000452041626\n",
      "Training epoch 169 ; accuracy: 0.9; loss: 0.19460690021514893\n",
      "Validation epoch 169 ; accuracy: 0.7466666666666667; loss: 1.4725446701049805\n",
      "Training epoch 170 ; accuracy: 0.9; loss: 0.1946093887090683\n",
      "Validation epoch 170 ; accuracy: 0.7466666666666667; loss: 1.4748643636703491\n",
      "Training epoch 171 ; accuracy: 0.9; loss: 0.1946094036102295\n",
      "Validation epoch 171 ; accuracy: 0.7466666666666667; loss: 1.4770599603652954\n",
      "Training epoch 172 ; accuracy: 0.9; loss: 0.1946108192205429\n",
      "Validation epoch 172 ; accuracy: 0.7466666666666667; loss: 1.4792286157608032\n",
      "Training epoch 173 ; accuracy: 0.9; loss: 0.1946074217557907\n",
      "Validation epoch 173 ; accuracy: 0.7466666666666667; loss: 1.4813874959945679\n",
      "Training epoch 174 ; accuracy: 0.9; loss: 0.19460761547088623\n",
      "Validation epoch 174 ; accuracy: 0.7466666666666667; loss: 1.4834412336349487\n",
      "Training epoch 175 ; accuracy: 0.9; loss: 0.19460880756378174\n",
      "Validation epoch 175 ; accuracy: 0.7466666666666667; loss: 1.485419750213623\n",
      "Training epoch 176 ; accuracy: 0.9; loss: 0.19460703432559967\n",
      "Validation epoch 176 ; accuracy: 0.7466666666666667; loss: 1.487343430519104\n",
      "Training epoch 177 ; accuracy: 0.9; loss: 0.19460800290107727\n",
      "Validation epoch 177 ; accuracy: 0.7466666666666667; loss: 1.4892886877059937\n",
      "Training epoch 178 ; accuracy: 0.9; loss: 0.19460754096508026\n",
      "Validation epoch 178 ; accuracy: 0.7466666666666667; loss: 1.4911640882492065\n",
      "Training epoch 179 ; accuracy: 0.9; loss: 0.19460470974445343\n",
      "Validation epoch 179 ; accuracy: 0.7466666666666667; loss: 1.4929709434509277\n",
      "Training epoch 180 ; accuracy: 0.9; loss: 0.194605752825737\n",
      "Validation epoch 180 ; accuracy: 0.7466666666666667; loss: 1.4947431087493896\n",
      "Training epoch 181 ; accuracy: 0.9; loss: 0.1946057230234146\n",
      "Validation epoch 181 ; accuracy: 0.7466666666666667; loss: 1.4964416027069092\n",
      "Training epoch 182 ; accuracy: 0.9; loss: 0.19460652768611908\n",
      "Validation epoch 182 ; accuracy: 0.7466666666666667; loss: 1.4980738162994385\n",
      "Training epoch 183 ; accuracy: 0.9; loss: 0.19460545480251312\n",
      "Validation epoch 183 ; accuracy: 0.7466666666666667; loss: 1.4996486902236938\n",
      "Training epoch 184 ; accuracy: 0.9; loss: 0.19460701942443848\n",
      "Validation epoch 184 ; accuracy: 0.7466666666666667; loss: 1.501142144203186\n",
      "Training epoch 185 ; accuracy: 0.9; loss: 0.1946050077676773\n",
      "Validation epoch 185 ; accuracy: 0.7466666666666667; loss: 1.5025756359100342\n",
      "Training epoch 186 ; accuracy: 0.9; loss: 0.19460435211658478\n",
      "Validation epoch 186 ; accuracy: 0.7466666666666667; loss: 1.503998875617981\n",
      "Training epoch 187 ; accuracy: 0.9; loss: 0.1946043223142624\n",
      "Validation epoch 187 ; accuracy: 0.7466666666666667; loss: 1.5053852796554565\n",
      "Training epoch 188 ; accuracy: 0.9; loss: 0.19460582733154297\n",
      "Validation epoch 188 ; accuracy: 0.7466666666666667; loss: 1.5067732334136963\n",
      "Training epoch 189 ; accuracy: 0.9; loss: 0.19460733234882355\n",
      "Validation epoch 189 ; accuracy: 0.7466666666666667; loss: 1.50806725025177\n",
      "Training epoch 190 ; accuracy: 0.9; loss: 0.1946066915988922\n",
      "Validation epoch 190 ; accuracy: 0.7466666666666667; loss: 1.5093755722045898\n",
      "Training epoch 191 ; accuracy: 0.9; loss: 0.19460448622703552\n",
      "Validation epoch 191 ; accuracy: 0.7466666666666667; loss: 1.5106518268585205\n",
      "Training epoch 192 ; accuracy: 0.9; loss: 0.1946023404598236\n",
      "Validation epoch 192 ; accuracy: 0.7466666666666667; loss: 1.5118896961212158\n",
      "Training epoch 193 ; accuracy: 0.9; loss: 0.1946042776107788\n",
      "Validation epoch 193 ; accuracy: 0.7466666666666667; loss: 1.5130733251571655\n",
      "Training epoch 194 ; accuracy: 0.9; loss: 0.19460386037826538\n",
      "Validation epoch 194 ; accuracy: 0.7466666666666667; loss: 1.514248251914978\n",
      "Training epoch 195 ; accuracy: 0.9; loss: 0.19460606575012207\n",
      "Validation epoch 195 ; accuracy: 0.7466666666666667; loss: 1.5154787302017212\n",
      "Training epoch 196 ; accuracy: 0.9; loss: 0.19460703432559967\n",
      "Validation epoch 196 ; accuracy: 0.7466666666666667; loss: 1.5168180465698242\n",
      "Training epoch 197 ; accuracy: 0.9; loss: 0.1946021467447281\n",
      "Validation epoch 197 ; accuracy: 0.7466666666666667; loss: 1.5180864334106445\n",
      "Training epoch 198 ; accuracy: 0.9; loss: 0.19460158050060272\n",
      "Validation epoch 198 ; accuracy: 0.7433333333333333; loss: 1.5193703174591064\n",
      "Training epoch 199 ; accuracy: 0.9; loss: 0.19460032880306244\n",
      "Validation epoch 199 ; accuracy: 0.7433333333333333; loss: 1.5206164121627808\n",
      "Training epoch 200 ; accuracy: 0.9; loss: 0.19460365176200867\n",
      "Validation epoch 200 ; accuracy: 0.7433333333333333; loss: 1.521726131439209\n",
      "Training epoch 201 ; accuracy: 0.9; loss: 0.19460192322731018\n",
      "Validation epoch 201 ; accuracy: 0.7433333333333333; loss: 1.5228267908096313\n",
      "Training epoch 202 ; accuracy: 0.9; loss: 0.19460256397724152\n",
      "Validation epoch 202 ; accuracy: 0.7433333333333333; loss: 1.5238513946533203\n",
      "Training epoch 203 ; accuracy: 0.9; loss: 0.1946030855178833\n",
      "Validation epoch 203 ; accuracy: 0.7433333333333333; loss: 1.5249171257019043\n",
      "Training epoch 204 ; accuracy: 0.9; loss: 0.19460362195968628\n",
      "Validation epoch 204 ; accuracy: 0.7433333333333333; loss: 1.5259982347488403\n",
      "Training epoch 205 ; accuracy: 0.9; loss: 0.19460202753543854\n",
      "Validation epoch 205 ; accuracy: 0.7433333333333333; loss: 1.5270805358886719\n",
      "Training epoch 206 ; accuracy: 0.9; loss: 0.1946021467447281\n",
      "Validation epoch 206 ; accuracy: 0.7433333333333333; loss: 1.5281554460525513\n",
      "Training epoch 207 ; accuracy: 0.9; loss: 0.1949232667684555\n",
      "Validation epoch 207 ; accuracy: 0.7433333333333333; loss: 1.5172662734985352\n",
      "Training epoch 208 ; accuracy: 0.9; loss: 0.19460032880306244\n",
      "Validation epoch 208 ; accuracy: 0.7433333333333333; loss: 1.5107396841049194\n",
      "Training epoch 209 ; accuracy: 0.9; loss: 0.1946028620004654\n",
      "Validation epoch 209 ; accuracy: 0.7433333333333333; loss: 1.5069016218185425\n",
      "Training epoch 210 ; accuracy: 0.9; loss: 0.19460545480251312\n",
      "Validation epoch 210 ; accuracy: 0.7433333333333333; loss: 1.5048187971115112\n",
      "Training epoch 211 ; accuracy: 0.9; loss: 0.194610133767128\n",
      "Validation epoch 211 ; accuracy: 0.7433333333333333; loss: 1.503944754600525\n",
      "Training epoch 212 ; accuracy: 0.9; loss: 0.1946180760860443\n",
      "Validation epoch 212 ; accuracy: 0.7466666666666667; loss: 1.5040065050125122\n",
      "Training epoch 213 ; accuracy: 0.9; loss: 0.1946248561143875\n",
      "Validation epoch 213 ; accuracy: 0.7466666666666667; loss: 1.5045828819274902\n",
      "Training epoch 214 ; accuracy: 0.9; loss: 0.19462451338768005\n",
      "Validation epoch 214 ; accuracy: 0.7466666666666667; loss: 1.5054197311401367\n",
      "Training epoch 215 ; accuracy: 0.9; loss: 0.19468863308429718\n",
      "Validation epoch 215 ; accuracy: 0.7466666666666667; loss: 1.4952750205993652\n",
      "Training epoch 216 ; accuracy: 0.9; loss: 0.19464120268821716\n",
      "Validation epoch 216 ; accuracy: 0.7466666666666667; loss: 1.4866150617599487\n",
      "Training epoch 217 ; accuracy: 0.9; loss: 0.19464793801307678\n",
      "Validation epoch 217 ; accuracy: 0.7466666666666667; loss: 1.4794389009475708\n",
      "Training epoch 218 ; accuracy: 0.9; loss: 0.19463205337524414\n",
      "Validation epoch 218 ; accuracy: 0.7466666666666667; loss: 1.4736955165863037\n",
      "Training epoch 219 ; accuracy: 0.9; loss: 0.19463811814785004\n",
      "Validation epoch 219 ; accuracy: 0.7466666666666667; loss: 1.4691249132156372\n",
      "Training epoch 220 ; accuracy: 0.9; loss: 0.19462856650352478\n",
      "Validation epoch 220 ; accuracy: 0.7466666666666667; loss: 1.4655531644821167\n",
      "Training epoch 221 ; accuracy: 0.9; loss: 0.19462309777736664\n",
      "Validation epoch 221 ; accuracy: 0.7433333333333333; loss: 1.4628198146820068\n",
      "Training epoch 222 ; accuracy: 0.9; loss: 0.1946236938238144\n",
      "Validation epoch 222 ; accuracy: 0.7433333333333333; loss: 1.4610899686813354\n",
      "Training epoch 223 ; accuracy: 0.9; loss: 0.19462384283542633\n",
      "Validation epoch 223 ; accuracy: 0.74; loss: 1.4600772857666016\n",
      "Training epoch 224 ; accuracy: 0.9; loss: 0.1946249008178711\n",
      "Validation epoch 224 ; accuracy: 0.7433333333333333; loss: 1.4596470594406128\n",
      "Training epoch 225 ; accuracy: 0.9; loss: 0.1946190446615219\n",
      "Validation epoch 225 ; accuracy: 0.7433333333333333; loss: 1.4597338438034058\n",
      "Training epoch 226 ; accuracy: 0.9; loss: 0.19461850821971893\n",
      "Validation epoch 226 ; accuracy: 0.7433333333333333; loss: 1.4601719379425049\n",
      "Training epoch 227 ; accuracy: 0.9; loss: 0.19461947679519653\n",
      "Validation epoch 227 ; accuracy: 0.7433333333333333; loss: 1.4609066247940063\n",
      "Training epoch 228 ; accuracy: 0.9; loss: 0.1946125626564026\n",
      "Validation epoch 228 ; accuracy: 0.7433333333333333; loss: 1.461867094039917\n",
      "Training epoch 229 ; accuracy: 0.9; loss: 0.1946127563714981\n",
      "Validation epoch 229 ; accuracy: 0.7433333333333333; loss: 1.462998390197754\n",
      "Training epoch 230 ; accuracy: 0.9; loss: 0.19461128115653992\n",
      "Validation epoch 230 ; accuracy: 0.7433333333333333; loss: 1.4642270803451538\n",
      "Training epoch 231 ; accuracy: 0.9; loss: 0.1946144700050354\n",
      "Validation epoch 231 ; accuracy: 0.7433333333333333; loss: 1.4655317068099976\n",
      "Training epoch 232 ; accuracy: 0.9; loss: 0.19461306929588318\n",
      "Validation epoch 232 ; accuracy: 0.7433333333333333; loss: 1.4669036865234375\n",
      "Training epoch 233 ; accuracy: 0.9; loss: 0.19461548328399658\n",
      "Validation epoch 233 ; accuracy: 0.7433333333333333; loss: 1.4683300256729126\n",
      "Training epoch 234 ; accuracy: 0.9; loss: 0.19461145997047424\n",
      "Validation epoch 234 ; accuracy: 0.7433333333333333; loss: 1.4697861671447754\n",
      "Training epoch 235 ; accuracy: 0.9; loss: 0.19461481273174286\n",
      "Validation epoch 235 ; accuracy: 0.7433333333333333; loss: 1.4713109731674194\n",
      "Training epoch 236 ; accuracy: 0.9; loss: 0.19461855292320251\n",
      "Validation epoch 236 ; accuracy: 0.7433333333333333; loss: 1.4727740287780762\n",
      "Training epoch 237 ; accuracy: 0.9; loss: 0.19460827112197876\n",
      "Validation epoch 237 ; accuracy: 0.7433333333333333; loss: 1.4741750955581665\n",
      "Training epoch 238 ; accuracy: 0.9; loss: 0.19461102783679962\n",
      "Validation epoch 238 ; accuracy: 0.7433333333333333; loss: 1.4755923748016357\n",
      "Training epoch 239 ; accuracy: 0.9; loss: 0.1946120411157608\n",
      "Validation epoch 239 ; accuracy: 0.74; loss: 1.4769625663757324\n",
      "Training epoch 240 ; accuracy: 0.9; loss: 0.1946134716272354\n",
      "Validation epoch 240 ; accuracy: 0.74; loss: 1.4783827066421509\n",
      "Training epoch 241 ; accuracy: 0.9; loss: 0.1946103870868683\n",
      "Validation epoch 241 ; accuracy: 0.74; loss: 1.4788012504577637\n",
      "Training epoch 242 ; accuracy: 0.9; loss: 0.19460615515708923\n",
      "Validation epoch 242 ; accuracy: 0.7366666666666667; loss: 1.4788035154342651\n",
      "Training epoch 243 ; accuracy: 0.9; loss: 0.19461198151111603\n",
      "Validation epoch 243 ; accuracy: 0.7366666666666667; loss: 1.4791085720062256\n",
      "Training epoch 244 ; accuracy: 0.9; loss: 0.19460877776145935\n",
      "Validation epoch 244 ; accuracy: 0.7366666666666667; loss: 1.4796817302703857\n",
      "Training epoch 245 ; accuracy: 0.9; loss: 0.19460733234882355\n",
      "Validation epoch 245 ; accuracy: 0.7366666666666667; loss: 1.4803757667541504\n",
      "Training epoch 246 ; accuracy: 0.9; loss: 0.19460657238960266\n",
      "Validation epoch 246 ; accuracy: 0.7366666666666667; loss: 1.4812037944793701\n",
      "Training epoch 247 ; accuracy: 0.9; loss: 0.19460587203502655\n",
      "Validation epoch 247 ; accuracy: 0.7366666666666667; loss: 1.4820553064346313\n",
      "Training epoch 248 ; accuracy: 0.9; loss: 0.19461044669151306\n",
      "Validation epoch 248 ; accuracy: 0.7366666666666667; loss: 1.4829174280166626\n",
      "Training epoch 249 ; accuracy: 0.9; loss: 0.19460830092430115\n",
      "Validation epoch 249 ; accuracy: 0.7366666666666667; loss: 1.4838095903396606\n",
      "Training epoch 250 ; accuracy: 0.9; loss: 0.1946096122264862\n",
      "Validation epoch 250 ; accuracy: 0.74; loss: 1.4846333265304565\n",
      "Training epoch 251 ; accuracy: 0.9; loss: 0.19460812211036682\n",
      "Validation epoch 251 ; accuracy: 0.74; loss: 1.485514760017395\n",
      "Training epoch 252 ; accuracy: 0.9; loss: 0.19460701942443848\n",
      "Validation epoch 252 ; accuracy: 0.74; loss: 1.486457109451294\n",
      "Training epoch 253 ; accuracy: 0.9; loss: 0.1946064680814743\n",
      "Validation epoch 253 ; accuracy: 0.74; loss: 1.4874192476272583\n",
      "Training epoch 254 ; accuracy: 0.9; loss: 0.19460855424404144\n",
      "Validation epoch 254 ; accuracy: 0.74; loss: 1.4884825944900513\n",
      "Training epoch 255 ; accuracy: 0.9; loss: 0.19461078941822052\n",
      "Validation epoch 255 ; accuracy: 0.74; loss: 1.4896634817123413\n",
      "Training epoch 256 ; accuracy: 0.9; loss: 0.19460390508174896\n",
      "Validation epoch 256 ; accuracy: 0.74; loss: 1.4908380508422852\n",
      "Training epoch 257 ; accuracy: 0.9; loss: 0.194605752825737\n",
      "Validation epoch 257 ; accuracy: 0.74; loss: 1.4920004606246948\n",
      "Training epoch 258 ; accuracy: 0.9; loss: 0.19460731744766235\n",
      "Validation epoch 258 ; accuracy: 0.74; loss: 1.493196964263916\n",
      "Training epoch 259 ; accuracy: 0.9; loss: 0.1946052759885788\n",
      "Validation epoch 259 ; accuracy: 0.74; loss: 1.4943889379501343\n",
      "Training epoch 260 ; accuracy: 0.9; loss: 0.1946086883544922\n",
      "Validation epoch 260 ; accuracy: 0.74; loss: 1.495589256286621\n",
      "Training epoch 261 ; accuracy: 0.9; loss: 0.19460773468017578\n",
      "Validation epoch 261 ; accuracy: 0.74; loss: 1.4967888593673706\n",
      "Training epoch 262 ; accuracy: 0.9; loss: 0.1946067214012146\n",
      "Validation epoch 262 ; accuracy: 0.74; loss: 1.497983694076538\n",
      "Training epoch 263 ; accuracy: 0.9; loss: 0.19460776448249817\n",
      "Validation epoch 263 ; accuracy: 0.7433333333333333; loss: 1.4992226362228394\n",
      "Training epoch 264 ; accuracy: 0.9; loss: 0.19460619986057281\n",
      "Validation epoch 264 ; accuracy: 0.7433333333333333; loss: 1.5004162788391113\n",
      "Training epoch 265 ; accuracy: 0.9; loss: 0.19460605084896088\n",
      "Validation epoch 265 ; accuracy: 0.7433333333333333; loss: 1.5016400814056396\n",
      "Training epoch 266 ; accuracy: 0.9; loss: 0.19460554420948029\n",
      "Validation epoch 266 ; accuracy: 0.7433333333333333; loss: 1.5029120445251465\n",
      "Training epoch 267 ; accuracy: 0.9; loss: 0.19460389018058777\n",
      "Validation epoch 267 ; accuracy: 0.7433333333333333; loss: 1.504179835319519\n",
      "Training epoch 268 ; accuracy: 0.9; loss: 0.19460637867450714\n",
      "Validation epoch 268 ; accuracy: 0.7433333333333333; loss: 1.50551438331604\n",
      "Training epoch 269 ; accuracy: 0.9; loss: 0.19460566341876984\n",
      "Validation epoch 269 ; accuracy: 0.7433333333333333; loss: 1.5067864656448364\n",
      "Training epoch 270 ; accuracy: 0.9; loss: 0.19460418820381165\n",
      "Validation epoch 270 ; accuracy: 0.7433333333333333; loss: 1.5081024169921875\n",
      "Training epoch 271 ; accuracy: 0.9; loss: 0.19460302591323853\n",
      "Validation epoch 271 ; accuracy: 0.7433333333333333; loss: 1.5094363689422607\n",
      "Training epoch 272 ; accuracy: 0.9; loss: 0.1946021467447281\n",
      "Validation epoch 272 ; accuracy: 0.7433333333333333; loss: 1.5107386112213135\n",
      "Training epoch 273 ; accuracy: 0.9; loss: 0.19460241496562958\n",
      "Validation epoch 273 ; accuracy: 0.7433333333333333; loss: 1.511997938156128\n",
      "Training epoch 274 ; accuracy: 0.9; loss: 0.19460098445415497\n",
      "Validation epoch 274 ; accuracy: 0.7433333333333333; loss: 1.5132476091384888\n",
      "Training epoch 275 ; accuracy: 0.9; loss: 0.1946031153202057\n",
      "Validation epoch 275 ; accuracy: 0.7433333333333333; loss: 1.514470100402832\n",
      "Training epoch 276 ; accuracy: 0.9; loss: 0.19460204243659973\n",
      "Validation epoch 276 ; accuracy: 0.7433333333333333; loss: 1.515654444694519\n",
      "Training epoch 277 ; accuracy: 0.9; loss: 0.1946008950471878\n",
      "Validation epoch 277 ; accuracy: 0.7433333333333333; loss: 1.5168300867080688\n",
      "Training epoch 278 ; accuracy: 0.9; loss: 0.19459941983222961\n",
      "Validation epoch 278 ; accuracy: 0.7433333333333333; loss: 1.5179897546768188\n",
      "Training epoch 279 ; accuracy: 0.9; loss: 0.19460232555866241\n",
      "Validation epoch 279 ; accuracy: 0.7433333333333333; loss: 1.5191853046417236\n",
      "Training epoch 280 ; accuracy: 0.9; loss: 0.1946016550064087\n",
      "Validation epoch 280 ; accuracy: 0.7433333333333333; loss: 1.5204075574874878\n",
      "Training epoch 281 ; accuracy: 0.9; loss: 0.19460289180278778\n",
      "Validation epoch 281 ; accuracy: 0.7433333333333333; loss: 1.5215128660202026\n",
      "Training epoch 282 ; accuracy: 0.9; loss: 0.19460026919841766\n",
      "Validation epoch 282 ; accuracy: 0.7433333333333333; loss: 1.522609829902649\n",
      "Training epoch 283 ; accuracy: 0.9; loss: 0.1946030557155609\n",
      "Validation epoch 283 ; accuracy: 0.7433333333333333; loss: 1.523720145225525\n",
      "Training epoch 284 ; accuracy: 0.9; loss: 0.19460198283195496\n",
      "Validation epoch 284 ; accuracy: 0.7433333333333333; loss: 1.5248254537582397\n",
      "Training epoch 285 ; accuracy: 0.9; loss: 0.19459877908229828\n",
      "Validation epoch 285 ; accuracy: 0.7433333333333333; loss: 1.5258516073226929\n",
      "Training epoch 286 ; accuracy: 0.9; loss: 0.1946008801460266\n",
      "Validation epoch 286 ; accuracy: 0.7433333333333333; loss: 1.5268877744674683\n",
      "Training epoch 287 ; accuracy: 0.9; loss: 0.1946040540933609\n",
      "Validation epoch 287 ; accuracy: 0.7433333333333333; loss: 1.528045415878296\n",
      "Training epoch 288 ; accuracy: 0.9; loss: 0.19459952414035797\n",
      "Validation epoch 288 ; accuracy: 0.7433333333333333; loss: 1.5291635990142822\n",
      "Training epoch 289 ; accuracy: 0.9; loss: 0.19460061192512512\n",
      "Validation epoch 289 ; accuracy: 0.7433333333333333; loss: 1.5302122831344604\n",
      "Training epoch 290 ; accuracy: 0.9; loss: 0.19459885358810425\n",
      "Validation epoch 290 ; accuracy: 0.7433333333333333; loss: 1.5312212705612183\n",
      "Training epoch 291 ; accuracy: 0.9; loss: 0.19460122287273407\n",
      "Validation epoch 291 ; accuracy: 0.7433333333333333; loss: 1.5321787595748901\n",
      "Training epoch 292 ; accuracy: 0.9; loss: 0.1946011334657669\n",
      "Validation epoch 292 ; accuracy: 0.7433333333333333; loss: 1.5331320762634277\n",
      "Training epoch 293 ; accuracy: 0.9; loss: 0.19460104405879974\n",
      "Validation epoch 293 ; accuracy: 0.7433333333333333; loss: 1.5341179370880127\n",
      "Training epoch 294 ; accuracy: 0.9; loss: 0.19459977746009827\n",
      "Validation epoch 294 ; accuracy: 0.7433333333333333; loss: 1.5351247787475586\n",
      "Training epoch 295 ; accuracy: 0.9; loss: 0.1945994794368744\n",
      "Validation epoch 295 ; accuracy: 0.7433333333333333; loss: 1.5360852479934692\n",
      "Training epoch 296 ; accuracy: 0.9; loss: 0.19459912180900574\n",
      "Validation epoch 296 ; accuracy: 0.7433333333333333; loss: 1.5369930267333984\n",
      "Training epoch 297 ; accuracy: 0.9; loss: 0.19460226595401764\n",
      "Validation epoch 297 ; accuracy: 0.7433333333333333; loss: 1.5377520322799683\n",
      "Training epoch 298 ; accuracy: 0.9; loss: 0.19459912180900574\n",
      "Validation epoch 298 ; accuracy: 0.7433333333333333; loss: 1.5385208129882812\n",
      "Training epoch 299 ; accuracy: 0.9; loss: 0.19459958374500275\n",
      "Validation epoch 299 ; accuracy: 0.7433333333333333; loss: 1.5393133163452148\n",
      "Training epoch 300 ; accuracy: 0.9; loss: 0.1946011334657669\n",
      "Validation epoch 300 ; accuracy: 0.7433333333333333; loss: 1.5400915145874023\n",
      "Training epoch 301 ; accuracy: 0.9; loss: 0.1945987492799759\n",
      "Validation epoch 301 ; accuracy: 0.7433333333333333; loss: 1.540885090827942\n",
      "Training epoch 302 ; accuracy: 0.9; loss: 0.1945989727973938\n",
      "Validation epoch 302 ; accuracy: 0.7433333333333333; loss: 1.5417015552520752\n",
      "Training epoch 303 ; accuracy: 0.9; loss: 0.19460006058216095\n",
      "Validation epoch 303 ; accuracy: 0.7433333333333333; loss: 1.5425602197647095\n",
      "Training epoch 304 ; accuracy: 0.9; loss: 0.19459886848926544\n",
      "Validation epoch 304 ; accuracy: 0.7433333333333333; loss: 1.5434225797653198\n",
      "Training epoch 305 ; accuracy: 0.9; loss: 0.19459989666938782\n",
      "Validation epoch 305 ; accuracy: 0.7433333333333333; loss: 1.544267177581787\n",
      "Training epoch 306 ; accuracy: 0.9; loss: 0.19459989666938782\n",
      "Validation epoch 306 ; accuracy: 0.7433333333333333; loss: 1.5450466871261597\n",
      "Training epoch 307 ; accuracy: 0.9; loss: 0.19459888339042664\n",
      "Validation epoch 307 ; accuracy: 0.7433333333333333; loss: 1.5458239316940308\n",
      "Training epoch 308 ; accuracy: 0.9; loss: 0.1945982575416565\n",
      "Validation epoch 308 ; accuracy: 0.7433333333333333; loss: 1.5466290712356567\n",
      "Training epoch 309 ; accuracy: 0.9; loss: 0.1945996731519699\n",
      "Validation epoch 309 ; accuracy: 0.7433333333333333; loss: 1.5474835634231567\n",
      "Training epoch 310 ; accuracy: 0.9; loss: 0.19459915161132812\n",
      "Validation epoch 310 ; accuracy: 0.7433333333333333; loss: 1.548341989517212\n",
      "Training epoch 311 ; accuracy: 0.9; loss: 0.19459860026836395\n",
      "Validation epoch 311 ; accuracy: 0.7433333333333333; loss: 1.5491880178451538\n",
      "Training epoch 312 ; accuracy: 0.9; loss: 0.19459830224514008\n",
      "Validation epoch 312 ; accuracy: 0.7433333333333333; loss: 1.5500274896621704\n",
      "Training epoch 313 ; accuracy: 0.9; loss: 0.19459711015224457\n",
      "Validation epoch 313 ; accuracy: 0.7433333333333333; loss: 1.550836205482483\n",
      "Training epoch 314 ; accuracy: 0.9; loss: 0.19459812343120575\n",
      "Validation epoch 314 ; accuracy: 0.7433333333333333; loss: 1.5516762733459473\n",
      "Training epoch 315 ; accuracy: 0.9; loss: 0.19459739327430725\n",
      "Validation epoch 315 ; accuracy: 0.7433333333333333; loss: 1.5525164604187012\n",
      "Training epoch 316 ; accuracy: 0.9; loss: 0.19459839165210724\n",
      "Validation epoch 316 ; accuracy: 0.7433333333333333; loss: 1.5533287525177002\n",
      "Training epoch 317 ; accuracy: 0.9; loss: 0.19459852576255798\n",
      "Validation epoch 317 ; accuracy: 0.7433333333333333; loss: 1.5541658401489258\n",
      "Training epoch 318 ; accuracy: 0.9; loss: 0.19459772109985352\n",
      "Validation epoch 318 ; accuracy: 0.7433333333333333; loss: 1.5549989938735962\n",
      "Training epoch 319 ; accuracy: 0.9; loss: 0.1945973038673401\n",
      "Validation epoch 319 ; accuracy: 0.7433333333333333; loss: 1.5558278560638428\n",
      "Training epoch 320 ; accuracy: 0.9; loss: 0.19459785521030426\n",
      "Validation epoch 320 ; accuracy: 0.7433333333333333; loss: 1.5566538572311401\n",
      "Training epoch 321 ; accuracy: 0.9; loss: 0.194597989320755\n",
      "Validation epoch 321 ; accuracy: 0.7433333333333333; loss: 1.5574615001678467\n",
      "Training epoch 322 ; accuracy: 0.9; loss: 0.1946016401052475\n",
      "Validation epoch 322 ; accuracy: 0.7433333333333333; loss: 1.5585060119628906\n",
      "Training epoch 323 ; accuracy: 0.9; loss: 0.19459764659404755\n",
      "Validation epoch 323 ; accuracy: 0.7433333333333333; loss: 1.5595415830612183\n",
      "Training epoch 324 ; accuracy: 0.9; loss: 0.19459757208824158\n",
      "Validation epoch 324 ; accuracy: 0.7433333333333333; loss: 1.5605393648147583\n",
      "Training epoch 325 ; accuracy: 0.9; loss: 0.194597527384758\n",
      "Validation epoch 325 ; accuracy: 0.7433333333333333; loss: 1.5614873170852661\n",
      "Training epoch 326 ; accuracy: 0.9; loss: 0.19459734857082367\n",
      "Validation epoch 326 ; accuracy: 0.7433333333333333; loss: 1.562408208847046\n",
      "Training epoch 327 ; accuracy: 0.9; loss: 0.19459788501262665\n",
      "Validation epoch 327 ; accuracy: 0.7433333333333333; loss: 1.5633240938186646\n",
      "Training epoch 328 ; accuracy: 0.9; loss: 0.19459787011146545\n",
      "Validation epoch 328 ; accuracy: 0.7433333333333333; loss: 1.5642409324645996\n",
      "Training epoch 329 ; accuracy: 0.9; loss: 0.19459812343120575\n",
      "Validation epoch 329 ; accuracy: 0.7433333333333333; loss: 1.565155029296875\n",
      "Training epoch 330 ; accuracy: 0.9; loss: 0.19459789991378784\n",
      "Validation epoch 330 ; accuracy: 0.7433333333333333; loss: 1.5660223960876465\n",
      "Training epoch 331 ; accuracy: 0.9; loss: 0.19459661841392517\n",
      "Validation epoch 331 ; accuracy: 0.7433333333333333; loss: 1.566879391670227\n",
      "Training epoch 332 ; accuracy: 0.9; loss: 0.19459684193134308\n",
      "Validation epoch 332 ; accuracy: 0.7433333333333333; loss: 1.5677233934402466\n",
      "Training epoch 333 ; accuracy: 0.9; loss: 0.19459854066371918\n",
      "Validation epoch 333 ; accuracy: 0.7433333333333333; loss: 1.5685827732086182\n",
      "Training epoch 334 ; accuracy: 0.9; loss: 0.1945977360010147\n",
      "Validation epoch 334 ; accuracy: 0.7433333333333333; loss: 1.5694280862808228\n",
      "Training epoch 335 ; accuracy: 0.9; loss: 0.19459779560565948\n",
      "Validation epoch 335 ; accuracy: 0.7433333333333333; loss: 1.5702762603759766\n",
      "Training epoch 336 ; accuracy: 0.9; loss: 0.19459818303585052\n",
      "Validation epoch 336 ; accuracy: 0.7433333333333333; loss: 1.571083664894104\n",
      "Training epoch 337 ; accuracy: 0.9; loss: 0.1945965737104416\n",
      "Validation epoch 337 ; accuracy: 0.7433333333333333; loss: 1.5718927383422852\n",
      "Training epoch 338 ; accuracy: 0.9; loss: 0.19459731876850128\n",
      "Validation epoch 338 ; accuracy: 0.7433333333333333; loss: 1.5727025270462036\n",
      "Training epoch 339 ; accuracy: 0.9; loss: 0.19459763169288635\n",
      "Validation epoch 339 ; accuracy: 0.7433333333333333; loss: 1.5734986066818237\n",
      "Training epoch 340 ; accuracy: 0.9; loss: 0.19459614157676697\n",
      "Validation epoch 340 ; accuracy: 0.7433333333333333; loss: 1.5742756128311157\n",
      "Training epoch 341 ; accuracy: 0.9; loss: 0.19459766149520874\n",
      "Validation epoch 341 ; accuracy: 0.7433333333333333; loss: 1.5750508308410645\n",
      "Training epoch 342 ; accuracy: 0.9; loss: 0.1945970505475998\n",
      "Validation epoch 342 ; accuracy: 0.7433333333333333; loss: 1.575801134109497\n",
      "Training epoch 343 ; accuracy: 0.9; loss: 0.19459682703018188\n",
      "Validation epoch 343 ; accuracy: 0.7433333333333333; loss: 1.5765399932861328\n",
      "Training epoch 344 ; accuracy: 0.9; loss: 0.19459687173366547\n",
      "Validation epoch 344 ; accuracy: 0.7433333333333333; loss: 1.5772590637207031\n",
      "Training epoch 345 ; accuracy: 0.9; loss: 0.19459664821624756\n",
      "Validation epoch 345 ; accuracy: 0.7433333333333333; loss: 1.577973484992981\n",
      "Training epoch 346 ; accuracy: 0.9; loss: 0.19459865987300873\n",
      "Validation epoch 346 ; accuracy: 0.7433333333333333; loss: 1.5786124467849731\n",
      "Training epoch 347 ; accuracy: 0.9; loss: 0.19459769129753113\n",
      "Validation epoch 347 ; accuracy: 0.7433333333333333; loss: 1.5791819095611572\n",
      "Training epoch 348 ; accuracy: 0.9; loss: 0.19459731876850128\n",
      "Validation epoch 348 ; accuracy: 0.7433333333333333; loss: 1.5797275304794312\n",
      "Training epoch 349 ; accuracy: 0.9; loss: 0.1945980042219162\n",
      "Validation epoch 349 ; accuracy: 0.7433333333333333; loss: 1.5803524255752563\n",
      "Training epoch 350 ; accuracy: 0.9; loss: 0.19459742307662964\n",
      "Validation epoch 350 ; accuracy: 0.7433333333333333; loss: 1.5809781551361084\n",
      "Training epoch 351 ; accuracy: 0.9; loss: 0.19459684193134308\n",
      "Validation epoch 351 ; accuracy: 0.7433333333333333; loss: 1.581627368927002\n",
      "Training epoch 352 ; accuracy: 0.9; loss: 0.1945982426404953\n",
      "Validation epoch 352 ; accuracy: 0.7433333333333333; loss: 1.582304835319519\n",
      "Training epoch 353 ; accuracy: 0.9; loss: 0.1945962756872177\n",
      "Validation epoch 353 ; accuracy: 0.7433333333333333; loss: 1.5829589366912842\n",
      "Training epoch 354 ; accuracy: 0.9; loss: 0.19459624588489532\n",
      "Validation epoch 354 ; accuracy: 0.7433333333333333; loss: 1.5836222171783447\n",
      "Training epoch 355 ; accuracy: 0.9; loss: 0.19459731876850128\n",
      "Validation epoch 355 ; accuracy: 0.7433333333333333; loss: 1.5842976570129395\n",
      "Training epoch 356 ; accuracy: 0.9; loss: 0.19459684193134308\n",
      "Validation epoch 356 ; accuracy: 0.7433333333333333; loss: 1.5849522352218628\n",
      "Training epoch 357 ; accuracy: 0.9; loss: 0.19459660351276398\n",
      "Validation epoch 357 ; accuracy: 0.7433333333333333; loss: 1.5856221914291382\n",
      "Training epoch 358 ; accuracy: 0.9; loss: 0.19459667801856995\n",
      "Validation epoch 358 ; accuracy: 0.7433333333333333; loss: 1.5862895250320435\n",
      "Training epoch 359 ; accuracy: 0.9; loss: 0.19459573924541473\n",
      "Validation epoch 359 ; accuracy: 0.7433333333333333; loss: 1.586942195892334\n",
      "Training epoch 360 ; accuracy: 0.9; loss: 0.1945965439081192\n",
      "Validation epoch 360 ; accuracy: 0.7433333333333333; loss: 1.58758544921875\n",
      "Training epoch 361 ; accuracy: 0.9; loss: 0.19459615647792816\n",
      "Validation epoch 361 ; accuracy: 0.7433333333333333; loss: 1.588196873664856\n",
      "Training epoch 362 ; accuracy: 0.9; loss: 0.1945965588092804\n",
      "Validation epoch 362 ; accuracy: 0.7433333333333333; loss: 1.5888057947158813\n",
      "Training epoch 363 ; accuracy: 0.9; loss: 0.19459626078605652\n",
      "Validation epoch 363 ; accuracy: 0.7433333333333333; loss: 1.589415192604065\n",
      "Training epoch 364 ; accuracy: 0.9; loss: 0.19459670782089233\n",
      "Validation epoch 364 ; accuracy: 0.7433333333333333; loss: 1.5900390148162842\n",
      "Training epoch 365 ; accuracy: 0.9; loss: 0.19459688663482666\n",
      "Validation epoch 365 ; accuracy: 0.7433333333333333; loss: 1.5906811952590942\n",
      "Training epoch 366 ; accuracy: 0.9; loss: 0.19459640979766846\n",
      "Validation epoch 366 ; accuracy: 0.7433333333333333; loss: 1.5913100242614746\n",
      "Training epoch 367 ; accuracy: 0.9; loss: 0.19459573924541473\n",
      "Validation epoch 367 ; accuracy: 0.7433333333333333; loss: 1.5919206142425537\n",
      "Training epoch 368 ; accuracy: 0.9; loss: 0.19459663331508636\n",
      "Validation epoch 368 ; accuracy: 0.7433333333333333; loss: 1.5926052331924438\n",
      "Training epoch 369 ; accuracy: 0.9; loss: 0.194596529006958\n",
      "Validation epoch 369 ; accuracy: 0.7433333333333333; loss: 1.5933154821395874\n",
      "Training epoch 370 ; accuracy: 0.9; loss: 0.19459648430347443\n",
      "Validation epoch 370 ; accuracy: 0.7433333333333333; loss: 1.59400475025177\n",
      "Training epoch 371 ; accuracy: 0.9; loss: 0.19459590315818787\n",
      "Validation epoch 371 ; accuracy: 0.7433333333333333; loss: 1.5946978330612183\n",
      "Training epoch 372 ; accuracy: 0.9; loss: 0.19459573924541473\n",
      "Validation epoch 372 ; accuracy: 0.7433333333333333; loss: 1.5953532457351685\n",
      "Training epoch 373 ; accuracy: 0.9; loss: 0.1945962756872177\n",
      "Validation epoch 373 ; accuracy: 0.7433333333333333; loss: 1.5959994792938232\n",
      "Training epoch 374 ; accuracy: 0.9; loss: 0.19459562003612518\n",
      "Validation epoch 374 ; accuracy: 0.7433333333333333; loss: 1.5966238975524902\n",
      "Training epoch 375 ; accuracy: 0.9; loss: 0.19459602236747742\n",
      "Validation epoch 375 ; accuracy: 0.7433333333333333; loss: 1.5972785949707031\n",
      "Training epoch 376 ; accuracy: 0.9; loss: 0.1945963203907013\n",
      "Validation epoch 376 ; accuracy: 0.7433333333333333; loss: 1.5979467630386353\n",
      "Training epoch 377 ; accuracy: 0.9; loss: 0.1945965588092804\n",
      "Validation epoch 377 ; accuracy: 0.7433333333333333; loss: 1.5986100435256958\n",
      "Training epoch 378 ; accuracy: 0.9; loss: 0.1945953071117401\n",
      "Validation epoch 378 ; accuracy: 0.7433333333333333; loss: 1.599247694015503\n",
      "Training epoch 379 ; accuracy: 0.9; loss: 0.19459590315818787\n",
      "Validation epoch 379 ; accuracy: 0.7433333333333333; loss: 1.5998727083206177\n",
      "Training epoch 380 ; accuracy: 0.9; loss: 0.19459615647792816\n",
      "Validation epoch 380 ; accuracy: 0.7433333333333333; loss: 1.6004693508148193\n",
      "Training epoch 381 ; accuracy: 0.9; loss: 0.19459617137908936\n",
      "Validation epoch 381 ; accuracy: 0.7433333333333333; loss: 1.6010512113571167\n",
      "Training epoch 382 ; accuracy: 0.9; loss: 0.1945950984954834\n",
      "Validation epoch 382 ; accuracy: 0.7433333333333333; loss: 1.601617455482483\n",
      "Training epoch 383 ; accuracy: 0.9; loss: 0.19459694623947144\n",
      "Validation epoch 383 ; accuracy: 0.7433333333333333; loss: 1.602202296257019\n",
      "Training epoch 384 ; accuracy: 0.9; loss: 0.19459550082683563\n",
      "Validation epoch 384 ; accuracy: 0.7433333333333333; loss: 1.6027661561965942\n",
      "Training epoch 385 ; accuracy: 0.9; loss: 0.19459541141986847\n",
      "Validation epoch 385 ; accuracy: 0.7433333333333333; loss: 1.6033458709716797\n",
      "Training epoch 386 ; accuracy: 0.9; loss: 0.1945953220129013\n",
      "Validation epoch 386 ; accuracy: 0.7433333333333333; loss: 1.6039245128631592\n",
      "Training epoch 387 ; accuracy: 0.9; loss: 0.1945955455303192\n",
      "Validation epoch 387 ; accuracy: 0.7433333333333333; loss: 1.604491114616394\n",
      "Training epoch 388 ; accuracy: 0.9; loss: 0.19459545612335205\n",
      "Validation epoch 388 ; accuracy: 0.7433333333333333; loss: 1.6050554513931274\n",
      "Training epoch 389 ; accuracy: 0.9; loss: 0.19459569454193115\n",
      "Validation epoch 389 ; accuracy: 0.7433333333333333; loss: 1.6056135892868042\n",
      "Training epoch 390 ; accuracy: 0.9; loss: 0.19459639489650726\n",
      "Validation epoch 390 ; accuracy: 0.7433333333333333; loss: 1.606174349784851\n",
      "Training epoch 391 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 391 ; accuracy: 0.7433333333333333; loss: 1.6067184209823608\n",
      "Training epoch 392 ; accuracy: 0.9; loss: 0.19459719955921173\n",
      "Validation epoch 392 ; accuracy: 0.7433333333333333; loss: 1.6071665287017822\n",
      "Training epoch 393 ; accuracy: 0.9; loss: 0.194596529006958\n",
      "Validation epoch 393 ; accuracy: 0.7433333333333333; loss: 1.6076620817184448\n",
      "Training epoch 394 ; accuracy: 0.9; loss: 0.19459551572799683\n",
      "Validation epoch 394 ; accuracy: 0.7433333333333333; loss: 1.6081589460372925\n",
      "Training epoch 395 ; accuracy: 0.9; loss: 0.19459621608257294\n",
      "Validation epoch 395 ; accuracy: 0.7433333333333333; loss: 1.6086472272872925\n",
      "Training epoch 396 ; accuracy: 0.9; loss: 0.19459626078605652\n",
      "Validation epoch 396 ; accuracy: 0.7433333333333333; loss: 1.6091375350952148\n",
      "Training epoch 397 ; accuracy: 0.9; loss: 0.1945953667163849\n",
      "Validation epoch 397 ; accuracy: 0.7433333333333333; loss: 1.6096271276474\n",
      "Training epoch 398 ; accuracy: 0.9; loss: 0.1945960521697998\n",
      "Validation epoch 398 ; accuracy: 0.7433333333333333; loss: 1.6101187467575073\n",
      "Training epoch 399 ; accuracy: 0.9; loss: 0.19459611177444458\n",
      "Validation epoch 399 ; accuracy: 0.7433333333333333; loss: 1.6106278896331787\n",
      "Training epoch 400 ; accuracy: 0.9; loss: 0.19459575414657593\n",
      "Validation epoch 400 ; accuracy: 0.7433333333333333; loss: 1.6111512184143066\n",
      "Training epoch 401 ; accuracy: 0.9; loss: 0.19459637999534607\n",
      "Validation epoch 401 ; accuracy: 0.7433333333333333; loss: 1.6117000579833984\n",
      "Training epoch 402 ; accuracy: 0.9; loss: 0.19459521770477295\n",
      "Validation epoch 402 ; accuracy: 0.7433333333333333; loss: 1.6122487783432007\n",
      "Training epoch 403 ; accuracy: 0.9; loss: 0.19459600746631622\n",
      "Validation epoch 403 ; accuracy: 0.7433333333333333; loss: 1.6127933263778687\n",
      "Training epoch 404 ; accuracy: 0.9; loss: 0.1945953518152237\n",
      "Validation epoch 404 ; accuracy: 0.7433333333333333; loss: 1.6133455038070679\n",
      "Training epoch 405 ; accuracy: 0.9; loss: 0.19459564983844757\n",
      "Validation epoch 405 ; accuracy: 0.7433333333333333; loss: 1.6138339042663574\n",
      "Training epoch 406 ; accuracy: 0.9; loss: 0.19459587335586548\n",
      "Validation epoch 406 ; accuracy: 0.7433333333333333; loss: 1.6143198013305664\n",
      "Training epoch 407 ; accuracy: 0.9; loss: 0.19459499418735504\n",
      "Validation epoch 407 ; accuracy: 0.7433333333333333; loss: 1.6148213148117065\n",
      "Training epoch 408 ; accuracy: 0.9; loss: 0.19459515810012817\n",
      "Validation epoch 408 ; accuracy: 0.7433333333333333; loss: 1.6153093576431274\n",
      "Training epoch 409 ; accuracy: 0.9; loss: 0.19459499418735504\n",
      "Validation epoch 409 ; accuracy: 0.7433333333333333; loss: 1.6158034801483154\n",
      "Training epoch 410 ; accuracy: 0.9; loss: 0.194596067070961\n",
      "Validation epoch 410 ; accuracy: 0.7433333333333333; loss: 1.6163015365600586\n",
      "Training epoch 411 ; accuracy: 0.9; loss: 0.1945950984954834\n",
      "Validation epoch 411 ; accuracy: 0.7433333333333333; loss: 1.6168065071105957\n",
      "Training epoch 412 ; accuracy: 0.9; loss: 0.19459478557109833\n",
      "Validation epoch 412 ; accuracy: 0.7433333333333333; loss: 1.6173001527786255\n",
      "Training epoch 413 ; accuracy: 0.9; loss: 0.19459624588489532\n",
      "Validation epoch 413 ; accuracy: 0.7433333333333333; loss: 1.6178035736083984\n",
      "Training epoch 414 ; accuracy: 0.9; loss: 0.19459448754787445\n",
      "Validation epoch 414 ; accuracy: 0.7433333333333333; loss: 1.618296504020691\n",
      "Training epoch 415 ; accuracy: 0.9; loss: 0.19459542632102966\n",
      "Validation epoch 415 ; accuracy: 0.7433333333333333; loss: 1.6187875270843506\n",
      "Training epoch 416 ; accuracy: 0.9; loss: 0.19459517300128937\n",
      "Validation epoch 416 ; accuracy: 0.7433333333333333; loss: 1.6192741394042969\n",
      "Training epoch 417 ; accuracy: 0.9; loss: 0.19459478557109833\n",
      "Validation epoch 417 ; accuracy: 0.7433333333333333; loss: 1.619768738746643\n",
      "Training epoch 418 ; accuracy: 0.9; loss: 0.19459611177444458\n",
      "Validation epoch 418 ; accuracy: 0.7433333333333333; loss: 1.6202818155288696\n",
      "Training epoch 419 ; accuracy: 0.9; loss: 0.19459518790245056\n",
      "Validation epoch 419 ; accuracy: 0.7433333333333333; loss: 1.6207871437072754\n",
      "Training epoch 420 ; accuracy: 0.9; loss: 0.19459505379199982\n",
      "Validation epoch 420 ; accuracy: 0.7433333333333333; loss: 1.6212674379348755\n",
      "Training epoch 421 ; accuracy: 0.9; loss: 0.19459517300128937\n",
      "Validation epoch 421 ; accuracy: 0.7433333333333333; loss: 1.621703028678894\n",
      "Training epoch 422 ; accuracy: 0.9; loss: 0.1945955753326416\n",
      "Validation epoch 422 ; accuracy: 0.7433333333333333; loss: 1.6221935749053955\n",
      "Training epoch 423 ; accuracy: 0.9; loss: 0.19459542632102966\n",
      "Validation epoch 423 ; accuracy: 0.7433333333333333; loss: 1.6226922273635864\n",
      "Training epoch 424 ; accuracy: 0.9; loss: 0.19459471106529236\n",
      "Validation epoch 424 ; accuracy: 0.7433333333333333; loss: 1.6231855154037476\n",
      "Training epoch 425 ; accuracy: 0.9; loss: 0.19459578394889832\n",
      "Validation epoch 425 ; accuracy: 0.7433333333333333; loss: 1.6236904859542847\n",
      "Training epoch 426 ; accuracy: 0.9; loss: 0.19459529221057892\n",
      "Validation epoch 426 ; accuracy: 0.7433333333333333; loss: 1.624200701713562\n",
      "Training epoch 427 ; accuracy: 0.9; loss: 0.1945955902338028\n",
      "Validation epoch 427 ; accuracy: 0.7433333333333333; loss: 1.6247180700302124\n",
      "Training epoch 428 ; accuracy: 0.9; loss: 0.19459454715251923\n",
      "Validation epoch 428 ; accuracy: 0.7433333333333333; loss: 1.6252388954162598\n",
      "Training epoch 429 ; accuracy: 0.9; loss: 0.1945948749780655\n",
      "Validation epoch 429 ; accuracy: 0.7433333333333333; loss: 1.6257511377334595\n",
      "Training epoch 430 ; accuracy: 0.9; loss: 0.19459664821624756\n",
      "Validation epoch 430 ; accuracy: 0.7433333333333333; loss: 1.6263759136199951\n",
      "Training epoch 431 ; accuracy: 0.9; loss: 0.1945950835943222\n",
      "Validation epoch 431 ; accuracy: 0.7433333333333333; loss: 1.6270090341567993\n",
      "Training epoch 432 ; accuracy: 0.9; loss: 0.1945948302745819\n",
      "Validation epoch 432 ; accuracy: 0.7433333333333333; loss: 1.62763512134552\n",
      "Training epoch 433 ; accuracy: 0.9; loss: 0.19459526240825653\n",
      "Validation epoch 433 ; accuracy: 0.7433333333333333; loss: 1.6282414197921753\n",
      "Training epoch 434 ; accuracy: 0.9; loss: 0.19459541141986847\n",
      "Validation epoch 434 ; accuracy: 0.7433333333333333; loss: 1.628879189491272\n",
      "Training epoch 435 ; accuracy: 0.9; loss: 0.19459477066993713\n",
      "Validation epoch 435 ; accuracy: 0.7433333333333333; loss: 1.6294986009597778\n",
      "Training epoch 436 ; accuracy: 0.9; loss: 0.19459469616413116\n",
      "Validation epoch 436 ; accuracy: 0.7433333333333333; loss: 1.6300889253616333\n",
      "Training epoch 437 ; accuracy: 0.9; loss: 0.19459448754787445\n",
      "Validation epoch 437 ; accuracy: 0.7433333333333333; loss: 1.6306614875793457\n",
      "Training epoch 438 ; accuracy: 0.9; loss: 0.1945951133966446\n",
      "Validation epoch 438 ; accuracy: 0.7433333333333333; loss: 1.6312087774276733\n",
      "Training epoch 439 ; accuracy: 0.9; loss: 0.19459407031536102\n",
      "Validation epoch 439 ; accuracy: 0.7433333333333333; loss: 1.631736397743225\n",
      "Training epoch 440 ; accuracy: 0.9; loss: 0.1945946216583252\n",
      "Validation epoch 440 ; accuracy: 0.7433333333333333; loss: 1.6322311162948608\n",
      "Training epoch 441 ; accuracy: 0.9; loss: 0.19459448754787445\n",
      "Validation epoch 441 ; accuracy: 0.7433333333333333; loss: 1.6327096223831177\n",
      "Training epoch 442 ; accuracy: 0.9; loss: 0.19459515810012817\n",
      "Validation epoch 442 ; accuracy: 0.7433333333333333; loss: 1.633178949356079\n",
      "Training epoch 443 ; accuracy: 0.9; loss: 0.19459445774555206\n",
      "Validation epoch 443 ; accuracy: 0.7433333333333333; loss: 1.6336534023284912\n",
      "Training epoch 444 ; accuracy: 0.9; loss: 0.19459497928619385\n",
      "Validation epoch 444 ; accuracy: 0.7433333333333333; loss: 1.634132742881775\n",
      "Training epoch 445 ; accuracy: 0.9; loss: 0.1945955753326416\n",
      "Validation epoch 445 ; accuracy: 0.7433333333333333; loss: 1.6346665620803833\n",
      "Training epoch 446 ; accuracy: 0.9; loss: 0.194595068693161\n",
      "Validation epoch 446 ; accuracy: 0.7433333333333333; loss: 1.6351702213287354\n",
      "Training epoch 447 ; accuracy: 0.9; loss: 0.1945950984954834\n",
      "Validation epoch 447 ; accuracy: 0.7433333333333333; loss: 1.6356550455093384\n",
      "Training epoch 448 ; accuracy: 0.9; loss: 0.194594606757164\n",
      "Validation epoch 448 ; accuracy: 0.7433333333333333; loss: 1.6361490488052368\n",
      "Training epoch 449 ; accuracy: 0.9; loss: 0.1945951282978058\n",
      "Validation epoch 449 ; accuracy: 0.7433333333333333; loss: 1.6365468502044678\n",
      "Training epoch 450 ; accuracy: 0.9; loss: 0.1945953518152237\n",
      "Validation epoch 450 ; accuracy: 0.7433333333333333; loss: 1.6369473934173584\n",
      "Training epoch 451 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 451 ; accuracy: 0.7433333333333333; loss: 1.6373388767242432\n",
      "Training epoch 452 ; accuracy: 0.9; loss: 0.19459480047225952\n",
      "Validation epoch 452 ; accuracy: 0.7433333333333333; loss: 1.6377184391021729\n",
      "Training epoch 453 ; accuracy: 0.9; loss: 0.1945943683385849\n",
      "Validation epoch 453 ; accuracy: 0.7433333333333333; loss: 1.638067364692688\n",
      "Training epoch 454 ; accuracy: 0.9; loss: 0.19459573924541473\n",
      "Validation epoch 454 ; accuracy: 0.7433333333333333; loss: 1.638342261314392\n",
      "Training epoch 455 ; accuracy: 0.9; loss: 0.1945941001176834\n",
      "Validation epoch 455 ; accuracy: 0.7433333333333333; loss: 1.63864266872406\n",
      "Training epoch 456 ; accuracy: 0.9; loss: 0.1945943385362625\n",
      "Validation epoch 456 ; accuracy: 0.7433333333333333; loss: 1.6389570236206055\n",
      "Training epoch 457 ; accuracy: 0.9; loss: 0.1945943385362625\n",
      "Validation epoch 457 ; accuracy: 0.7433333333333333; loss: 1.6392617225646973\n",
      "Training epoch 458 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 458 ; accuracy: 0.7433333333333333; loss: 1.6395601034164429\n",
      "Training epoch 459 ; accuracy: 0.9; loss: 0.1945938915014267\n",
      "Validation epoch 459 ; accuracy: 0.7433333333333333; loss: 1.639867901802063\n",
      "Training epoch 460 ; accuracy: 0.9; loss: 0.19459526240825653\n",
      "Validation epoch 460 ; accuracy: 0.7433333333333333; loss: 1.640139102935791\n",
      "Training epoch 461 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 461 ; accuracy: 0.7433333333333333; loss: 1.6404284238815308\n",
      "Training epoch 462 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 462 ; accuracy: 0.7433333333333333; loss: 1.6407526731491089\n",
      "Training epoch 463 ; accuracy: 0.9; loss: 0.1945945918560028\n",
      "Validation epoch 463 ; accuracy: 0.7433333333333333; loss: 1.641072154045105\n",
      "Training epoch 464 ; accuracy: 0.9; loss: 0.19459523260593414\n",
      "Validation epoch 464 ; accuracy: 0.7433333333333333; loss: 1.6413953304290771\n",
      "Training epoch 465 ; accuracy: 0.9; loss: 0.1945948451757431\n",
      "Validation epoch 465 ; accuracy: 0.7433333333333333; loss: 1.6417316198349\n",
      "Training epoch 466 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 466 ; accuracy: 0.7433333333333333; loss: 1.642082691192627\n",
      "Training epoch 467 ; accuracy: 0.9; loss: 0.1945945918560028\n",
      "Validation epoch 467 ; accuracy: 0.7433333333333333; loss: 1.6424474716186523\n",
      "Training epoch 468 ; accuracy: 0.9; loss: 0.19459478557109833\n",
      "Validation epoch 468 ; accuracy: 0.7433333333333333; loss: 1.642837405204773\n",
      "Training epoch 469 ; accuracy: 0.9; loss: 0.19459587335586548\n",
      "Validation epoch 469 ; accuracy: 0.7433333333333333; loss: 1.643311858177185\n",
      "Training epoch 470 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 470 ; accuracy: 0.7433333333333333; loss: 1.6437702178955078\n",
      "Training epoch 471 ; accuracy: 0.9; loss: 0.1945941001176834\n",
      "Validation epoch 471 ; accuracy: 0.7433333333333333; loss: 1.6442419290542603\n",
      "Training epoch 472 ; accuracy: 0.9; loss: 0.19459417462348938\n",
      "Validation epoch 472 ; accuracy: 0.7433333333333333; loss: 1.6446961164474487\n",
      "Training epoch 473 ; accuracy: 0.9; loss: 0.1945965737104416\n",
      "Validation epoch 473 ; accuracy: 0.7433333333333333; loss: 1.6453099250793457\n",
      "Training epoch 474 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 474 ; accuracy: 0.7433333333333333; loss: 1.645909070968628\n",
      "Training epoch 475 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 475 ; accuracy: 0.7433333333333333; loss: 1.646475911140442\n",
      "Training epoch 476 ; accuracy: 0.9; loss: 0.19459466636180878\n",
      "Validation epoch 476 ; accuracy: 0.7433333333333333; loss: 1.6470288038253784\n",
      "Training epoch 477 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 477 ; accuracy: 0.7433333333333333; loss: 1.647546648979187\n",
      "Training epoch 478 ; accuracy: 0.9; loss: 0.19459417462348938\n",
      "Validation epoch 478 ; accuracy: 0.7433333333333333; loss: 1.6480658054351807\n",
      "Training epoch 479 ; accuracy: 0.9; loss: 0.1945943981409073\n",
      "Validation epoch 479 ; accuracy: 0.7433333333333333; loss: 1.648569107055664\n",
      "Training epoch 480 ; accuracy: 0.9; loss: 0.19459392130374908\n",
      "Validation epoch 480 ; accuracy: 0.7433333333333333; loss: 1.6490678787231445\n",
      "Training epoch 481 ; accuracy: 0.9; loss: 0.19459445774555206\n",
      "Validation epoch 481 ; accuracy: 0.7433333333333333; loss: 1.6495524644851685\n",
      "Training epoch 482 ; accuracy: 0.9; loss: 0.1945939064025879\n",
      "Validation epoch 482 ; accuracy: 0.7433333333333333; loss: 1.650011658668518\n",
      "Training epoch 483 ; accuracy: 0.9; loss: 0.1945941299200058\n",
      "Validation epoch 483 ; accuracy: 0.7433333333333333; loss: 1.650458812713623\n",
      "Training epoch 484 ; accuracy: 0.9; loss: 0.19459401071071625\n",
      "Validation epoch 484 ; accuracy: 0.7433333333333333; loss: 1.650909185409546\n",
      "Training epoch 485 ; accuracy: 0.9; loss: 0.19459415972232819\n",
      "Validation epoch 485 ; accuracy: 0.7433333333333333; loss: 1.651343822479248\n",
      "Training epoch 486 ; accuracy: 0.9; loss: 0.19459515810012817\n",
      "Validation epoch 486 ; accuracy: 0.7433333333333333; loss: 1.6517709493637085\n",
      "Training epoch 487 ; accuracy: 0.9; loss: 0.19459392130374908\n",
      "Validation epoch 487 ; accuracy: 0.7433333333333333; loss: 1.6521943807601929\n",
      "Training epoch 488 ; accuracy: 0.9; loss: 0.1945958286523819\n",
      "Validation epoch 488 ; accuracy: 0.7433333333333333; loss: 1.6526641845703125\n",
      "Training epoch 489 ; accuracy: 0.9; loss: 0.1945941001176834\n",
      "Validation epoch 489 ; accuracy: 0.7433333333333333; loss: 1.6531267166137695\n",
      "Training epoch 490 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 490 ; accuracy: 0.7433333333333333; loss: 1.6535745859146118\n",
      "Training epoch 491 ; accuracy: 0.9; loss: 0.1945941001176834\n",
      "Validation epoch 491 ; accuracy: 0.7433333333333333; loss: 1.654032588005066\n",
      "Training epoch 492 ; accuracy: 0.9; loss: 0.19459404051303864\n",
      "Validation epoch 492 ; accuracy: 0.7433333333333333; loss: 1.6544734239578247\n",
      "Training epoch 493 ; accuracy: 0.9; loss: 0.1945946216583252\n",
      "Validation epoch 493 ; accuracy: 0.7433333333333333; loss: 1.6548916101455688\n",
      "Training epoch 494 ; accuracy: 0.9; loss: 0.19459407031536102\n",
      "Validation epoch 494 ; accuracy: 0.7433333333333333; loss: 1.655310034751892\n",
      "Training epoch 495 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 495 ; accuracy: 0.7433333333333333; loss: 1.6557289361953735\n",
      "Training epoch 496 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 496 ; accuracy: 0.7433333333333333; loss: 1.6561344861984253\n",
      "Training epoch 497 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 497 ; accuracy: 0.7433333333333333; loss: 1.6565375328063965\n",
      "Training epoch 498 ; accuracy: 0.9; loss: 0.19459466636180878\n",
      "Validation epoch 498 ; accuracy: 0.7433333333333333; loss: 1.6569201946258545\n",
      "Training epoch 499 ; accuracy: 0.9; loss: 0.19459396600723267\n",
      "Validation epoch 499 ; accuracy: 0.7433333333333333; loss: 1.6573185920715332\n",
      "Training epoch 500 ; accuracy: 0.9; loss: 0.19459405541419983\n",
      "Validation epoch 500 ; accuracy: 0.7433333333333333; loss: 1.6576964855194092\n",
      "Training epoch 501 ; accuracy: 0.9; loss: 0.19459426403045654\n",
      "Validation epoch 501 ; accuracy: 0.7433333333333333; loss: 1.658046007156372\n",
      "Training epoch 502 ; accuracy: 0.9; loss: 0.19459432363510132\n",
      "Validation epoch 502 ; accuracy: 0.7433333333333333; loss: 1.6584032773971558\n",
      "Training epoch 503 ; accuracy: 0.9; loss: 0.19459372758865356\n",
      "Validation epoch 503 ; accuracy: 0.7433333333333333; loss: 1.6587579250335693\n",
      "Training epoch 504 ; accuracy: 0.9; loss: 0.19459368288516998\n",
      "Validation epoch 504 ; accuracy: 0.7433333333333333; loss: 1.6591212749481201\n",
      "Training epoch 505 ; accuracy: 0.9; loss: 0.1945943385362625\n",
      "Validation epoch 505 ; accuracy: 0.7433333333333333; loss: 1.6595057249069214\n",
      "Training epoch 506 ; accuracy: 0.9; loss: 0.19459396600723267\n",
      "Validation epoch 506 ; accuracy: 0.7433333333333333; loss: 1.6599113941192627\n",
      "Training epoch 507 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 507 ; accuracy: 0.7433333333333333; loss: 1.6603343486785889\n",
      "Training epoch 508 ; accuracy: 0.9; loss: 0.1945939064025879\n",
      "Validation epoch 508 ; accuracy: 0.7433333333333333; loss: 1.6607533693313599\n",
      "Training epoch 509 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 509 ; accuracy: 0.7433333333333333; loss: 1.661169171333313\n",
      "Training epoch 510 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 510 ; accuracy: 0.7433333333333333; loss: 1.6615686416625977\n",
      "Training epoch 511 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 511 ; accuracy: 0.7433333333333333; loss: 1.661930799484253\n",
      "Training epoch 512 ; accuracy: 0.9; loss: 0.1945936381816864\n",
      "Validation epoch 512 ; accuracy: 0.7433333333333333; loss: 1.6622618436813354\n",
      "Training epoch 513 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 513 ; accuracy: 0.7433333333333333; loss: 1.6625925302505493\n",
      "Training epoch 514 ; accuracy: 0.9; loss: 0.19459345936775208\n",
      "Validation epoch 514 ; accuracy: 0.7433333333333333; loss: 1.6629302501678467\n",
      "Training epoch 515 ; accuracy: 0.9; loss: 0.19459371268749237\n",
      "Validation epoch 515 ; accuracy: 0.7433333333333333; loss: 1.6632590293884277\n",
      "Training epoch 516 ; accuracy: 0.9; loss: 0.19459351897239685\n",
      "Validation epoch 516 ; accuracy: 0.7433333333333333; loss: 1.6635819673538208\n",
      "Training epoch 517 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 517 ; accuracy: 0.7433333333333333; loss: 1.6638928651809692\n",
      "Training epoch 518 ; accuracy: 0.9; loss: 0.19459378719329834\n",
      "Validation epoch 518 ; accuracy: 0.7433333333333333; loss: 1.6641939878463745\n",
      "Training epoch 519 ; accuracy: 0.9; loss: 0.19459456205368042\n",
      "Validation epoch 519 ; accuracy: 0.7433333333333333; loss: 1.6645137071609497\n",
      "Training epoch 520 ; accuracy: 0.9; loss: 0.19459380209445953\n",
      "Validation epoch 520 ; accuracy: 0.7433333333333333; loss: 1.664816975593567\n",
      "Training epoch 521 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 521 ; accuracy: 0.7433333333333333; loss: 1.6651196479797363\n",
      "Training epoch 522 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 522 ; accuracy: 0.7433333333333333; loss: 1.6654243469238281\n",
      "Training epoch 523 ; accuracy: 0.9; loss: 0.19459378719329834\n",
      "Validation epoch 523 ; accuracy: 0.7433333333333333; loss: 1.6657055616378784\n",
      "Training epoch 524 ; accuracy: 0.9; loss: 0.1945941150188446\n",
      "Validation epoch 524 ; accuracy: 0.7433333333333333; loss: 1.666019320487976\n",
      "Training epoch 525 ; accuracy: 0.9; loss: 0.19459426403045654\n",
      "Validation epoch 525 ; accuracy: 0.7433333333333333; loss: 1.6663687229156494\n",
      "Training epoch 526 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 526 ; accuracy: 0.7433333333333333; loss: 1.666737675666809\n",
      "Training epoch 527 ; accuracy: 0.9; loss: 0.19459402561187744\n",
      "Validation epoch 527 ; accuracy: 0.7433333333333333; loss: 1.667100191116333\n",
      "Training epoch 528 ; accuracy: 0.9; loss: 0.19459348917007446\n",
      "Validation epoch 528 ; accuracy: 0.7433333333333333; loss: 1.667457103729248\n",
      "Training epoch 529 ; accuracy: 0.9; loss: 0.19459401071071625\n",
      "Validation epoch 529 ; accuracy: 0.7433333333333333; loss: 1.667802333831787\n",
      "Training epoch 530 ; accuracy: 0.9; loss: 0.19459347426891327\n",
      "Validation epoch 530 ; accuracy: 0.7433333333333333; loss: 1.6681575775146484\n",
      "Training epoch 531 ; accuracy: 0.9; loss: 0.19459469616413116\n",
      "Validation epoch 531 ; accuracy: 0.7433333333333333; loss: 1.6685012578964233\n",
      "Training epoch 532 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 532 ; accuracy: 0.7433333333333333; loss: 1.668838381767273\n",
      "Training epoch 533 ; accuracy: 0.9; loss: 0.19459393620491028\n",
      "Validation epoch 533 ; accuracy: 0.7433333333333333; loss: 1.6691703796386719\n",
      "Training epoch 534 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 534 ; accuracy: 0.7433333333333333; loss: 1.6694928407669067\n",
      "Training epoch 535 ; accuracy: 0.9; loss: 0.19459375739097595\n",
      "Validation epoch 535 ; accuracy: 0.7433333333333333; loss: 1.6698514223098755\n",
      "Training epoch 536 ; accuracy: 0.9; loss: 0.19459354877471924\n",
      "Validation epoch 536 ; accuracy: 0.7433333333333333; loss: 1.6702027320861816\n",
      "Training epoch 537 ; accuracy: 0.9; loss: 0.19459356367588043\n",
      "Validation epoch 537 ; accuracy: 0.7433333333333333; loss: 1.670541524887085\n",
      "Training epoch 538 ; accuracy: 0.9; loss: 0.19459323585033417\n",
      "Validation epoch 538 ; accuracy: 0.7433333333333333; loss: 1.670875072479248\n",
      "Training epoch 539 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 539 ; accuracy: 0.7433333333333333; loss: 1.671208143234253\n",
      "Training epoch 540 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 540 ; accuracy: 0.7433333333333333; loss: 1.6715357303619385\n",
      "Training epoch 541 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 541 ; accuracy: 0.7433333333333333; loss: 1.6718485355377197\n",
      "Training epoch 542 ; accuracy: 0.9; loss: 0.1945943683385849\n",
      "Validation epoch 542 ; accuracy: 0.7433333333333333; loss: 1.6721724271774292\n",
      "Training epoch 543 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 543 ; accuracy: 0.7433333333333333; loss: 1.672494649887085\n",
      "Training epoch 544 ; accuracy: 0.9; loss: 0.19459375739097595\n",
      "Validation epoch 544 ; accuracy: 0.7433333333333333; loss: 1.6728230714797974\n",
      "Training epoch 545 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 545 ; accuracy: 0.7433333333333333; loss: 1.6731213331222534\n",
      "Training epoch 546 ; accuracy: 0.9; loss: 0.19459429383277893\n",
      "Validation epoch 546 ; accuracy: 0.7433333333333333; loss: 1.673496127128601\n",
      "Training epoch 547 ; accuracy: 0.9; loss: 0.19459354877471924\n",
      "Validation epoch 547 ; accuracy: 0.7433333333333333; loss: 1.6738725900650024\n",
      "Training epoch 548 ; accuracy: 0.9; loss: 0.1945936530828476\n",
      "Validation epoch 548 ; accuracy: 0.7433333333333333; loss: 1.6742500066757202\n",
      "Training epoch 549 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 549 ; accuracy: 0.7433333333333333; loss: 1.6746368408203125\n",
      "Training epoch 550 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 550 ; accuracy: 0.7433333333333333; loss: 1.6750109195709229\n",
      "Training epoch 551 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 551 ; accuracy: 0.7433333333333333; loss: 1.675413727760315\n",
      "Training epoch 552 ; accuracy: 0.9; loss: 0.19459356367588043\n",
      "Validation epoch 552 ; accuracy: 0.7433333333333333; loss: 1.6757872104644775\n",
      "Training epoch 553 ; accuracy: 0.9; loss: 0.19459359347820282\n",
      "Validation epoch 553 ; accuracy: 0.7433333333333333; loss: 1.6761609315872192\n",
      "Training epoch 554 ; accuracy: 0.9; loss: 0.19459347426891327\n",
      "Validation epoch 554 ; accuracy: 0.7433333333333333; loss: 1.676526665687561\n",
      "Training epoch 555 ; accuracy: 0.9; loss: 0.1945934146642685\n",
      "Validation epoch 555 ; accuracy: 0.7433333333333333; loss: 1.676904559135437\n",
      "Training epoch 556 ; accuracy: 0.9; loss: 0.19459401071071625\n",
      "Validation epoch 556 ; accuracy: 0.7433333333333333; loss: 1.6772836446762085\n",
      "Training epoch 557 ; accuracy: 0.9; loss: 0.194593608379364\n",
      "Validation epoch 557 ; accuracy: 0.7433333333333333; loss: 1.6776255369186401\n",
      "Training epoch 558 ; accuracy: 0.9; loss: 0.19459311664104462\n",
      "Validation epoch 558 ; accuracy: 0.7433333333333333; loss: 1.6779601573944092\n",
      "Training epoch 559 ; accuracy: 0.9; loss: 0.1945934295654297\n",
      "Validation epoch 559 ; accuracy: 0.7433333333333333; loss: 1.6782885789871216\n",
      "Training epoch 560 ; accuracy: 0.9; loss: 0.19459351897239685\n",
      "Validation epoch 560 ; accuracy: 0.7433333333333333; loss: 1.678605079650879\n",
      "Training epoch 561 ; accuracy: 0.9; loss: 0.19459374248981476\n",
      "Validation epoch 561 ; accuracy: 0.7433333333333333; loss: 1.678903579711914\n",
      "Training epoch 562 ; accuracy: 0.9; loss: 0.1945931613445282\n",
      "Validation epoch 562 ; accuracy: 0.7433333333333333; loss: 1.6791801452636719\n",
      "Training epoch 563 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 563 ; accuracy: 0.7433333333333333; loss: 1.6794650554656982\n",
      "Training epoch 564 ; accuracy: 0.9; loss: 0.19459371268749237\n",
      "Validation epoch 564 ; accuracy: 0.7433333333333333; loss: 1.679785132408142\n",
      "Training epoch 565 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 565 ; accuracy: 0.7433333333333333; loss: 1.6801211833953857\n",
      "Training epoch 566 ; accuracy: 0.9; loss: 0.19459374248981476\n",
      "Validation epoch 566 ; accuracy: 0.7433333333333333; loss: 1.6804503202438354\n",
      "Training epoch 567 ; accuracy: 0.9; loss: 0.1945934295654297\n",
      "Validation epoch 567 ; accuracy: 0.7433333333333333; loss: 1.680783987045288\n",
      "Training epoch 568 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 568 ; accuracy: 0.7433333333333333; loss: 1.6811563968658447\n",
      "Training epoch 569 ; accuracy: 0.9; loss: 0.19459359347820282\n",
      "Validation epoch 569 ; accuracy: 0.7433333333333333; loss: 1.681578516960144\n",
      "Training epoch 570 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 570 ; accuracy: 0.7433333333333333; loss: 1.6819629669189453\n",
      "Training epoch 571 ; accuracy: 0.9; loss: 0.19459305703639984\n",
      "Validation epoch 571 ; accuracy: 0.7433333333333333; loss: 1.6823327541351318\n",
      "Training epoch 572 ; accuracy: 0.9; loss: 0.19459344446659088\n",
      "Validation epoch 572 ; accuracy: 0.7433333333333333; loss: 1.682694435119629\n",
      "Training epoch 573 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 573 ; accuracy: 0.7433333333333333; loss: 1.6830518245697021\n",
      "Training epoch 574 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 574 ; accuracy: 0.7433333333333333; loss: 1.6833999156951904\n",
      "Training epoch 575 ; accuracy: 0.9; loss: 0.19459369778633118\n",
      "Validation epoch 575 ; accuracy: 0.7433333333333333; loss: 1.6837456226348877\n",
      "Training epoch 576 ; accuracy: 0.9; loss: 0.19459350407123566\n",
      "Validation epoch 576 ; accuracy: 0.7433333333333333; loss: 1.6840884685516357\n",
      "Training epoch 577 ; accuracy: 0.9; loss: 0.19459407031536102\n",
      "Validation epoch 577 ; accuracy: 0.7433333333333333; loss: 1.684380292892456\n",
      "Training epoch 578 ; accuracy: 0.9; loss: 0.19459378719329834\n",
      "Validation epoch 578 ; accuracy: 0.7433333333333333; loss: 1.6846758127212524\n",
      "Training epoch 579 ; accuracy: 0.9; loss: 0.19459353387355804\n",
      "Validation epoch 579 ; accuracy: 0.7433333333333333; loss: 1.6849521398544312\n",
      "Training epoch 580 ; accuracy: 0.9; loss: 0.19459369778633118\n",
      "Validation epoch 580 ; accuracy: 0.7433333333333333; loss: 1.6852235794067383\n",
      "Training epoch 581 ; accuracy: 0.9; loss: 0.19459392130374908\n",
      "Validation epoch 581 ; accuracy: 0.7433333333333333; loss: 1.6855382919311523\n",
      "Training epoch 582 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 582 ; accuracy: 0.7433333333333333; loss: 1.6858352422714233\n",
      "Training epoch 583 ; accuracy: 0.9; loss: 0.19459353387355804\n",
      "Validation epoch 583 ; accuracy: 0.7433333333333333; loss: 1.6861376762390137\n",
      "Training epoch 584 ; accuracy: 0.9; loss: 0.19459344446659088\n",
      "Validation epoch 584 ; accuracy: 0.7433333333333333; loss: 1.6864433288574219\n",
      "Training epoch 585 ; accuracy: 0.9; loss: 0.19459348917007446\n",
      "Validation epoch 585 ; accuracy: 0.7433333333333333; loss: 1.6867302656173706\n",
      "Training epoch 586 ; accuracy: 0.9; loss: 0.19459322094917297\n",
      "Validation epoch 586 ; accuracy: 0.7433333333333333; loss: 1.687018632888794\n",
      "Training epoch 587 ; accuracy: 0.9; loss: 0.19459375739097595\n",
      "Validation epoch 587 ; accuracy: 0.7433333333333333; loss: 1.687322735786438\n",
      "Training epoch 588 ; accuracy: 0.9; loss: 0.19459347426891327\n",
      "Validation epoch 588 ; accuracy: 0.7433333333333333; loss: 1.6876224279403687\n",
      "Training epoch 589 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 589 ; accuracy: 0.7433333333333333; loss: 1.6878738403320312\n",
      "Training epoch 590 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 590 ; accuracy: 0.7433333333333333; loss: 1.6881579160690308\n",
      "Training epoch 591 ; accuracy: 0.9; loss: 0.19459405541419983\n",
      "Validation epoch 591 ; accuracy: 0.7433333333333333; loss: 1.688437819480896\n",
      "Training epoch 592 ; accuracy: 0.9; loss: 0.1945945769548416\n",
      "Validation epoch 592 ; accuracy: 0.7433333333333333; loss: 1.6887050867080688\n",
      "Training epoch 593 ; accuracy: 0.9; loss: 0.19459301233291626\n",
      "Validation epoch 593 ; accuracy: 0.7433333333333333; loss: 1.6889644861221313\n",
      "Training epoch 594 ; accuracy: 0.9; loss: 0.1945934146642685\n",
      "Validation epoch 594 ; accuracy: 0.7433333333333333; loss: 1.689200758934021\n",
      "Training epoch 595 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 595 ; accuracy: 0.7433333333333333; loss: 1.6894654035568237\n",
      "Training epoch 596 ; accuracy: 0.9; loss: 0.19459310173988342\n",
      "Validation epoch 596 ; accuracy: 0.7433333333333333; loss: 1.6897329092025757\n",
      "Training epoch 597 ; accuracy: 0.9; loss: 0.19459348917007446\n",
      "Validation epoch 597 ; accuracy: 0.7433333333333333; loss: 1.6900113821029663\n",
      "Training epoch 598 ; accuracy: 0.9; loss: 0.19459281861782074\n",
      "Validation epoch 598 ; accuracy: 0.7433333333333333; loss: 1.6902843713760376\n",
      "Training epoch 599 ; accuracy: 0.9; loss: 0.1945938766002655\n",
      "Validation epoch 599 ; accuracy: 0.7433333333333333; loss: 1.6905412673950195\n",
      "Training epoch 600 ; accuracy: 0.9; loss: 0.19459320604801178\n",
      "Validation epoch 600 ; accuracy: 0.7433333333333333; loss: 1.6908015012741089\n",
      "Training epoch 601 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 601 ; accuracy: 0.7433333333333333; loss: 1.6910611391067505\n",
      "Training epoch 602 ; accuracy: 0.9; loss: 0.19459295272827148\n",
      "Validation epoch 602 ; accuracy: 0.7433333333333333; loss: 1.6913224458694458\n",
      "Training epoch 603 ; accuracy: 0.9; loss: 0.19459326565265656\n",
      "Validation epoch 603 ; accuracy: 0.7433333333333333; loss: 1.691583275794983\n",
      "Training epoch 604 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 604 ; accuracy: 0.7433333333333333; loss: 1.6918413639068604\n",
      "Training epoch 605 ; accuracy: 0.9; loss: 0.1945933699607849\n",
      "Validation epoch 605 ; accuracy: 0.7433333333333333; loss: 1.6921027898788452\n",
      "Training epoch 606 ; accuracy: 0.9; loss: 0.19459310173988342\n",
      "Validation epoch 606 ; accuracy: 0.7433333333333333; loss: 1.6923718452453613\n",
      "Training epoch 607 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 607 ; accuracy: 0.7433333333333333; loss: 1.692633867263794\n",
      "Training epoch 608 ; accuracy: 0.9; loss: 0.1945931613445282\n",
      "Validation epoch 608 ; accuracy: 0.7433333333333333; loss: 1.6929287910461426\n",
      "Training epoch 609 ; accuracy: 0.9; loss: 0.19477392733097076\n",
      "Validation epoch 609 ; accuracy: 0.7466666666666667; loss: 1.6638394594192505\n",
      "Training epoch 610 ; accuracy: 0.9; loss: 0.19459432363510132\n",
      "Validation epoch 610 ; accuracy: 0.75; loss: 1.6526566743850708\n",
      "Training epoch 611 ; accuracy: 0.9; loss: 0.194597989320755\n",
      "Validation epoch 611 ; accuracy: 0.7433333333333333; loss: 1.6496738195419312\n",
      "Training epoch 612 ; accuracy: 0.9; loss: 0.19460193812847137\n",
      "Validation epoch 612 ; accuracy: 0.7366666666666667; loss: 1.6504583358764648\n",
      "Training epoch 613 ; accuracy: 0.9; loss: 0.194608673453331\n",
      "Validation epoch 613 ; accuracy: 0.7433333333333333; loss: 1.653136134147644\n",
      "Training epoch 614 ; accuracy: 0.9; loss: 0.1946413815021515\n",
      "Validation epoch 614 ; accuracy: 0.74; loss: 1.654913067817688\n",
      "Training epoch 615 ; accuracy: 0.9; loss: 0.19464276731014252\n",
      "Validation epoch 615 ; accuracy: 0.74; loss: 1.6549296379089355\n",
      "Training epoch 616 ; accuracy: 0.9; loss: 0.19465310871601105\n",
      "Validation epoch 616 ; accuracy: 0.7433333333333333; loss: 1.6519732475280762\n",
      "Training epoch 617 ; accuracy: 0.9; loss: 0.19465109705924988\n",
      "Validation epoch 617 ; accuracy: 0.74; loss: 1.646755337715149\n",
      "Training epoch 618 ; accuracy: 0.9; loss: 0.1947304904460907\n",
      "Validation epoch 618 ; accuracy: 0.74; loss: 1.6341743469238281\n",
      "Training epoch 619 ; accuracy: 0.9; loss: 0.19465011358261108\n",
      "Validation epoch 619 ; accuracy: 0.7333333333333333; loss: 1.6213667392730713\n",
      "Training epoch 620 ; accuracy: 0.9; loss: 0.1946568787097931\n",
      "Validation epoch 620 ; accuracy: 0.7333333333333333; loss: 1.6082431077957153\n",
      "Training epoch 621 ; accuracy: 0.9; loss: 0.19462108612060547\n",
      "Validation epoch 621 ; accuracy: 0.7333333333333333; loss: 1.596699595451355\n",
      "Training epoch 622 ; accuracy: 0.9; loss: 0.19461308419704437\n",
      "Validation epoch 622 ; accuracy: 0.74; loss: 1.5866862535476685\n",
      "Training epoch 623 ; accuracy: 0.9; loss: 0.19461140036582947\n",
      "Validation epoch 623 ; accuracy: 0.74; loss: 1.5781855583190918\n",
      "Training epoch 624 ; accuracy: 0.9; loss: 0.1946069598197937\n",
      "Validation epoch 624 ; accuracy: 0.7433333333333333; loss: 1.571290135383606\n",
      "Training epoch 625 ; accuracy: 0.9; loss: 0.19460125267505646\n",
      "Validation epoch 625 ; accuracy: 0.74; loss: 1.5657782554626465\n",
      "Training epoch 626 ; accuracy: 0.9; loss: 0.19459812343120575\n",
      "Validation epoch 626 ; accuracy: 0.74; loss: 1.5615177154541016\n",
      "Training epoch 627 ; accuracy: 0.9; loss: 0.19459961354732513\n",
      "Validation epoch 627 ; accuracy: 0.7433333333333333; loss: 1.5583323240280151\n",
      "Training epoch 628 ; accuracy: 0.9; loss: 0.1945979744195938\n",
      "Validation epoch 628 ; accuracy: 0.7466666666666667; loss: 1.5558831691741943\n",
      "Training epoch 629 ; accuracy: 0.9; loss: 0.1945977360010147\n",
      "Validation epoch 629 ; accuracy: 0.7466666666666667; loss: 1.5538667440414429\n",
      "Training epoch 630 ; accuracy: 0.9; loss: 0.1945987492799759\n",
      "Validation epoch 630 ; accuracy: 0.7533333333333333; loss: 1.552303433418274\n",
      "Training epoch 631 ; accuracy: 0.9; loss: 0.19459748268127441\n",
      "Validation epoch 631 ; accuracy: 0.7533333333333333; loss: 1.5513094663619995\n",
      "Training epoch 632 ; accuracy: 0.9; loss: 0.19459836184978485\n",
      "Validation epoch 632 ; accuracy: 0.7533333333333333; loss: 1.550783634185791\n",
      "Training epoch 633 ; accuracy: 0.9; loss: 0.19459721446037292\n",
      "Validation epoch 633 ; accuracy: 0.7533333333333333; loss: 1.5506384372711182\n",
      "Training epoch 634 ; accuracy: 0.9; loss: 0.19459767639636993\n",
      "Validation epoch 634 ; accuracy: 0.7533333333333333; loss: 1.5507968664169312\n",
      "Training epoch 635 ; accuracy: 0.9; loss: 0.19459812343120575\n",
      "Validation epoch 635 ; accuracy: 0.7533333333333333; loss: 1.5512449741363525\n",
      "Training epoch 636 ; accuracy: 0.9; loss: 0.19459600746631622\n",
      "Validation epoch 636 ; accuracy: 0.7433333333333333; loss: 1.5518831014633179\n",
      "Training epoch 637 ; accuracy: 0.9; loss: 0.19459637999534607\n",
      "Validation epoch 637 ; accuracy: 0.7433333333333333; loss: 1.5527039766311646\n",
      "Training epoch 638 ; accuracy: 0.9; loss: 0.19459740817546844\n",
      "Validation epoch 638 ; accuracy: 0.7433333333333333; loss: 1.5536901950836182\n",
      "Training epoch 639 ; accuracy: 0.9; loss: 0.1945970505475998\n",
      "Validation epoch 639 ; accuracy: 0.7433333333333333; loss: 1.5547399520874023\n",
      "Training epoch 640 ; accuracy: 0.9; loss: 0.19459743797779083\n",
      "Validation epoch 640 ; accuracy: 0.7433333333333333; loss: 1.5559070110321045\n",
      "Training epoch 641 ; accuracy: 0.9; loss: 0.19459985196590424\n",
      "Validation epoch 641 ; accuracy: 0.7433333333333333; loss: 1.5572426319122314\n",
      "Training epoch 642 ; accuracy: 0.9; loss: 0.19459593296051025\n",
      "Validation epoch 642 ; accuracy: 0.7433333333333333; loss: 1.558596134185791\n",
      "Training epoch 643 ; accuracy: 0.9; loss: 0.1945974975824356\n",
      "Validation epoch 643 ; accuracy: 0.7433333333333333; loss: 1.5600441694259644\n",
      "Training epoch 644 ; accuracy: 0.9; loss: 0.19459764659404755\n",
      "Validation epoch 644 ; accuracy: 0.7433333333333333; loss: 1.5615787506103516\n",
      "Training epoch 645 ; accuracy: 0.9; loss: 0.1945992112159729\n",
      "Validation epoch 645 ; accuracy: 0.7433333333333333; loss: 1.5632215738296509\n",
      "Training epoch 646 ; accuracy: 0.9; loss: 0.19459612667560577\n",
      "Validation epoch 646 ; accuracy: 0.7433333333333333; loss: 1.5648326873779297\n",
      "Training epoch 647 ; accuracy: 0.9; loss: 0.19460022449493408\n",
      "Validation epoch 647 ; accuracy: 0.7433333333333333; loss: 1.5665134191513062\n",
      "Training epoch 648 ; accuracy: 0.9; loss: 0.1945962756872177\n",
      "Validation epoch 648 ; accuracy: 0.7433333333333333; loss: 1.5681812763214111\n",
      "Training epoch 649 ; accuracy: 0.9; loss: 0.19459562003612518\n",
      "Validation epoch 649 ; accuracy: 0.7433333333333333; loss: 1.5698310136795044\n",
      "Training epoch 650 ; accuracy: 0.9; loss: 0.19459578394889832\n",
      "Validation epoch 650 ; accuracy: 0.7433333333333333; loss: 1.5714318752288818\n",
      "Training epoch 651 ; accuracy: 0.9; loss: 0.19459840655326843\n",
      "Validation epoch 651 ; accuracy: 0.7433333333333333; loss: 1.573062539100647\n",
      "Training epoch 652 ; accuracy: 0.9; loss: 0.19485360383987427\n",
      "Validation epoch 652 ; accuracy: 0.7366666666666667; loss: 1.5941498279571533\n",
      "Training epoch 653 ; accuracy: 0.9; loss: 0.19459733366966248\n",
      "Validation epoch 653 ; accuracy: 0.7333333333333333; loss: 1.620559811592102\n",
      "Training epoch 654 ; accuracy: 0.9; loss: 0.19459718465805054\n",
      "Validation epoch 654 ; accuracy: 0.7366666666666667; loss: 1.6478663682937622\n",
      "Training epoch 655 ; accuracy: 0.9; loss: 0.19459836184978485\n",
      "Validation epoch 655 ; accuracy: 0.7366666666666667; loss: 1.674548625946045\n",
      "Training epoch 656 ; accuracy: 0.9; loss: 0.19460168480873108\n",
      "Validation epoch 656 ; accuracy: 0.7333333333333333; loss: 1.6976054906845093\n",
      "Training epoch 657 ; accuracy: 0.9; loss: 0.1946052461862564\n",
      "Validation epoch 657 ; accuracy: 0.73; loss: 1.719900131225586\n",
      "Training epoch 658 ; accuracy: 0.9; loss: 0.1946137398481369\n",
      "Validation epoch 658 ; accuracy: 0.7233333333333334; loss: 1.739458441734314\n",
      "Training epoch 659 ; accuracy: 0.9; loss: 0.19461624324321747\n",
      "Validation epoch 659 ; accuracy: 0.72; loss: 1.7547863721847534\n",
      "Training epoch 660 ; accuracy: 0.9; loss: 0.19463112950325012\n",
      "Validation epoch 660 ; accuracy: 0.72; loss: 1.7635524272918701\n",
      "Training epoch 661 ; accuracy: 0.9; loss: 0.19467292726039886\n",
      "Validation epoch 661 ; accuracy: 0.72; loss: 1.758454442024231\n",
      "Training epoch 662 ; accuracy: 0.9; loss: 0.19465424120426178\n",
      "Validation epoch 662 ; accuracy: 0.7233333333333334; loss: 1.7454410791397095\n",
      "Training epoch 663 ; accuracy: 0.9; loss: 0.19462375342845917\n",
      "Validation epoch 663 ; accuracy: 0.7233333333333334; loss: 1.7306034564971924\n",
      "Training epoch 664 ; accuracy: 0.9; loss: 0.1946130394935608\n",
      "Validation epoch 664 ; accuracy: 0.7233333333333334; loss: 1.7163209915161133\n",
      "Training epoch 665 ; accuracy: 0.9; loss: 0.19460444152355194\n",
      "Validation epoch 665 ; accuracy: 0.73; loss: 1.7034926414489746\n",
      "Training epoch 666 ; accuracy: 0.9; loss: 0.1946028172969818\n",
      "Validation epoch 666 ; accuracy: 0.7333333333333333; loss: 1.6921391487121582\n",
      "Training epoch 667 ; accuracy: 0.9; loss: 0.19460152089595795\n",
      "Validation epoch 667 ; accuracy: 0.74; loss: 1.6815624237060547\n",
      "Training epoch 668 ; accuracy: 0.9; loss: 0.19459767639636993\n",
      "Validation epoch 668 ; accuracy: 0.74; loss: 1.6724164485931396\n",
      "Training epoch 669 ; accuracy: 0.9; loss: 0.19459868967533112\n",
      "Validation epoch 669 ; accuracy: 0.7433333333333333; loss: 1.6644355058670044\n",
      "Training epoch 670 ; accuracy: 0.9; loss: 0.19459757208824158\n",
      "Validation epoch 670 ; accuracy: 0.7433333333333333; loss: 1.6573278903961182\n",
      "Training epoch 671 ; accuracy: 0.9; loss: 0.19459885358810425\n",
      "Validation epoch 671 ; accuracy: 0.7433333333333333; loss: 1.6503735780715942\n",
      "Training epoch 672 ; accuracy: 0.9; loss: 0.19459649920463562\n",
      "Validation epoch 672 ; accuracy: 0.7466666666666667; loss: 1.644373893737793\n",
      "Training epoch 673 ; accuracy: 0.9; loss: 0.19459639489650726\n",
      "Validation epoch 673 ; accuracy: 0.7466666666666667; loss: 1.6392490863800049\n",
      "Training epoch 674 ; accuracy: 0.9; loss: 0.19459760189056396\n",
      "Validation epoch 674 ; accuracy: 0.75; loss: 1.634915828704834\n",
      "Training epoch 675 ; accuracy: 0.9; loss: 0.19459593296051025\n",
      "Validation epoch 675 ; accuracy: 0.75; loss: 1.6312410831451416\n",
      "Training epoch 676 ; accuracy: 0.9; loss: 0.19459719955921173\n",
      "Validation epoch 676 ; accuracy: 0.75; loss: 1.6282614469528198\n",
      "Training epoch 677 ; accuracy: 0.9; loss: 0.1945955753326416\n",
      "Validation epoch 677 ; accuracy: 0.75; loss: 1.6257822513580322\n",
      "Training epoch 678 ; accuracy: 0.9; loss: 0.1945970058441162\n",
      "Validation epoch 678 ; accuracy: 0.7466666666666667; loss: 1.6238113641738892\n",
      "Training epoch 679 ; accuracy: 0.9; loss: 0.19459602236747742\n",
      "Validation epoch 679 ; accuracy: 0.7466666666666667; loss: 1.6222198009490967\n",
      "Training epoch 680 ; accuracy: 0.9; loss: 0.1945962756872177\n",
      "Validation epoch 680 ; accuracy: 0.7466666666666667; loss: 1.6210037469863892\n",
      "Training epoch 681 ; accuracy: 0.9; loss: 0.1946035623550415\n",
      "Validation epoch 681 ; accuracy: 0.7466666666666667; loss: 1.620689034461975\n",
      "Training epoch 682 ; accuracy: 0.9; loss: 0.19459550082683563\n",
      "Validation epoch 682 ; accuracy: 0.7466666666666667; loss: 1.6205542087554932\n",
      "Training epoch 683 ; accuracy: 0.9; loss: 0.1945963352918625\n",
      "Validation epoch 683 ; accuracy: 0.7466666666666667; loss: 1.6205852031707764\n",
      "Training epoch 684 ; accuracy: 0.9; loss: 0.19459658861160278\n",
      "Validation epoch 684 ; accuracy: 0.7466666666666667; loss: 1.6207908391952515\n",
      "Training epoch 685 ; accuracy: 0.9; loss: 0.19459931552410126\n",
      "Validation epoch 685 ; accuracy: 0.7466666666666667; loss: 1.621368169784546\n",
      "Training epoch 686 ; accuracy: 0.9; loss: 0.19459562003612518\n",
      "Validation epoch 686 ; accuracy: 0.7466666666666667; loss: 1.6220496892929077\n",
      "Training epoch 687 ; accuracy: 0.9; loss: 0.19459612667560577\n",
      "Validation epoch 687 ; accuracy: 0.7466666666666667; loss: 1.6228121519088745\n",
      "Training epoch 688 ; accuracy: 0.9; loss: 0.19459541141986847\n",
      "Validation epoch 688 ; accuracy: 0.7466666666666667; loss: 1.6236249208450317\n",
      "Training epoch 689 ; accuracy: 0.9; loss: 0.19459548592567444\n",
      "Validation epoch 689 ; accuracy: 0.7466666666666667; loss: 1.6244796514511108\n",
      "Training epoch 690 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 690 ; accuracy: 0.7466666666666667; loss: 1.6253371238708496\n",
      "Training epoch 691 ; accuracy: 0.9; loss: 0.19459669291973114\n",
      "Validation epoch 691 ; accuracy: 0.7466666666666667; loss: 1.6263611316680908\n",
      "Training epoch 692 ; accuracy: 0.9; loss: 0.19459547102451324\n",
      "Validation epoch 692 ; accuracy: 0.7466666666666667; loss: 1.627416729927063\n",
      "Training epoch 693 ; accuracy: 0.9; loss: 0.19459445774555206\n",
      "Validation epoch 693 ; accuracy: 0.7466666666666667; loss: 1.628473162651062\n",
      "Training epoch 694 ; accuracy: 0.9; loss: 0.19459576904773712\n",
      "Validation epoch 694 ; accuracy: 0.7466666666666667; loss: 1.6295421123504639\n",
      "Training epoch 695 ; accuracy: 0.9; loss: 0.19459518790245056\n",
      "Validation epoch 695 ; accuracy: 0.7466666666666667; loss: 1.6306347846984863\n",
      "Training epoch 696 ; accuracy: 0.9; loss: 0.1945951133966446\n",
      "Validation epoch 696 ; accuracy: 0.7466666666666667; loss: 1.6317496299743652\n",
      "Training epoch 697 ; accuracy: 0.9; loss: 0.19459505379199982\n",
      "Validation epoch 697 ; accuracy: 0.7466666666666667; loss: 1.6328898668289185\n",
      "Training epoch 698 ; accuracy: 0.9; loss: 0.1945950984954834\n",
      "Validation epoch 698 ; accuracy: 0.7466666666666667; loss: 1.6340221166610718\n",
      "Training epoch 699 ; accuracy: 0.9; loss: 0.194594606757164\n",
      "Validation epoch 699 ; accuracy: 0.7466666666666667; loss: 1.6351405382156372\n",
      "Training epoch 700 ; accuracy: 0.9; loss: 0.1946055144071579\n",
      "Validation epoch 700 ; accuracy: 0.7466666666666667; loss: 1.6370609998703003\n",
      "Training epoch 701 ; accuracy: 0.9; loss: 0.19459472596645355\n",
      "Validation epoch 701 ; accuracy: 0.7466666666666667; loss: 1.638921856880188\n",
      "Training epoch 702 ; accuracy: 0.9; loss: 0.1945950984954834\n",
      "Validation epoch 702 ; accuracy: 0.7466666666666667; loss: 1.6407580375671387\n",
      "Training epoch 703 ; accuracy: 0.9; loss: 0.19459465146064758\n",
      "Validation epoch 703 ; accuracy: 0.7466666666666667; loss: 1.6425323486328125\n",
      "Training epoch 704 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 704 ; accuracy: 0.7466666666666667; loss: 1.6442333459854126\n",
      "Training epoch 705 ; accuracy: 0.9; loss: 0.1945941150188446\n",
      "Validation epoch 705 ; accuracy: 0.7466666666666667; loss: 1.64585280418396\n",
      "Training epoch 706 ; accuracy: 0.9; loss: 0.19459404051303864\n",
      "Validation epoch 706 ; accuracy: 0.7466666666666667; loss: 1.6474043130874634\n",
      "Training epoch 707 ; accuracy: 0.9; loss: 0.19459420442581177\n",
      "Validation epoch 707 ; accuracy: 0.7466666666666667; loss: 1.648894190788269\n",
      "Training epoch 708 ; accuracy: 0.9; loss: 0.1945948302745819\n",
      "Validation epoch 708 ; accuracy: 0.7466666666666667; loss: 1.650380253791809\n",
      "Training epoch 709 ; accuracy: 0.9; loss: 0.19459444284439087\n",
      "Validation epoch 709 ; accuracy: 0.7466666666666667; loss: 1.6518162488937378\n",
      "Training epoch 710 ; accuracy: 0.9; loss: 0.19459448754787445\n",
      "Validation epoch 710 ; accuracy: 0.7466666666666667; loss: 1.653171181678772\n",
      "Training epoch 711 ; accuracy: 0.9; loss: 0.19459474086761475\n",
      "Validation epoch 711 ; accuracy: 0.7466666666666667; loss: 1.6544876098632812\n",
      "Training epoch 712 ; accuracy: 0.9; loss: 0.1945936530828476\n",
      "Validation epoch 712 ; accuracy: 0.7466666666666667; loss: 1.6557445526123047\n",
      "Training epoch 713 ; accuracy: 0.9; loss: 0.19459384679794312\n",
      "Validation epoch 713 ; accuracy: 0.7466666666666667; loss: 1.6569334268569946\n",
      "Training epoch 714 ; accuracy: 0.9; loss: 0.19459424912929535\n",
      "Validation epoch 714 ; accuracy: 0.7466666666666667; loss: 1.6580742597579956\n",
      "Training epoch 715 ; accuracy: 0.9; loss: 0.19459404051303864\n",
      "Validation epoch 715 ; accuracy: 0.7466666666666667; loss: 1.6591689586639404\n",
      "Training epoch 716 ; accuracy: 0.9; loss: 0.19459377229213715\n",
      "Validation epoch 716 ; accuracy: 0.7466666666666667; loss: 1.6601988077163696\n",
      "Training epoch 717 ; accuracy: 0.9; loss: 0.1945955455303192\n",
      "Validation epoch 717 ; accuracy: 0.7466666666666667; loss: 1.6613234281539917\n",
      "Training epoch 718 ; accuracy: 0.9; loss: 0.1945936232805252\n",
      "Validation epoch 718 ; accuracy: 0.7466666666666667; loss: 1.6623921394348145\n",
      "Training epoch 719 ; accuracy: 0.9; loss: 0.1945943981409073\n",
      "Validation epoch 719 ; accuracy: 0.7466666666666667; loss: 1.6634470224380493\n",
      "Training epoch 720 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 720 ; accuracy: 0.7466666666666667; loss: 1.6644604206085205\n",
      "Training epoch 721 ; accuracy: 0.9; loss: 0.19459393620491028\n",
      "Validation epoch 721 ; accuracy: 0.7466666666666667; loss: 1.6654313802719116\n",
      "Training epoch 722 ; accuracy: 0.9; loss: 0.19459351897239685\n",
      "Validation epoch 722 ; accuracy: 0.7466666666666667; loss: 1.6663569211959839\n",
      "Training epoch 723 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 723 ; accuracy: 0.7466666666666667; loss: 1.6672247648239136\n",
      "Training epoch 724 ; accuracy: 0.9; loss: 0.1945938616991043\n",
      "Validation epoch 724 ; accuracy: 0.7466666666666667; loss: 1.6680868864059448\n",
      "Training epoch 725 ; accuracy: 0.9; loss: 0.19459347426891327\n",
      "Validation epoch 725 ; accuracy: 0.7466666666666667; loss: 1.6689141988754272\n",
      "Training epoch 726 ; accuracy: 0.9; loss: 0.19459393620491028\n",
      "Validation epoch 726 ; accuracy: 0.7466666666666667; loss: 1.6697108745574951\n",
      "Training epoch 727 ; accuracy: 0.9; loss: 0.1945936530828476\n",
      "Validation epoch 727 ; accuracy: 0.7466666666666667; loss: 1.6705029010772705\n",
      "Training epoch 728 ; accuracy: 0.9; loss: 0.19459354877471924\n",
      "Validation epoch 728 ; accuracy: 0.7466666666666667; loss: 1.6712570190429688\n",
      "Training epoch 729 ; accuracy: 0.9; loss: 0.19459377229213715\n",
      "Validation epoch 729 ; accuracy: 0.7466666666666667; loss: 1.671991229057312\n",
      "Training epoch 730 ; accuracy: 0.9; loss: 0.19459451735019684\n",
      "Validation epoch 730 ; accuracy: 0.7466666666666667; loss: 1.6727510690689087\n",
      "Training epoch 731 ; accuracy: 0.9; loss: 0.19459408521652222\n",
      "Validation epoch 731 ; accuracy: 0.7466666666666667; loss: 1.6735129356384277\n",
      "Training epoch 732 ; accuracy: 0.9; loss: 0.19459454715251923\n",
      "Validation epoch 732 ; accuracy: 0.7466666666666667; loss: 1.6743000745773315\n",
      "Training epoch 733 ; accuracy: 0.9; loss: 0.19459371268749237\n",
      "Validation epoch 733 ; accuracy: 0.7466666666666667; loss: 1.6750751733779907\n",
      "Training epoch 734 ; accuracy: 0.9; loss: 0.1945939064025879\n",
      "Validation epoch 734 ; accuracy: 0.7466666666666667; loss: 1.6758447885513306\n",
      "Training epoch 735 ; accuracy: 0.9; loss: 0.19459383189678192\n",
      "Validation epoch 735 ; accuracy: 0.7466666666666667; loss: 1.6765940189361572\n",
      "Training epoch 736 ; accuracy: 0.9; loss: 0.19459359347820282\n",
      "Validation epoch 736 ; accuracy: 0.7466666666666667; loss: 1.677315592765808\n",
      "Training epoch 737 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 737 ; accuracy: 0.7466666666666667; loss: 1.678006649017334\n",
      "Training epoch 738 ; accuracy: 0.9; loss: 0.19459359347820282\n",
      "Validation epoch 738 ; accuracy: 0.7466666666666667; loss: 1.678695797920227\n",
      "Training epoch 739 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 739 ; accuracy: 0.7466666666666667; loss: 1.6793965101242065\n",
      "Training epoch 740 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 740 ; accuracy: 0.7466666666666667; loss: 1.6801077127456665\n",
      "Training epoch 741 ; accuracy: 0.9; loss: 0.19459396600723267\n",
      "Validation epoch 741 ; accuracy: 0.7466666666666667; loss: 1.6808069944381714\n",
      "Training epoch 742 ; accuracy: 0.9; loss: 0.19459472596645355\n",
      "Validation epoch 742 ; accuracy: 0.7466666666666667; loss: 1.6816277503967285\n",
      "Training epoch 743 ; accuracy: 0.9; loss: 0.19459353387355804\n",
      "Validation epoch 743 ; accuracy: 0.7466666666666667; loss: 1.6824212074279785\n",
      "Training epoch 744 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 744 ; accuracy: 0.7466666666666667; loss: 1.6831847429275513\n",
      "Training epoch 745 ; accuracy: 0.9; loss: 0.1945933848619461\n",
      "Validation epoch 745 ; accuracy: 0.7466666666666667; loss: 1.6839271783828735\n",
      "Training epoch 746 ; accuracy: 0.9; loss: 0.19459371268749237\n",
      "Validation epoch 746 ; accuracy: 0.7466666666666667; loss: 1.684702754020691\n",
      "Training epoch 747 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 747 ; accuracy: 0.7466666666666667; loss: 1.6854442358016968\n",
      "Training epoch 748 ; accuracy: 0.9; loss: 0.1945938616991043\n",
      "Validation epoch 748 ; accuracy: 0.7466666666666667; loss: 1.6861681938171387\n",
      "Training epoch 749 ; accuracy: 0.9; loss: 0.19459347426891327\n",
      "Validation epoch 749 ; accuracy: 0.7466666666666667; loss: 1.6868637800216675\n",
      "Training epoch 750 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 750 ; accuracy: 0.7466666666666667; loss: 1.6875287294387817\n",
      "Training epoch 751 ; accuracy: 0.9; loss: 0.1945936381816864\n",
      "Validation epoch 751 ; accuracy: 0.7466666666666667; loss: 1.6881818771362305\n",
      "Training epoch 752 ; accuracy: 0.9; loss: 0.1945936679840088\n",
      "Validation epoch 752 ; accuracy: 0.7466666666666667; loss: 1.6888302564620972\n",
      "Training epoch 753 ; accuracy: 0.9; loss: 0.1945931762456894\n",
      "Validation epoch 753 ; accuracy: 0.7466666666666667; loss: 1.6894583702087402\n",
      "Training epoch 754 ; accuracy: 0.9; loss: 0.19459392130374908\n",
      "Validation epoch 754 ; accuracy: 0.7466666666666667; loss: 1.6900781393051147\n",
      "Training epoch 755 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 755 ; accuracy: 0.7466666666666667; loss: 1.6907143592834473\n",
      "Training epoch 756 ; accuracy: 0.9; loss: 0.194593146443367\n",
      "Validation epoch 756 ; accuracy: 0.7466666666666667; loss: 1.6913297176361084\n",
      "Training epoch 757 ; accuracy: 0.9; loss: 0.19459308683872223\n",
      "Validation epoch 757 ; accuracy: 0.7466666666666667; loss: 1.6919212341308594\n",
      "Training epoch 758 ; accuracy: 0.9; loss: 0.19459348917007446\n",
      "Validation epoch 758 ; accuracy: 0.7466666666666667; loss: 1.6925232410430908\n",
      "Training epoch 759 ; accuracy: 0.9; loss: 0.1945933699607849\n",
      "Validation epoch 759 ; accuracy: 0.7466666666666667; loss: 1.6931183338165283\n",
      "Training epoch 760 ; accuracy: 0.9; loss: 0.19459354877471924\n",
      "Validation epoch 760 ; accuracy: 0.7466666666666667; loss: 1.6937267780303955\n",
      "Training epoch 761 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 761 ; accuracy: 0.7466666666666667; loss: 1.6943126916885376\n",
      "Training epoch 762 ; accuracy: 0.9; loss: 0.1945941299200058\n",
      "Validation epoch 762 ; accuracy: 0.7466666666666667; loss: 1.6949244737625122\n",
      "Training epoch 763 ; accuracy: 0.9; loss: 0.19459353387355804\n",
      "Validation epoch 763 ; accuracy: 0.7466666666666667; loss: 1.6955206394195557\n",
      "Training epoch 764 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 764 ; accuracy: 0.7466666666666667; loss: 1.6961112022399902\n",
      "Training epoch 765 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 765 ; accuracy: 0.7466666666666667; loss: 1.696666955947876\n",
      "Training epoch 766 ; accuracy: 0.9; loss: 0.194593146443367\n",
      "Validation epoch 766 ; accuracy: 0.7466666666666667; loss: 1.6972261667251587\n",
      "Training epoch 767 ; accuracy: 0.9; loss: 0.1945929378271103\n",
      "Validation epoch 767 ; accuracy: 0.7466666666666667; loss: 1.697759747505188\n",
      "Training epoch 768 ; accuracy: 0.9; loss: 0.19459322094917297\n",
      "Validation epoch 768 ; accuracy: 0.7466666666666667; loss: 1.6983020305633545\n",
      "Training epoch 769 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 769 ; accuracy: 0.7466666666666667; loss: 1.6988202333450317\n",
      "Training epoch 770 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 770 ; accuracy: 0.7466666666666667; loss: 1.6993353366851807\n",
      "Training epoch 771 ; accuracy: 0.9; loss: 0.1945929080247879\n",
      "Validation epoch 771 ; accuracy: 0.7466666666666667; loss: 1.6998292207717896\n",
      "Training epoch 772 ; accuracy: 0.9; loss: 0.19459319114685059\n",
      "Validation epoch 772 ; accuracy: 0.7466666666666667; loss: 1.7003251314163208\n",
      "Training epoch 773 ; accuracy: 0.9; loss: 0.19459286332130432\n",
      "Validation epoch 773 ; accuracy: 0.7466666666666667; loss: 1.7007991075515747\n",
      "Training epoch 774 ; accuracy: 0.9; loss: 0.1945931315422058\n",
      "Validation epoch 774 ; accuracy: 0.7466666666666667; loss: 1.7012745141983032\n",
      "Training epoch 775 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 775 ; accuracy: 0.7466666666666667; loss: 1.7017579078674316\n",
      "Training epoch 776 ; accuracy: 0.9; loss: 0.19459310173988342\n",
      "Validation epoch 776 ; accuracy: 0.7466666666666667; loss: 1.7022472620010376\n",
      "Training epoch 777 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 777 ; accuracy: 0.7466666666666667; loss: 1.7027208805084229\n",
      "Training epoch 778 ; accuracy: 0.9; loss: 0.19459320604801178\n",
      "Validation epoch 778 ; accuracy: 0.7466666666666667; loss: 1.7031872272491455\n",
      "Training epoch 779 ; accuracy: 0.9; loss: 0.1945931613445282\n",
      "Validation epoch 779 ; accuracy: 0.7466666666666667; loss: 1.703639030456543\n",
      "Training epoch 780 ; accuracy: 0.9; loss: 0.19459304213523865\n",
      "Validation epoch 780 ; accuracy: 0.7466666666666667; loss: 1.7040866613388062\n",
      "Training epoch 781 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 781 ; accuracy: 0.7466666666666667; loss: 1.7045141458511353\n",
      "Training epoch 782 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 782 ; accuracy: 0.7466666666666667; loss: 1.704939365386963\n",
      "Training epoch 783 ; accuracy: 0.9; loss: 0.19459283351898193\n",
      "Validation epoch 783 ; accuracy: 0.7466666666666667; loss: 1.7053545713424683\n",
      "Training epoch 784 ; accuracy: 0.9; loss: 0.19459350407123566\n",
      "Validation epoch 784 ; accuracy: 0.7466666666666667; loss: 1.7057983875274658\n",
      "Training epoch 785 ; accuracy: 0.9; loss: 0.1945934146642685\n",
      "Validation epoch 785 ; accuracy: 0.7466666666666667; loss: 1.7062522172927856\n",
      "Training epoch 786 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 786 ; accuracy: 0.7466666666666667; loss: 1.706694483757019\n",
      "Training epoch 787 ; accuracy: 0.9; loss: 0.1945929378271103\n",
      "Validation epoch 787 ; accuracy: 0.7466666666666667; loss: 1.7071346044540405\n",
      "Training epoch 788 ; accuracy: 0.9; loss: 0.19459308683872223\n",
      "Validation epoch 788 ; accuracy: 0.7466666666666667; loss: 1.7075693607330322\n",
      "Training epoch 789 ; accuracy: 0.9; loss: 0.1945931315422058\n",
      "Validation epoch 789 ; accuracy: 0.7466666666666667; loss: 1.7080163955688477\n",
      "Training epoch 790 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 790 ; accuracy: 0.7466666666666667; loss: 1.708458662033081\n",
      "Training epoch 791 ; accuracy: 0.9; loss: 0.1945928931236267\n",
      "Validation epoch 791 ; accuracy: 0.7466666666666667; loss: 1.7089074850082397\n",
      "Training epoch 792 ; accuracy: 0.9; loss: 0.1945933848619461\n",
      "Validation epoch 792 ; accuracy: 0.7466666666666667; loss: 1.7093816995620728\n",
      "Training epoch 793 ; accuracy: 0.9; loss: 0.1945929378271103\n",
      "Validation epoch 793 ; accuracy: 0.7466666666666667; loss: 1.7098537683486938\n",
      "Training epoch 794 ; accuracy: 0.9; loss: 0.19459283351898193\n",
      "Validation epoch 794 ; accuracy: 0.7466666666666667; loss: 1.7103230953216553\n",
      "Training epoch 795 ; accuracy: 0.9; loss: 0.19459354877471924\n",
      "Validation epoch 795 ; accuracy: 0.7466666666666667; loss: 1.7108471393585205\n",
      "Training epoch 796 ; accuracy: 0.9; loss: 0.194593146443367\n",
      "Validation epoch 796 ; accuracy: 0.7466666666666667; loss: 1.7113757133483887\n",
      "Training epoch 797 ; accuracy: 0.9; loss: 0.19459283351898193\n",
      "Validation epoch 797 ; accuracy: 0.7466666666666667; loss: 1.7118803262710571\n",
      "Training epoch 798 ; accuracy: 0.9; loss: 0.19459298253059387\n",
      "Validation epoch 798 ; accuracy: 0.7466666666666667; loss: 1.7123721837997437\n",
      "Training epoch 799 ; accuracy: 0.9; loss: 0.19459307193756104\n",
      "Validation epoch 799 ; accuracy: 0.7466666666666667; loss: 1.712864637374878\n",
      "Training epoch 800 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 800 ; accuracy: 0.7466666666666667; loss: 1.7133369445800781\n",
      "Training epoch 801 ; accuracy: 0.9; loss: 0.19459295272827148\n",
      "Validation epoch 801 ; accuracy: 0.7466666666666667; loss: 1.7137833833694458\n",
      "Training epoch 802 ; accuracy: 0.9; loss: 0.19459319114685059\n",
      "Validation epoch 802 ; accuracy: 0.7466666666666667; loss: 1.7142291069030762\n",
      "Training epoch 803 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 803 ; accuracy: 0.7466666666666667; loss: 1.7146726846694946\n",
      "Training epoch 804 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 804 ; accuracy: 0.7466666666666667; loss: 1.7151072025299072\n",
      "Training epoch 805 ; accuracy: 0.9; loss: 0.19459304213523865\n",
      "Validation epoch 805 ; accuracy: 0.7466666666666667; loss: 1.7155358791351318\n",
      "Training epoch 806 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 806 ; accuracy: 0.7466666666666667; loss: 1.7159565687179565\n",
      "Training epoch 807 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 807 ; accuracy: 0.7466666666666667; loss: 1.716387152671814\n",
      "Training epoch 808 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 808 ; accuracy: 0.7466666666666667; loss: 1.716817855834961\n",
      "Training epoch 809 ; accuracy: 0.9; loss: 0.1945929080247879\n",
      "Validation epoch 809 ; accuracy: 0.7466666666666667; loss: 1.7172452211380005\n",
      "Training epoch 810 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 810 ; accuracy: 0.7466666666666667; loss: 1.7176746129989624\n",
      "Training epoch 811 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 811 ; accuracy: 0.7466666666666667; loss: 1.7180984020233154\n",
      "Training epoch 812 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 812 ; accuracy: 0.7466666666666667; loss: 1.7185231447219849\n",
      "Training epoch 813 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 813 ; accuracy: 0.7466666666666667; loss: 1.7189512252807617\n",
      "Training epoch 814 ; accuracy: 0.9; loss: 0.1945929378271103\n",
      "Validation epoch 814 ; accuracy: 0.7466666666666667; loss: 1.7193641662597656\n",
      "Training epoch 815 ; accuracy: 0.9; loss: 0.19459298253059387\n",
      "Validation epoch 815 ; accuracy: 0.7466666666666667; loss: 1.719785213470459\n",
      "Training epoch 816 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 816 ; accuracy: 0.7466666666666667; loss: 1.720200777053833\n",
      "Training epoch 817 ; accuracy: 0.9; loss: 0.19459305703639984\n",
      "Validation epoch 817 ; accuracy: 0.7466666666666667; loss: 1.7206193208694458\n",
      "Training epoch 818 ; accuracy: 0.9; loss: 0.1945929378271103\n",
      "Validation epoch 818 ; accuracy: 0.7466666666666667; loss: 1.7210357189178467\n",
      "Training epoch 819 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 819 ; accuracy: 0.7466666666666667; loss: 1.7214449644088745\n",
      "Training epoch 820 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 820 ; accuracy: 0.7466666666666667; loss: 1.721852421760559\n",
      "Training epoch 821 ; accuracy: 0.9; loss: 0.19459320604801178\n",
      "Validation epoch 821 ; accuracy: 0.7466666666666667; loss: 1.7222782373428345\n",
      "Training epoch 822 ; accuracy: 0.9; loss: 0.19459238648414612\n",
      "Validation epoch 822 ; accuracy: 0.7466666666666667; loss: 1.7226847410202026\n",
      "Training epoch 823 ; accuracy: 0.9; loss: 0.19459375739097595\n",
      "Validation epoch 823 ; accuracy: 0.7466666666666667; loss: 1.7231398820877075\n",
      "Training epoch 824 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 824 ; accuracy: 0.7466666666666667; loss: 1.7235660552978516\n",
      "Training epoch 825 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 825 ; accuracy: 0.7466666666666667; loss: 1.7240021228790283\n",
      "Training epoch 826 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 826 ; accuracy: 0.7466666666666667; loss: 1.7244348526000977\n",
      "Training epoch 827 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 827 ; accuracy: 0.7466666666666667; loss: 1.724846363067627\n",
      "Training epoch 828 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 828 ; accuracy: 0.7466666666666667; loss: 1.7252620458602905\n",
      "Training epoch 829 ; accuracy: 0.9; loss: 0.19459272921085358\n",
      "Validation epoch 829 ; accuracy: 0.7466666666666667; loss: 1.7256619930267334\n",
      "Training epoch 830 ; accuracy: 0.9; loss: 0.19459304213523865\n",
      "Validation epoch 830 ; accuracy: 0.7466666666666667; loss: 1.726076602935791\n",
      "Training epoch 831 ; accuracy: 0.9; loss: 0.1945926547050476\n",
      "Validation epoch 831 ; accuracy: 0.7466666666666667; loss: 1.7264888286590576\n",
      "Training epoch 832 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 832 ; accuracy: 0.7466666666666667; loss: 1.7269033193588257\n",
      "Training epoch 833 ; accuracy: 0.9; loss: 0.1945926696062088\n",
      "Validation epoch 833 ; accuracy: 0.7466666666666667; loss: 1.7273075580596924\n",
      "Training epoch 834 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 834 ; accuracy: 0.7466666666666667; loss: 1.7277040481567383\n",
      "Training epoch 835 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 835 ; accuracy: 0.7466666666666667; loss: 1.7280679941177368\n",
      "Training epoch 836 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 836 ; accuracy: 0.7466666666666667; loss: 1.7284283638000488\n",
      "Training epoch 837 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 837 ; accuracy: 0.7466666666666667; loss: 1.7287906408309937\n",
      "Training epoch 838 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 838 ; accuracy: 0.7466666666666667; loss: 1.7291789054870605\n",
      "Training epoch 839 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 839 ; accuracy: 0.7466666666666667; loss: 1.729554295539856\n",
      "Training epoch 840 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 840 ; accuracy: 0.7466666666666667; loss: 1.729926347732544\n",
      "Training epoch 841 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 841 ; accuracy: 0.7466666666666667; loss: 1.7302966117858887\n",
      "Training epoch 842 ; accuracy: 0.9; loss: 0.19459268450737\n",
      "Validation epoch 842 ; accuracy: 0.7466666666666667; loss: 1.7306840419769287\n",
      "Training epoch 843 ; accuracy: 0.9; loss: 0.1945926547050476\n",
      "Validation epoch 843 ; accuracy: 0.7466666666666667; loss: 1.7310705184936523\n",
      "Training epoch 844 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 844 ; accuracy: 0.7466666666666667; loss: 1.731442928314209\n",
      "Training epoch 845 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 845 ; accuracy: 0.7466666666666667; loss: 1.7318123579025269\n",
      "Training epoch 846 ; accuracy: 0.9; loss: 0.19459262490272522\n",
      "Validation epoch 846 ; accuracy: 0.7466666666666667; loss: 1.7321823835372925\n",
      "Training epoch 847 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 847 ; accuracy: 0.7466666666666667; loss: 1.732547402381897\n",
      "Training epoch 848 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 848 ; accuracy: 0.7466666666666667; loss: 1.7329533100128174\n",
      "Training epoch 849 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 849 ; accuracy: 0.7466666666666667; loss: 1.7333705425262451\n",
      "Training epoch 850 ; accuracy: 0.9; loss: 0.19459249079227448\n",
      "Validation epoch 850 ; accuracy: 0.7466666666666667; loss: 1.7337740659713745\n",
      "Training epoch 851 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 851 ; accuracy: 0.7466666666666667; loss: 1.7341597080230713\n",
      "Training epoch 852 ; accuracy: 0.9; loss: 0.19459268450737\n",
      "Validation epoch 852 ; accuracy: 0.7466666666666667; loss: 1.7345526218414307\n",
      "Training epoch 853 ; accuracy: 0.9; loss: 0.19459274411201477\n",
      "Validation epoch 853 ; accuracy: 0.7466666666666667; loss: 1.7349300384521484\n",
      "Training epoch 854 ; accuracy: 0.9; loss: 0.19459272921085358\n",
      "Validation epoch 854 ; accuracy: 0.7466666666666667; loss: 1.735324740409851\n",
      "Training epoch 855 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 855 ; accuracy: 0.7466666666666667; loss: 1.7357085943222046\n",
      "Training epoch 856 ; accuracy: 0.9; loss: 0.19459249079227448\n",
      "Validation epoch 856 ; accuracy: 0.7466666666666667; loss: 1.7360923290252686\n",
      "Training epoch 857 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 857 ; accuracy: 0.7466666666666667; loss: 1.7364760637283325\n",
      "Training epoch 858 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 858 ; accuracy: 0.7466666666666667; loss: 1.7368327379226685\n",
      "Training epoch 859 ; accuracy: 0.9; loss: 0.19459281861782074\n",
      "Validation epoch 859 ; accuracy: 0.7466666666666667; loss: 1.7371759414672852\n",
      "Training epoch 860 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 860 ; accuracy: 0.7466666666666667; loss: 1.7375209331512451\n",
      "Training epoch 861 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 861 ; accuracy: 0.7466666666666667; loss: 1.7378473281860352\n",
      "Training epoch 862 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 862 ; accuracy: 0.7466666666666667; loss: 1.7381784915924072\n",
      "Training epoch 863 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 863 ; accuracy: 0.7466666666666667; loss: 1.7385083436965942\n",
      "Training epoch 864 ; accuracy: 0.9; loss: 0.19459238648414612\n",
      "Validation epoch 864 ; accuracy: 0.7466666666666667; loss: 1.7388337850570679\n",
      "Training epoch 865 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 865 ; accuracy: 0.7466666666666667; loss: 1.739158272743225\n",
      "Training epoch 866 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 866 ; accuracy: 0.7466666666666667; loss: 1.7394840717315674\n",
      "Training epoch 867 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 867 ; accuracy: 0.7466666666666667; loss: 1.7398154735565186\n",
      "Training epoch 868 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 868 ; accuracy: 0.7466666666666667; loss: 1.7401405572891235\n",
      "Training epoch 869 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 869 ; accuracy: 0.7466666666666667; loss: 1.7404561042785645\n",
      "Training epoch 870 ; accuracy: 0.9; loss: 0.19459252059459686\n",
      "Validation epoch 870 ; accuracy: 0.7466666666666667; loss: 1.7407681941986084\n",
      "Training epoch 871 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 871 ; accuracy: 0.7466666666666667; loss: 1.7410699129104614\n",
      "Training epoch 872 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 872 ; accuracy: 0.7466666666666667; loss: 1.741370677947998\n",
      "Training epoch 873 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 873 ; accuracy: 0.7466666666666667; loss: 1.7416530847549438\n",
      "Training epoch 874 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 874 ; accuracy: 0.7466666666666667; loss: 1.741926908493042\n",
      "Training epoch 875 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 875 ; accuracy: 0.7466666666666667; loss: 1.7422151565551758\n",
      "Training epoch 876 ; accuracy: 0.9; loss: 0.1945924162864685\n",
      "Validation epoch 876 ; accuracy: 0.7466666666666667; loss: 1.7424975633621216\n",
      "Training epoch 877 ; accuracy: 0.9; loss: 0.19459284842014313\n",
      "Validation epoch 877 ; accuracy: 0.7466666666666667; loss: 1.7428025007247925\n",
      "Training epoch 878 ; accuracy: 0.9; loss: 0.19459252059459686\n",
      "Validation epoch 878 ; accuracy: 0.7466666666666667; loss: 1.7431068420410156\n",
      "Training epoch 879 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 879 ; accuracy: 0.7466666666666667; loss: 1.7434557676315308\n",
      "Training epoch 880 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 880 ; accuracy: 0.7466666666666667; loss: 1.7437974214553833\n",
      "Training epoch 881 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 881 ; accuracy: 0.7466666666666667; loss: 1.7441247701644897\n",
      "Training epoch 882 ; accuracy: 0.9; loss: 0.19459232687950134\n",
      "Validation epoch 882 ; accuracy: 0.7466666666666667; loss: 1.7444441318511963\n",
      "Training epoch 883 ; accuracy: 0.9; loss: 0.19459225237369537\n",
      "Validation epoch 883 ; accuracy: 0.7466666666666667; loss: 1.744755506515503\n",
      "Training epoch 884 ; accuracy: 0.9; loss: 0.1945924162864685\n",
      "Validation epoch 884 ; accuracy: 0.7466666666666667; loss: 1.7450746297836304\n",
      "Training epoch 885 ; accuracy: 0.9; loss: 0.1945924162864685\n",
      "Validation epoch 885 ; accuracy: 0.7466666666666667; loss: 1.7453950643539429\n",
      "Training epoch 886 ; accuracy: 0.9; loss: 0.19459231197834015\n",
      "Validation epoch 886 ; accuracy: 0.7466666666666667; loss: 1.7456918954849243\n",
      "Training epoch 887 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 887 ; accuracy: 0.7466666666666667; loss: 1.7459872961044312\n",
      "Training epoch 888 ; accuracy: 0.9; loss: 0.19459238648414612\n",
      "Validation epoch 888 ; accuracy: 0.7466666666666667; loss: 1.74628746509552\n",
      "Training epoch 889 ; accuracy: 0.9; loss: 0.19459274411201477\n",
      "Validation epoch 889 ; accuracy: 0.7466666666666667; loss: 1.746608018875122\n",
      "Training epoch 890 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 890 ; accuracy: 0.7466666666666667; loss: 1.746928095817566\n",
      "Training epoch 891 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 891 ; accuracy: 0.7466666666666667; loss: 1.7472383975982666\n",
      "Training epoch 892 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 892 ; accuracy: 0.7466666666666667; loss: 1.7475430965423584\n",
      "Training epoch 893 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 893 ; accuracy: 0.7466666666666667; loss: 1.7478694915771484\n",
      "Training epoch 894 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 894 ; accuracy: 0.7466666666666667; loss: 1.7481775283813477\n",
      "Training epoch 895 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 895 ; accuracy: 0.7466666666666667; loss: 1.7484773397445679\n",
      "Training epoch 896 ; accuracy: 0.9; loss: 0.19459225237369537\n",
      "Validation epoch 896 ; accuracy: 0.7466666666666667; loss: 1.7487657070159912\n",
      "Training epoch 897 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 897 ; accuracy: 0.7466666666666667; loss: 1.7490556240081787\n",
      "Training epoch 898 ; accuracy: 0.9; loss: 0.1945924162864685\n",
      "Validation epoch 898 ; accuracy: 0.7466666666666667; loss: 1.7493711709976196\n",
      "Training epoch 899 ; accuracy: 0.9; loss: 0.19459232687950134\n",
      "Validation epoch 899 ; accuracy: 0.7466666666666667; loss: 1.7496740818023682\n",
      "Training epoch 900 ; accuracy: 0.9; loss: 0.1948169618844986\n",
      "Validation epoch 900 ; accuracy: 0.7433333333333333; loss: 1.7076579332351685\n",
      "Training epoch 901 ; accuracy: 0.9; loss: 0.19459262490272522\n",
      "Validation epoch 901 ; accuracy: 0.74; loss: 1.6761938333511353\n",
      "Training epoch 902 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 902 ; accuracy: 0.7366666666666667; loss: 1.6514712572097778\n",
      "Training epoch 903 ; accuracy: 0.9; loss: 0.19459481537342072\n",
      "Validation epoch 903 ; accuracy: 0.7366666666666667; loss: 1.6316697597503662\n",
      "Training epoch 904 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 904 ; accuracy: 0.7366666666666667; loss: 1.6163341999053955\n",
      "Training epoch 905 ; accuracy: 0.9; loss: 0.19459643959999084\n",
      "Validation epoch 905 ; accuracy: 0.7366666666666667; loss: 1.6044076681137085\n",
      "Training epoch 906 ; accuracy: 0.9; loss: 0.194600909948349\n",
      "Validation epoch 906 ; accuracy: 0.7366666666666667; loss: 1.5952047109603882\n",
      "Training epoch 907 ; accuracy: 0.9; loss: 0.19460369646549225\n",
      "Validation epoch 907 ; accuracy: 0.7333333333333333; loss: 1.588078260421753\n",
      "Training epoch 908 ; accuracy: 0.9; loss: 0.19461074471473694\n",
      "Validation epoch 908 ; accuracy: 0.7333333333333333; loss: 1.5809953212738037\n",
      "Training epoch 909 ; accuracy: 0.9; loss: 0.19461101293563843\n",
      "Validation epoch 909 ; accuracy: 0.7333333333333333; loss: 1.5749181509017944\n",
      "Training epoch 910 ; accuracy: 0.9; loss: 0.19462770223617554\n",
      "Validation epoch 910 ; accuracy: 0.7366666666666667; loss: 1.5698256492614746\n",
      "Training epoch 911 ; accuracy: 0.9; loss: 0.1946200132369995\n",
      "Validation epoch 911 ; accuracy: 0.7366666666666667; loss: 1.5657702684402466\n",
      "Training epoch 912 ; accuracy: 0.9; loss: 0.19461941719055176\n",
      "Validation epoch 912 ; accuracy: 0.7366666666666667; loss: 1.562355399131775\n",
      "Training epoch 913 ; accuracy: 0.9; loss: 0.19462330639362335\n",
      "Validation epoch 913 ; accuracy: 0.7333333333333333; loss: 1.5595672130584717\n",
      "Training epoch 914 ; accuracy: 0.9; loss: 0.19461564719676971\n",
      "Validation epoch 914 ; accuracy: 0.7366666666666667; loss: 1.5573925971984863\n",
      "Training epoch 915 ; accuracy: 0.9; loss: 0.19462145864963531\n",
      "Validation epoch 915 ; accuracy: 0.7366666666666667; loss: 1.5556782484054565\n",
      "Training epoch 916 ; accuracy: 0.9; loss: 0.19461755454540253\n",
      "Validation epoch 916 ; accuracy: 0.7366666666666667; loss: 1.5546908378601074\n",
      "Training epoch 917 ; accuracy: 0.9; loss: 0.19461938738822937\n",
      "Validation epoch 917 ; accuracy: 0.7333333333333333; loss: 1.55436110496521\n",
      "Training epoch 918 ; accuracy: 0.9; loss: 0.19460827112197876\n",
      "Validation epoch 918 ; accuracy: 0.7333333333333333; loss: 1.5545680522918701\n",
      "Training epoch 919 ; accuracy: 0.9; loss: 0.19460728764533997\n",
      "Validation epoch 919 ; accuracy: 0.7333333333333333; loss: 1.5553174018859863\n",
      "Training epoch 920 ; accuracy: 0.9; loss: 0.1946057230234146\n",
      "Validation epoch 920 ; accuracy: 0.73; loss: 1.5564231872558594\n",
      "Training epoch 921 ; accuracy: 0.9; loss: 0.19460821151733398\n",
      "Validation epoch 921 ; accuracy: 0.73; loss: 1.5580004453659058\n",
      "Training epoch 922 ; accuracy: 0.9; loss: 0.1946011334657669\n",
      "Validation epoch 922 ; accuracy: 0.7333333333333333; loss: 1.55988609790802\n",
      "Training epoch 923 ; accuracy: 0.9; loss: 0.1946086287498474\n",
      "Validation epoch 923 ; accuracy: 0.7333333333333333; loss: 1.5622756481170654\n",
      "Training epoch 924 ; accuracy: 0.9; loss: 0.19459953904151917\n",
      "Validation epoch 924 ; accuracy: 0.7333333333333333; loss: 1.5648484230041504\n",
      "Training epoch 925 ; accuracy: 0.9; loss: 0.19459998607635498\n",
      "Validation epoch 925 ; accuracy: 0.7333333333333333; loss: 1.5676891803741455\n",
      "Training epoch 926 ; accuracy: 0.9; loss: 0.1945975124835968\n",
      "Validation epoch 926 ; accuracy: 0.7333333333333333; loss: 1.5706379413604736\n",
      "Training epoch 927 ; accuracy: 0.9; loss: 0.19459550082683563\n",
      "Validation epoch 927 ; accuracy: 0.7333333333333333; loss: 1.573455810546875\n",
      "Training epoch 928 ; accuracy: 0.9; loss: 0.1945955753326416\n",
      "Validation epoch 928 ; accuracy: 0.7333333333333333; loss: 1.576102375984192\n",
      "Training epoch 929 ; accuracy: 0.9; loss: 0.19459651410579681\n",
      "Validation epoch 929 ; accuracy: 0.7333333333333333; loss: 1.5787498950958252\n",
      "Training epoch 930 ; accuracy: 0.9; loss: 0.19459521770477295\n",
      "Validation epoch 930 ; accuracy: 0.7333333333333333; loss: 1.5813701152801514\n",
      "Training epoch 931 ; accuracy: 0.9; loss: 0.19459493458271027\n",
      "Validation epoch 931 ; accuracy: 0.7333333333333333; loss: 1.5838985443115234\n",
      "Training epoch 932 ; accuracy: 0.9; loss: 0.1945958286523819\n",
      "Validation epoch 932 ; accuracy: 0.7333333333333333; loss: 1.586273431777954\n",
      "Training epoch 933 ; accuracy: 0.9; loss: 0.19459480047225952\n",
      "Validation epoch 933 ; accuracy: 0.7333333333333333; loss: 1.5886023044586182\n",
      "Training epoch 934 ; accuracy: 0.9; loss: 0.1945950835943222\n",
      "Validation epoch 934 ; accuracy: 0.7333333333333333; loss: 1.5908945798873901\n",
      "Training epoch 935 ; accuracy: 0.9; loss: 0.1945943832397461\n",
      "Validation epoch 935 ; accuracy: 0.7333333333333333; loss: 1.593131184577942\n",
      "Training epoch 936 ; accuracy: 0.9; loss: 0.1945943683385849\n",
      "Validation epoch 936 ; accuracy: 0.7333333333333333; loss: 1.5953229665756226\n",
      "Training epoch 937 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 937 ; accuracy: 0.7333333333333333; loss: 1.5974938869476318\n",
      "Training epoch 938 ; accuracy: 0.9; loss: 0.194594606757164\n",
      "Validation epoch 938 ; accuracy: 0.7333333333333333; loss: 1.5996400117874146\n",
      "Training epoch 939 ; accuracy: 0.9; loss: 0.1945934295654297\n",
      "Validation epoch 939 ; accuracy: 0.7333333333333333; loss: 1.6016874313354492\n",
      "Training epoch 940 ; accuracy: 0.9; loss: 0.19459368288516998\n",
      "Validation epoch 940 ; accuracy: 0.7333333333333333; loss: 1.6036473512649536\n",
      "Training epoch 941 ; accuracy: 0.9; loss: 0.194594144821167\n",
      "Validation epoch 941 ; accuracy: 0.7333333333333333; loss: 1.6055392026901245\n",
      "Training epoch 942 ; accuracy: 0.9; loss: 0.1945948302745819\n",
      "Validation epoch 942 ; accuracy: 0.7333333333333333; loss: 1.6073966026306152\n",
      "Training epoch 943 ; accuracy: 0.9; loss: 0.19459398090839386\n",
      "Validation epoch 943 ; accuracy: 0.7333333333333333; loss: 1.609197974205017\n",
      "Training epoch 944 ; accuracy: 0.9; loss: 0.19459415972232819\n",
      "Validation epoch 944 ; accuracy: 0.7366666666666667; loss: 1.6109588146209717\n",
      "Training epoch 945 ; accuracy: 0.9; loss: 0.19459304213523865\n",
      "Validation epoch 945 ; accuracy: 0.7366666666666667; loss: 1.6126247644424438\n",
      "Training epoch 946 ; accuracy: 0.9; loss: 0.19459311664104462\n",
      "Validation epoch 946 ; accuracy: 0.7366666666666667; loss: 1.614211916923523\n",
      "Training epoch 947 ; accuracy: 0.9; loss: 0.194593146443367\n",
      "Validation epoch 947 ; accuracy: 0.7366666666666667; loss: 1.6156972646713257\n",
      "Training epoch 948 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 948 ; accuracy: 0.7366666666666667; loss: 1.617068886756897\n",
      "Training epoch 949 ; accuracy: 0.9; loss: 0.1945943385362625\n",
      "Validation epoch 949 ; accuracy: 0.7366666666666667; loss: 1.618443250656128\n",
      "Training epoch 950 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 950 ; accuracy: 0.7366666666666667; loss: 1.6197443008422852\n",
      "Training epoch 951 ; accuracy: 0.9; loss: 0.19509601593017578\n",
      "Validation epoch 951 ; accuracy: 0.7366666666666667; loss: 1.601824164390564\n",
      "Training epoch 952 ; accuracy: 0.9; loss: 0.19459447264671326\n",
      "Validation epoch 952 ; accuracy: 0.74; loss: 1.589166522026062\n",
      "Training epoch 953 ; accuracy: 0.9; loss: 0.19459448754787445\n",
      "Validation epoch 953 ; accuracy: 0.7433333333333333; loss: 1.5803688764572144\n",
      "Training epoch 954 ; accuracy: 0.9; loss: 0.19459736347198486\n",
      "Validation epoch 954 ; accuracy: 0.7466666666666667; loss: 1.5748218297958374\n",
      "Training epoch 955 ; accuracy: 0.9; loss: 0.19459502398967743\n",
      "Validation epoch 955 ; accuracy: 0.7466666666666667; loss: 1.5715014934539795\n",
      "Training epoch 956 ; accuracy: 0.9; loss: 0.19459715485572815\n",
      "Validation epoch 956 ; accuracy: 0.75; loss: 1.5697662830352783\n",
      "Training epoch 957 ; accuracy: 0.9; loss: 0.19460007548332214\n",
      "Validation epoch 957 ; accuracy: 0.75; loss: 1.569197177886963\n",
      "Training epoch 958 ; accuracy: 0.9; loss: 0.1946002095937729\n",
      "Validation epoch 958 ; accuracy: 0.75; loss: 1.5694575309753418\n",
      "Training epoch 959 ; accuracy: 0.9; loss: 0.1946149319410324\n",
      "Validation epoch 959 ; accuracy: 0.7466666666666667; loss: 1.569849967956543\n",
      "Training epoch 960 ; accuracy: 0.9; loss: 0.19460560381412506\n",
      "Validation epoch 960 ; accuracy: 0.7433333333333333; loss: 1.5706754922866821\n",
      "Training epoch 961 ; accuracy: 0.9; loss: 0.19461782276630402\n",
      "Validation epoch 961 ; accuracy: 0.7433333333333333; loss: 1.5716506242752075\n",
      "Training epoch 962 ; accuracy: 0.9; loss: 0.19461242854595184\n",
      "Validation epoch 962 ; accuracy: 0.7433333333333333; loss: 1.572906255722046\n",
      "Training epoch 963 ; accuracy: 0.9; loss: 0.19463075697422028\n",
      "Validation epoch 963 ; accuracy: 0.74; loss: 1.5751696825027466\n",
      "Training epoch 964 ; accuracy: 0.9; loss: 0.19461719691753387\n",
      "Validation epoch 964 ; accuracy: 0.7433333333333333; loss: 1.5776487588882446\n",
      "Training epoch 965 ; accuracy: 0.9; loss: 0.1946103572845459\n",
      "Validation epoch 965 ; accuracy: 0.74; loss: 1.580128788948059\n",
      "Training epoch 966 ; accuracy: 0.9; loss: 0.19461014866828918\n",
      "Validation epoch 966 ; accuracy: 0.7433333333333333; loss: 1.5827819108963013\n",
      "Training epoch 967 ; accuracy: 0.9; loss: 0.19460918009281158\n",
      "Validation epoch 967 ; accuracy: 0.7433333333333333; loss: 1.585511326789856\n",
      "Training epoch 968 ; accuracy: 0.9; loss: 0.19461219012737274\n",
      "Validation epoch 968 ; accuracy: 0.7433333333333333; loss: 1.5885748863220215\n",
      "Training epoch 969 ; accuracy: 0.9; loss: 0.1946020871400833\n",
      "Validation epoch 969 ; accuracy: 0.7433333333333333; loss: 1.5918031930923462\n",
      "Training epoch 970 ; accuracy: 0.9; loss: 0.19460038840770721\n",
      "Validation epoch 970 ; accuracy: 0.7433333333333333; loss: 1.5951347351074219\n",
      "Training epoch 971 ; accuracy: 0.9; loss: 0.19459785521030426\n",
      "Validation epoch 971 ; accuracy: 0.7433333333333333; loss: 1.598494291305542\n",
      "Training epoch 972 ; accuracy: 0.9; loss: 0.19459766149520874\n",
      "Validation epoch 972 ; accuracy: 0.7433333333333333; loss: 1.6018319129943848\n",
      "Training epoch 973 ; accuracy: 0.9; loss: 0.1945987194776535\n",
      "Validation epoch 973 ; accuracy: 0.7433333333333333; loss: 1.6051530838012695\n",
      "Training epoch 974 ; accuracy: 0.9; loss: 0.1945982426404953\n",
      "Validation epoch 974 ; accuracy: 0.74; loss: 1.6084719896316528\n",
      "Training epoch 975 ; accuracy: 0.9; loss: 0.19459639489650726\n",
      "Validation epoch 975 ; accuracy: 0.7433333333333333; loss: 1.6117467880249023\n",
      "Training epoch 976 ; accuracy: 0.9; loss: 0.19459684193134308\n",
      "Validation epoch 976 ; accuracy: 0.7433333333333333; loss: 1.6149629354476929\n",
      "Training epoch 977 ; accuracy: 0.9; loss: 0.19459694623947144\n",
      "Validation epoch 977 ; accuracy: 0.7433333333333333; loss: 1.6181060075759888\n",
      "Training epoch 978 ; accuracy: 0.9; loss: 0.1945958286523819\n",
      "Validation epoch 978 ; accuracy: 0.7433333333333333; loss: 1.6211578845977783\n",
      "Training epoch 979 ; accuracy: 0.9; loss: 0.19459529221057892\n",
      "Validation epoch 979 ; accuracy: 0.7433333333333333; loss: 1.6241077184677124\n",
      "Training epoch 980 ; accuracy: 0.9; loss: 0.19459445774555206\n",
      "Validation epoch 980 ; accuracy: 0.7433333333333333; loss: 1.6269172430038452\n",
      "Training epoch 981 ; accuracy: 0.9; loss: 0.1945943981409073\n",
      "Validation epoch 981 ; accuracy: 0.7433333333333333; loss: 1.629564642906189\n",
      "Training epoch 982 ; accuracy: 0.9; loss: 0.19459475576877594\n",
      "Validation epoch 982 ; accuracy: 0.7433333333333333; loss: 1.6321004629135132\n",
      "Training epoch 983 ; accuracy: 0.9; loss: 0.19459490478038788\n",
      "Validation epoch 983 ; accuracy: 0.7433333333333333; loss: 1.634482741355896\n",
      "Training epoch 984 ; accuracy: 0.9; loss: 0.19459399580955505\n",
      "Validation epoch 984 ; accuracy: 0.7433333333333333; loss: 1.6367267370224\n",
      "Training epoch 985 ; accuracy: 0.9; loss: 0.19459453225135803\n",
      "Validation epoch 985 ; accuracy: 0.7433333333333333; loss: 1.6388615369796753\n",
      "Training epoch 986 ; accuracy: 0.9; loss: 0.19459445774555206\n",
      "Validation epoch 986 ; accuracy: 0.7433333333333333; loss: 1.6409125328063965\n",
      "Training epoch 987 ; accuracy: 0.9; loss: 0.19459368288516998\n",
      "Validation epoch 987 ; accuracy: 0.7433333333333333; loss: 1.6428556442260742\n",
      "Training epoch 988 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 988 ; accuracy: 0.74; loss: 1.6447163820266724\n",
      "Training epoch 989 ; accuracy: 0.9; loss: 0.19459424912929535\n",
      "Validation epoch 989 ; accuracy: 0.74; loss: 1.646489143371582\n",
      "Training epoch 990 ; accuracy: 0.9; loss: 0.19459356367588043\n",
      "Validation epoch 990 ; accuracy: 0.74; loss: 1.6481784582138062\n",
      "Training epoch 991 ; accuracy: 0.9; loss: 0.19459326565265656\n",
      "Validation epoch 991 ; accuracy: 0.74; loss: 1.6497753858566284\n",
      "Training epoch 992 ; accuracy: 0.9; loss: 0.1945933997631073\n",
      "Validation epoch 992 ; accuracy: 0.74; loss: 1.6512641906738281\n",
      "Training epoch 993 ; accuracy: 0.9; loss: 0.19459353387355804\n",
      "Validation epoch 993 ; accuracy: 0.74; loss: 1.6526728868484497\n",
      "Training epoch 994 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 994 ; accuracy: 0.74; loss: 1.6539998054504395\n",
      "Training epoch 995 ; accuracy: 0.9; loss: 0.19459320604801178\n",
      "Validation epoch 995 ; accuracy: 0.74; loss: 1.6552542448043823\n",
      "Training epoch 996 ; accuracy: 0.9; loss: 0.19459329545497894\n",
      "Validation epoch 996 ; accuracy: 0.74; loss: 1.6564387083053589\n",
      "Training epoch 997 ; accuracy: 0.9; loss: 0.19459351897239685\n",
      "Validation epoch 997 ; accuracy: 0.74; loss: 1.6575697660446167\n",
      "Training epoch 998 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 998 ; accuracy: 0.74; loss: 1.6586365699768066\n",
      "Training epoch 999 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 999 ; accuracy: 0.74; loss: 1.6596492528915405\n",
      "Training epoch 1000 ; accuracy: 0.9; loss: 0.19459323585033417\n",
      "Validation epoch 1000 ; accuracy: 0.74; loss: 1.6606184244155884\n",
      "Training epoch 1001 ; accuracy: 0.9; loss: 0.19459311664104462\n",
      "Validation epoch 1001 ; accuracy: 0.74; loss: 1.661535620689392\n",
      "Training epoch 1002 ; accuracy: 0.9; loss: 0.19459281861782074\n",
      "Validation epoch 1002 ; accuracy: 0.74; loss: 1.6624032258987427\n",
      "Training epoch 1003 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 1003 ; accuracy: 0.74; loss: 1.6632517576217651\n",
      "Training epoch 1004 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 1004 ; accuracy: 0.74; loss: 1.6640621423721313\n",
      "Training epoch 1005 ; accuracy: 0.9; loss: 0.1945933848619461\n",
      "Validation epoch 1005 ; accuracy: 0.74; loss: 1.6648188829421997\n",
      "Training epoch 1006 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1006 ; accuracy: 0.74; loss: 1.6655478477478027\n",
      "Training epoch 1007 ; accuracy: 0.9; loss: 0.1945933699607849\n",
      "Validation epoch 1007 ; accuracy: 0.74; loss: 1.6662510633468628\n",
      "Training epoch 1008 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 1008 ; accuracy: 0.74; loss: 1.6669189929962158\n",
      "Training epoch 1009 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 1009 ; accuracy: 0.74; loss: 1.6675463914871216\n",
      "Training epoch 1010 ; accuracy: 0.9; loss: 0.19459329545497894\n",
      "Validation epoch 1010 ; accuracy: 0.74; loss: 1.668176293373108\n",
      "Training epoch 1011 ; accuracy: 0.9; loss: 0.1945928931236267\n",
      "Validation epoch 1011 ; accuracy: 0.74; loss: 1.6687897443771362\n",
      "Training epoch 1012 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1012 ; accuracy: 0.74; loss: 1.669382095336914\n",
      "Training epoch 1013 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 1013 ; accuracy: 0.74; loss: 1.6699544191360474\n",
      "Training epoch 1014 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 1014 ; accuracy: 0.74; loss: 1.6705169677734375\n",
      "Training epoch 1015 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1015 ; accuracy: 0.74; loss: 1.6710543632507324\n",
      "Training epoch 1016 ; accuracy: 0.9; loss: 0.1945931762456894\n",
      "Validation epoch 1016 ; accuracy: 0.74; loss: 1.671574354171753\n",
      "Training epoch 1017 ; accuracy: 0.9; loss: 0.19459275901317596\n",
      "Validation epoch 1017 ; accuracy: 0.74; loss: 1.6720771789550781\n",
      "Training epoch 1018 ; accuracy: 0.9; loss: 0.1945931315422058\n",
      "Validation epoch 1018 ; accuracy: 0.74; loss: 1.6725939512252808\n",
      "Training epoch 1019 ; accuracy: 0.9; loss: 0.1945931613445282\n",
      "Validation epoch 1019 ; accuracy: 0.74; loss: 1.6730952262878418\n",
      "Training epoch 1020 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 1020 ; accuracy: 0.74; loss: 1.673577070236206\n",
      "Training epoch 1021 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 1021 ; accuracy: 0.74; loss: 1.6740710735321045\n",
      "Training epoch 1022 ; accuracy: 0.9; loss: 0.19459301233291626\n",
      "Validation epoch 1022 ; accuracy: 0.74; loss: 1.6745601892471313\n",
      "Training epoch 1023 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 1023 ; accuracy: 0.74; loss: 1.6750351190567017\n",
      "Training epoch 1024 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 1024 ; accuracy: 0.74; loss: 1.6754966974258423\n",
      "Training epoch 1025 ; accuracy: 0.9; loss: 0.19459307193756104\n",
      "Validation epoch 1025 ; accuracy: 0.74; loss: 1.6759635210037231\n",
      "Training epoch 1026 ; accuracy: 0.9; loss: 0.19459281861782074\n",
      "Validation epoch 1026 ; accuracy: 0.74; loss: 1.6764335632324219\n",
      "Training epoch 1027 ; accuracy: 0.9; loss: 0.19459301233291626\n",
      "Validation epoch 1027 ; accuracy: 0.74; loss: 1.6769204139709473\n",
      "Training epoch 1028 ; accuracy: 0.9; loss: 0.19459268450737\n",
      "Validation epoch 1028 ; accuracy: 0.74; loss: 1.6773966550827026\n",
      "Training epoch 1029 ; accuracy: 0.9; loss: 0.19459252059459686\n",
      "Validation epoch 1029 ; accuracy: 0.74; loss: 1.6778494119644165\n",
      "Training epoch 1030 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 1030 ; accuracy: 0.74; loss: 1.6782991886138916\n",
      "Training epoch 1031 ; accuracy: 0.9; loss: 0.1945929229259491\n",
      "Validation epoch 1031 ; accuracy: 0.74; loss: 1.6787577867507935\n",
      "Training epoch 1032 ; accuracy: 0.9; loss: 0.1945928931236267\n",
      "Validation epoch 1032 ; accuracy: 0.74; loss: 1.6792160272598267\n",
      "Training epoch 1033 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 1033 ; accuracy: 0.74; loss: 1.6796488761901855\n",
      "Training epoch 1034 ; accuracy: 0.9; loss: 0.19459268450737\n",
      "Validation epoch 1034 ; accuracy: 0.74; loss: 1.6800756454467773\n",
      "Training epoch 1035 ; accuracy: 0.9; loss: 0.19459261000156403\n",
      "Validation epoch 1035 ; accuracy: 0.74; loss: 1.680490493774414\n",
      "Training epoch 1036 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 1036 ; accuracy: 0.74; loss: 1.680892825126648\n",
      "Training epoch 1037 ; accuracy: 0.9; loss: 0.19459298253059387\n",
      "Validation epoch 1037 ; accuracy: 0.74; loss: 1.6812652349472046\n",
      "Training epoch 1038 ; accuracy: 0.9; loss: 0.19459261000156403\n",
      "Validation epoch 1038 ; accuracy: 0.74; loss: 1.6816236972808838\n",
      "Training epoch 1039 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 1039 ; accuracy: 0.74; loss: 1.6819815635681152\n",
      "Training epoch 1040 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 1040 ; accuracy: 0.74; loss: 1.6823441982269287\n",
      "Training epoch 1041 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 1041 ; accuracy: 0.74; loss: 1.6827150583267212\n",
      "Training epoch 1042 ; accuracy: 0.9; loss: 0.19459274411201477\n",
      "Validation epoch 1042 ; accuracy: 0.74; loss: 1.6831037998199463\n",
      "Training epoch 1043 ; accuracy: 0.9; loss: 0.19459308683872223\n",
      "Validation epoch 1043 ; accuracy: 0.74; loss: 1.6835013628005981\n",
      "Training epoch 1044 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 1044 ; accuracy: 0.74; loss: 1.6839015483856201\n",
      "Training epoch 1045 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 1045 ; accuracy: 0.74; loss: 1.6843012571334839\n",
      "Training epoch 1046 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1046 ; accuracy: 0.74; loss: 1.6847023963928223\n",
      "Training epoch 1047 ; accuracy: 0.9; loss: 0.19459237158298492\n",
      "Validation epoch 1047 ; accuracy: 0.74; loss: 1.6851049661636353\n",
      "Training epoch 1048 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1048 ; accuracy: 0.74; loss: 1.6855076551437378\n",
      "Training epoch 1049 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 1049 ; accuracy: 0.74; loss: 1.685879111289978\n",
      "Training epoch 1050 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 1050 ; accuracy: 0.74; loss: 1.686252474784851\n",
      "Training epoch 1051 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 1051 ; accuracy: 0.74; loss: 1.6866309642791748\n",
      "Training epoch 1052 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 1052 ; accuracy: 0.74; loss: 1.687024474143982\n",
      "Training epoch 1053 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 1053 ; accuracy: 0.74; loss: 1.6874080896377563\n",
      "Training epoch 1054 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 1054 ; accuracy: 0.74; loss: 1.6877918243408203\n",
      "Training epoch 1055 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 1055 ; accuracy: 0.74; loss: 1.6881675720214844\n",
      "Training epoch 1056 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 1056 ; accuracy: 0.74; loss: 1.6885402202606201\n",
      "Training epoch 1057 ; accuracy: 0.9; loss: 0.19459308683872223\n",
      "Validation epoch 1057 ; accuracy: 0.74; loss: 1.6889148950576782\n",
      "Training epoch 1058 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1058 ; accuracy: 0.74; loss: 1.689290165901184\n",
      "Training epoch 1059 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1059 ; accuracy: 0.74; loss: 1.6896687746047974\n",
      "Training epoch 1060 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 1060 ; accuracy: 0.74; loss: 1.6900441646575928\n",
      "Training epoch 1061 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 1061 ; accuracy: 0.74; loss: 1.6904369592666626\n",
      "Training epoch 1062 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 1062 ; accuracy: 0.74; loss: 1.6908308267593384\n",
      "Training epoch 1063 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1063 ; accuracy: 0.74; loss: 1.6912227869033813\n",
      "Training epoch 1064 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 1064 ; accuracy: 0.74; loss: 1.6916147470474243\n",
      "Training epoch 1065 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1065 ; accuracy: 0.74; loss: 1.6920021772384644\n",
      "Training epoch 1066 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 1066 ; accuracy: 0.74; loss: 1.6924011707305908\n",
      "Training epoch 1067 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1067 ; accuracy: 0.74; loss: 1.6927889585494995\n",
      "Training epoch 1068 ; accuracy: 0.9; loss: 0.1945921927690506\n",
      "Validation epoch 1068 ; accuracy: 0.74; loss: 1.6931625604629517\n",
      "Training epoch 1069 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 1069 ; accuracy: 0.74; loss: 1.6935378313064575\n",
      "Training epoch 1070 ; accuracy: 0.9; loss: 0.19459255039691925\n",
      "Validation epoch 1070 ; accuracy: 0.74; loss: 1.6939222812652588\n",
      "Training epoch 1071 ; accuracy: 0.9; loss: 0.1945924162864685\n",
      "Validation epoch 1071 ; accuracy: 0.74; loss: 1.6942952871322632\n",
      "Training epoch 1072 ; accuracy: 0.9; loss: 0.19459232687950134\n",
      "Validation epoch 1072 ; accuracy: 0.74; loss: 1.6946563720703125\n",
      "Training epoch 1073 ; accuracy: 0.9; loss: 0.19459238648414612\n",
      "Validation epoch 1073 ; accuracy: 0.74; loss: 1.6950206756591797\n",
      "Training epoch 1074 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1074 ; accuracy: 0.74; loss: 1.695395588874817\n",
      "Training epoch 1075 ; accuracy: 0.9; loss: 0.19459228217601776\n",
      "Validation epoch 1075 ; accuracy: 0.74; loss: 1.6957499980926514\n",
      "Training epoch 1076 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 1076 ; accuracy: 0.74; loss: 1.6961078643798828\n",
      "Training epoch 1077 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 1077 ; accuracy: 0.74; loss: 1.6964688301086426\n",
      "Training epoch 1078 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1078 ; accuracy: 0.74; loss: 1.6968231201171875\n",
      "Training epoch 1079 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 1079 ; accuracy: 0.74; loss: 1.6971755027770996\n",
      "Training epoch 1080 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1080 ; accuracy: 0.74; loss: 1.697517991065979\n",
      "Training epoch 1081 ; accuracy: 0.9; loss: 0.19462883472442627\n",
      "Validation epoch 1081 ; accuracy: 0.7366666666666667; loss: 1.6953620910644531\n",
      "Training epoch 1082 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 1082 ; accuracy: 0.74; loss: 1.697537899017334\n",
      "Training epoch 1083 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 1083 ; accuracy: 0.7333333333333333; loss: 1.6967231035232544\n",
      "Training epoch 1084 ; accuracy: 0.9; loss: 0.19459328055381775\n",
      "Validation epoch 1084 ; accuracy: 0.7333333333333333; loss: 1.6949456930160522\n",
      "Training epoch 1085 ; accuracy: 0.9; loss: 0.1945960521697998\n",
      "Validation epoch 1085 ; accuracy: 0.7333333333333333; loss: 1.6947851181030273\n",
      "Training epoch 1086 ; accuracy: 0.9; loss: 0.19459836184978485\n",
      "Validation epoch 1086 ; accuracy: 0.7333333333333333; loss: 1.6954504251480103\n",
      "Training epoch 1087 ; accuracy: 0.9; loss: 0.1946001797914505\n",
      "Validation epoch 1087 ; accuracy: 0.7333333333333333; loss: 1.6962770223617554\n",
      "Training epoch 1088 ; accuracy: 0.9; loss: 0.1946064531803131\n",
      "Validation epoch 1088 ; accuracy: 0.7333333333333333; loss: 1.6964375972747803\n",
      "Training epoch 1089 ; accuracy: 0.9; loss: 0.1946108192205429\n",
      "Validation epoch 1089 ; accuracy: 0.7333333333333333; loss: 1.6967934370040894\n",
      "Training epoch 1090 ; accuracy: 0.9; loss: 0.1946108043193817\n",
      "Validation epoch 1090 ; accuracy: 0.7333333333333333; loss: 1.6966207027435303\n",
      "Training epoch 1091 ; accuracy: 0.9; loss: 0.1946125030517578\n",
      "Validation epoch 1091 ; accuracy: 0.7333333333333333; loss: 1.6955745220184326\n",
      "Training epoch 1092 ; accuracy: 0.9; loss: 0.19460707902908325\n",
      "Validation epoch 1092 ; accuracy: 0.7333333333333333; loss: 1.6942427158355713\n",
      "Training epoch 1093 ; accuracy: 0.9; loss: 0.19460982084274292\n",
      "Validation epoch 1093 ; accuracy: 0.7333333333333333; loss: 1.692557692527771\n",
      "Training epoch 1094 ; accuracy: 0.9; loss: 0.1946050077676773\n",
      "Validation epoch 1094 ; accuracy: 0.7333333333333333; loss: 1.690904974937439\n",
      "Training epoch 1095 ; accuracy: 0.9; loss: 0.19460263848304749\n",
      "Validation epoch 1095 ; accuracy: 0.7333333333333333; loss: 1.6896039247512817\n",
      "Training epoch 1096 ; accuracy: 0.9; loss: 0.19459989666938782\n",
      "Validation epoch 1096 ; accuracy: 0.7333333333333333; loss: 1.6886488199234009\n",
      "Training epoch 1097 ; accuracy: 0.9; loss: 0.1945982724428177\n",
      "Validation epoch 1097 ; accuracy: 0.73; loss: 1.6880625486373901\n",
      "Training epoch 1098 ; accuracy: 0.9; loss: 0.19459736347198486\n",
      "Validation epoch 1098 ; accuracy: 0.73; loss: 1.6878708600997925\n",
      "Training epoch 1099 ; accuracy: 0.9; loss: 0.19459667801856995\n",
      "Validation epoch 1099 ; accuracy: 0.7366666666666667; loss: 1.6880061626434326\n",
      "Training epoch 1100 ; accuracy: 0.9; loss: 0.19459639489650726\n",
      "Validation epoch 1100 ; accuracy: 0.7366666666666667; loss: 1.6884310245513916\n",
      "Training epoch 1101 ; accuracy: 0.9; loss: 0.19459639489650726\n",
      "Validation epoch 1101 ; accuracy: 0.7366666666666667; loss: 1.6889970302581787\n",
      "Training epoch 1102 ; accuracy: 0.9; loss: 0.1945945769548416\n",
      "Validation epoch 1102 ; accuracy: 0.7366666666666667; loss: 1.6894519329071045\n",
      "Training epoch 1103 ; accuracy: 0.9; loss: 0.194594606757164\n",
      "Validation epoch 1103 ; accuracy: 0.7366666666666667; loss: 1.6900652647018433\n",
      "Training epoch 1104 ; accuracy: 0.9; loss: 0.19459538161754608\n",
      "Validation epoch 1104 ; accuracy: 0.7366666666666667; loss: 1.6908029317855835\n",
      "Training epoch 1105 ; accuracy: 0.9; loss: 0.1945946365594864\n",
      "Validation epoch 1105 ; accuracy: 0.7366666666666667; loss: 1.6916475296020508\n",
      "Training epoch 1106 ; accuracy: 0.9; loss: 0.19459393620491028\n",
      "Validation epoch 1106 ; accuracy: 0.7366666666666667; loss: 1.6925379037857056\n",
      "Training epoch 1107 ; accuracy: 0.9; loss: 0.19459402561187744\n",
      "Validation epoch 1107 ; accuracy: 0.7366666666666667; loss: 1.693471074104309\n",
      "Training epoch 1108 ; accuracy: 0.9; loss: 0.1945938915014267\n",
      "Validation epoch 1108 ; accuracy: 0.7366666666666667; loss: 1.6944242715835571\n",
      "Training epoch 1109 ; accuracy: 0.9; loss: 0.19459430873394012\n",
      "Validation epoch 1109 ; accuracy: 0.7366666666666667; loss: 1.6952025890350342\n",
      "Training epoch 1110 ; accuracy: 0.9; loss: 0.19459396600723267\n",
      "Validation epoch 1110 ; accuracy: 0.7366666666666667; loss: 1.695995807647705\n",
      "Training epoch 1111 ; accuracy: 0.9; loss: 0.1945938915014267\n",
      "Validation epoch 1111 ; accuracy: 0.7366666666666667; loss: 1.6967847347259521\n",
      "Training epoch 1112 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 1112 ; accuracy: 0.7366666666666667; loss: 1.6975573301315308\n",
      "Training epoch 1113 ; accuracy: 0.9; loss: 0.19459381699562073\n",
      "Validation epoch 1113 ; accuracy: 0.7366666666666667; loss: 1.6983393430709839\n",
      "Training epoch 1114 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 1114 ; accuracy: 0.74; loss: 1.6991050243377686\n",
      "Training epoch 1115 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 1115 ; accuracy: 0.74; loss: 1.6998486518859863\n",
      "Training epoch 1116 ; accuracy: 0.9; loss: 0.1945934295654297\n",
      "Validation epoch 1116 ; accuracy: 0.74; loss: 1.7005829811096191\n",
      "Training epoch 1117 ; accuracy: 0.9; loss: 0.19459350407123566\n",
      "Validation epoch 1117 ; accuracy: 0.74; loss: 1.7013285160064697\n",
      "Training epoch 1118 ; accuracy: 0.9; loss: 0.19459350407123566\n",
      "Validation epoch 1118 ; accuracy: 0.74; loss: 1.702083945274353\n",
      "Training epoch 1119 ; accuracy: 0.9; loss: 0.19459311664104462\n",
      "Validation epoch 1119 ; accuracy: 0.74; loss: 1.7028166055679321\n",
      "Training epoch 1120 ; accuracy: 0.9; loss: 0.19459335505962372\n",
      "Validation epoch 1120 ; accuracy: 0.74; loss: 1.7035431861877441\n",
      "Training epoch 1121 ; accuracy: 0.9; loss: 0.19459334015846252\n",
      "Validation epoch 1121 ; accuracy: 0.74; loss: 1.704261302947998\n",
      "Training epoch 1122 ; accuracy: 0.9; loss: 0.19459287822246552\n",
      "Validation epoch 1122 ; accuracy: 0.74; loss: 1.7049592733383179\n",
      "Training epoch 1123 ; accuracy: 0.9; loss: 0.19459332525730133\n",
      "Validation epoch 1123 ; accuracy: 0.74; loss: 1.7056034803390503\n",
      "Training epoch 1124 ; accuracy: 0.9; loss: 0.19459286332130432\n",
      "Validation epoch 1124 ; accuracy: 0.74; loss: 1.7062199115753174\n",
      "Training epoch 1125 ; accuracy: 0.9; loss: 0.19459344446659088\n",
      "Validation epoch 1125 ; accuracy: 0.74; loss: 1.706861138343811\n",
      "Training epoch 1126 ; accuracy: 0.9; loss: 0.1945931762456894\n",
      "Validation epoch 1126 ; accuracy: 0.74; loss: 1.7074803113937378\n",
      "Training epoch 1127 ; accuracy: 0.9; loss: 0.19459319114685059\n",
      "Validation epoch 1127 ; accuracy: 0.74; loss: 1.7080878019332886\n",
      "Training epoch 1128 ; accuracy: 0.9; loss: 0.19459301233291626\n",
      "Validation epoch 1128 ; accuracy: 0.74; loss: 1.7087005376815796\n",
      "Training epoch 1129 ; accuracy: 0.9; loss: 0.19459356367588043\n",
      "Validation epoch 1129 ; accuracy: 0.74; loss: 1.7093064785003662\n",
      "Training epoch 1130 ; accuracy: 0.9; loss: 0.19459328055381775\n",
      "Validation epoch 1130 ; accuracy: 0.74; loss: 1.7099169492721558\n",
      "Training epoch 1131 ; accuracy: 0.9; loss: 0.19459298253059387\n",
      "Validation epoch 1131 ; accuracy: 0.74; loss: 1.71052086353302\n",
      "Training epoch 1132 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 1132 ; accuracy: 0.74; loss: 1.7111001014709473\n",
      "Training epoch 1133 ; accuracy: 0.9; loss: 0.19459295272827148\n",
      "Validation epoch 1133 ; accuracy: 0.74; loss: 1.7116641998291016\n",
      "Training epoch 1134 ; accuracy: 0.9; loss: 0.1945933848619461\n",
      "Validation epoch 1134 ; accuracy: 0.74; loss: 1.7122273445129395\n",
      "Training epoch 1135 ; accuracy: 0.9; loss: 0.19459305703639984\n",
      "Validation epoch 1135 ; accuracy: 0.74; loss: 1.7127772569656372\n",
      "Training epoch 1136 ; accuracy: 0.9; loss: 0.1945931762456894\n",
      "Validation epoch 1136 ; accuracy: 0.74; loss: 1.7133216857910156\n",
      "Training epoch 1137 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1137 ; accuracy: 0.74; loss: 1.7138502597808838\n",
      "Training epoch 1138 ; accuracy: 0.9; loss: 0.1945929080247879\n",
      "Validation epoch 1138 ; accuracy: 0.74; loss: 1.714356541633606\n",
      "Training epoch 1139 ; accuracy: 0.9; loss: 0.19459295272827148\n",
      "Validation epoch 1139 ; accuracy: 0.74; loss: 1.7148551940917969\n",
      "Training epoch 1140 ; accuracy: 0.9; loss: 0.1945931315422058\n",
      "Validation epoch 1140 ; accuracy: 0.74; loss: 1.7153511047363281\n",
      "Training epoch 1141 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 1141 ; accuracy: 0.74; loss: 1.7158534526824951\n",
      "Training epoch 1142 ; accuracy: 0.9; loss: 0.19459322094917297\n",
      "Validation epoch 1142 ; accuracy: 0.74; loss: 1.716372013092041\n",
      "Training epoch 1143 ; accuracy: 0.9; loss: 0.19459331035614014\n",
      "Validation epoch 1143 ; accuracy: 0.74; loss: 1.7169100046157837\n",
      "Training epoch 1144 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1144 ; accuracy: 0.74; loss: 1.717437505722046\n",
      "Training epoch 1145 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1145 ; accuracy: 0.74; loss: 1.7179465293884277\n",
      "Training epoch 1146 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 1146 ; accuracy: 0.74; loss: 1.718438982963562\n",
      "Training epoch 1147 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 1147 ; accuracy: 0.74; loss: 1.7189245223999023\n",
      "Training epoch 1148 ; accuracy: 0.9; loss: 0.19459280371665955\n",
      "Validation epoch 1148 ; accuracy: 0.74; loss: 1.7194092273712158\n",
      "Training epoch 1149 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 1149 ; accuracy: 0.74; loss: 1.7198947668075562\n",
      "Training epoch 1150 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 1150 ; accuracy: 0.74; loss: 1.7203863859176636\n",
      "Training epoch 1151 ; accuracy: 0.9; loss: 0.19459272921085358\n",
      "Validation epoch 1151 ; accuracy: 0.74; loss: 1.720875859260559\n",
      "Training epoch 1152 ; accuracy: 0.9; loss: 0.1945929080247879\n",
      "Validation epoch 1152 ; accuracy: 0.74; loss: 1.7213586568832397\n",
      "Training epoch 1153 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 1153 ; accuracy: 0.74; loss: 1.7218555212020874\n",
      "Training epoch 1154 ; accuracy: 0.9; loss: 0.1945929229259491\n",
      "Validation epoch 1154 ; accuracy: 0.74; loss: 1.7223429679870605\n",
      "Training epoch 1155 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 1155 ; accuracy: 0.74; loss: 1.7228310108184814\n",
      "Training epoch 1156 ; accuracy: 0.9; loss: 0.1945929229259491\n",
      "Validation epoch 1156 ; accuracy: 0.74; loss: 1.7233268022537231\n",
      "Training epoch 1157 ; accuracy: 0.9; loss: 0.19459296762943268\n",
      "Validation epoch 1157 ; accuracy: 0.74; loss: 1.7238150835037231\n",
      "Training epoch 1158 ; accuracy: 0.9; loss: 0.19459262490272522\n",
      "Validation epoch 1158 ; accuracy: 0.74; loss: 1.7243001461029053\n",
      "Training epoch 1159 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 1159 ; accuracy: 0.74; loss: 1.724795937538147\n",
      "Training epoch 1160 ; accuracy: 0.9; loss: 0.19459301233291626\n",
      "Validation epoch 1160 ; accuracy: 0.74; loss: 1.7253046035766602\n",
      "Training epoch 1161 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 1161 ; accuracy: 0.74; loss: 1.7257999181747437\n",
      "Training epoch 1162 ; accuracy: 0.9; loss: 0.19459262490272522\n",
      "Validation epoch 1162 ; accuracy: 0.74; loss: 1.7262907028198242\n",
      "Training epoch 1163 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 1163 ; accuracy: 0.74; loss: 1.726758599281311\n",
      "Training epoch 1164 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 1164 ; accuracy: 0.74; loss: 1.7272344827651978\n",
      "Training epoch 1165 ; accuracy: 0.9; loss: 0.1945926696062088\n",
      "Validation epoch 1165 ; accuracy: 0.74; loss: 1.7276971340179443\n",
      "Training epoch 1166 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1166 ; accuracy: 0.74; loss: 1.7281467914581299\n",
      "Training epoch 1167 ; accuracy: 0.9; loss: 0.1945926547050476\n",
      "Validation epoch 1167 ; accuracy: 0.74; loss: 1.728581190109253\n",
      "Training epoch 1168 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 1168 ; accuracy: 0.74; loss: 1.7290070056915283\n",
      "Training epoch 1169 ; accuracy: 0.9; loss: 0.19459262490272522\n",
      "Validation epoch 1169 ; accuracy: 0.7366666666666667; loss: 1.7294481992721558\n",
      "Training epoch 1170 ; accuracy: 0.9; loss: 0.19459271430969238\n",
      "Validation epoch 1170 ; accuracy: 0.7366666666666667; loss: 1.7298976182937622\n",
      "Training epoch 1171 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 1171 ; accuracy: 0.7366666666666667; loss: 1.7303463220596313\n",
      "Training epoch 1172 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1172 ; accuracy: 0.7366666666666667; loss: 1.7308052778244019\n",
      "Training epoch 1173 ; accuracy: 0.9; loss: 0.19459261000156403\n",
      "Validation epoch 1173 ; accuracy: 0.7366666666666667; loss: 1.731258749961853\n",
      "Training epoch 1174 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 1174 ; accuracy: 0.7366666666666667; loss: 1.7317171096801758\n",
      "Training epoch 1175 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1175 ; accuracy: 0.7366666666666667; loss: 1.7321630716323853\n",
      "Training epoch 1176 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 1176 ; accuracy: 0.7366666666666667; loss: 1.7326210737228394\n",
      "Training epoch 1177 ; accuracy: 0.9; loss: 0.19459249079227448\n",
      "Validation epoch 1177 ; accuracy: 0.7366666666666667; loss: 1.7330774068832397\n",
      "Training epoch 1178 ; accuracy: 0.9; loss: 0.19459237158298492\n",
      "Validation epoch 1178 ; accuracy: 0.7366666666666667; loss: 1.733534574508667\n",
      "Training epoch 1179 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 1179 ; accuracy: 0.7366666666666667; loss: 1.7339911460876465\n",
      "Training epoch 1180 ; accuracy: 0.9; loss: 0.19459277391433716\n",
      "Validation epoch 1180 ; accuracy: 0.7366666666666667; loss: 1.7344481945037842\n",
      "Training epoch 1181 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 1181 ; accuracy: 0.7366666666666667; loss: 1.7348828315734863\n",
      "Training epoch 1182 ; accuracy: 0.9; loss: 0.19459258019924164\n",
      "Validation epoch 1182 ; accuracy: 0.7366666666666667; loss: 1.7353265285491943\n",
      "Training epoch 1183 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 1183 ; accuracy: 0.7366666666666667; loss: 1.735772728919983\n",
      "Training epoch 1184 ; accuracy: 0.9; loss: 0.19459247589111328\n",
      "Validation epoch 1184 ; accuracy: 0.7366666666666667; loss: 1.736213207244873\n",
      "Training epoch 1185 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1185 ; accuracy: 0.7366666666666667; loss: 1.7366496324539185\n",
      "Training epoch 1186 ; accuracy: 0.9; loss: 0.1945926696062088\n",
      "Validation epoch 1186 ; accuracy: 0.7366666666666667; loss: 1.737090826034546\n",
      "Training epoch 1187 ; accuracy: 0.9; loss: 0.19459259510040283\n",
      "Validation epoch 1187 ; accuracy: 0.7366666666666667; loss: 1.7375437021255493\n",
      "Training epoch 1188 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1188 ; accuracy: 0.7366666666666667; loss: 1.737981915473938\n",
      "Training epoch 1189 ; accuracy: 0.9; loss: 0.1945926994085312\n",
      "Validation epoch 1189 ; accuracy: 0.7366666666666667; loss: 1.7384082078933716\n",
      "Training epoch 1190 ; accuracy: 0.9; loss: 0.19459228217601776\n",
      "Validation epoch 1190 ; accuracy: 0.7366666666666667; loss: 1.7388254404067993\n",
      "Training epoch 1191 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1191 ; accuracy: 0.7366666666666667; loss: 1.7392399311065674\n",
      "Training epoch 1192 ; accuracy: 0.9; loss: 0.19459250569343567\n",
      "Validation epoch 1192 ; accuracy: 0.7366666666666667; loss: 1.7396631240844727\n",
      "Training epoch 1193 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 1193 ; accuracy: 0.7366666666666667; loss: 1.7400952577590942\n",
      "Training epoch 1194 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 1194 ; accuracy: 0.7366666666666667; loss: 1.7405222654342651\n",
      "Training epoch 1195 ; accuracy: 0.9; loss: 0.1945921778678894\n",
      "Validation epoch 1195 ; accuracy: 0.7366666666666667; loss: 1.7409454584121704\n",
      "Training epoch 1196 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 1196 ; accuracy: 0.7366666666666667; loss: 1.7413749694824219\n",
      "Training epoch 1197 ; accuracy: 0.9; loss: 0.19459253549575806\n",
      "Validation epoch 1197 ; accuracy: 0.7366666666666667; loss: 1.7418181896209717\n",
      "Training epoch 1198 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1198 ; accuracy: 0.7366666666666667; loss: 1.7422552108764648\n",
      "Training epoch 1199 ; accuracy: 0.9; loss: 0.1945924013853073\n",
      "Validation epoch 1199 ; accuracy: 0.7366666666666667; loss: 1.7426918745040894\n",
      "Training epoch 1200 ; accuracy: 0.9; loss: 0.19459278881549835\n",
      "Validation epoch 1200 ; accuracy: 0.7366666666666667; loss: 1.7431567907333374\n",
      "Training epoch 1201 ; accuracy: 0.9; loss: 0.1945922076702118\n",
      "Validation epoch 1201 ; accuracy: 0.7366666666666667; loss: 1.7436128854751587\n",
      "Training epoch 1202 ; accuracy: 0.9; loss: 0.1945924311876297\n",
      "Validation epoch 1202 ; accuracy: 0.7366666666666667; loss: 1.7440608739852905\n",
      "Training epoch 1203 ; accuracy: 0.9; loss: 0.19459237158298492\n",
      "Validation epoch 1203 ; accuracy: 0.7366666666666667; loss: 1.7444978952407837\n",
      "Training epoch 1204 ; accuracy: 0.9; loss: 0.1945921778678894\n",
      "Validation epoch 1204 ; accuracy: 0.7366666666666667; loss: 1.7449334859848022\n",
      "Training epoch 1205 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1205 ; accuracy: 0.7366666666666667; loss: 1.745368242263794\n",
      "Training epoch 1206 ; accuracy: 0.9; loss: 0.19459210336208344\n",
      "Validation epoch 1206 ; accuracy: 0.7366666666666667; loss: 1.7457906007766724\n",
      "Training epoch 1207 ; accuracy: 0.9; loss: 0.19459232687950134\n",
      "Validation epoch 1207 ; accuracy: 0.7366666666666667; loss: 1.7461905479431152\n",
      "Training epoch 1208 ; accuracy: 0.9; loss: 0.19459256529808044\n",
      "Validation epoch 1208 ; accuracy: 0.7366666666666667; loss: 1.7465978860855103\n",
      "Training epoch 1209 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1209 ; accuracy: 0.7366666666666667; loss: 1.7470009326934814\n",
      "Training epoch 1210 ; accuracy: 0.9; loss: 0.1945921778678894\n",
      "Validation epoch 1210 ; accuracy: 0.7366666666666667; loss: 1.7473958730697632\n",
      "Training epoch 1211 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1211 ; accuracy: 0.7366666666666667; loss: 1.7477926015853882\n",
      "Training epoch 1212 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1212 ; accuracy: 0.7366666666666667; loss: 1.7481884956359863\n",
      "Training epoch 1213 ; accuracy: 0.9; loss: 0.1945922076702118\n",
      "Validation epoch 1213 ; accuracy: 0.7366666666666667; loss: 1.7485847473144531\n",
      "Training epoch 1214 ; accuracy: 0.9; loss: 0.1945921629667282\n",
      "Validation epoch 1214 ; accuracy: 0.7366666666666667; loss: 1.7489784955978394\n",
      "Training epoch 1215 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 1215 ; accuracy: 0.7366666666666667; loss: 1.7493677139282227\n",
      "Training epoch 1216 ; accuracy: 0.9; loss: 0.19459232687950134\n",
      "Validation epoch 1216 ; accuracy: 0.7366666666666667; loss: 1.749746322631836\n",
      "Training epoch 1217 ; accuracy: 0.9; loss: 0.19459231197834015\n",
      "Validation epoch 1217 ; accuracy: 0.7366666666666667; loss: 1.7501283884048462\n",
      "Training epoch 1218 ; accuracy: 0.9; loss: 0.1945921629667282\n",
      "Validation epoch 1218 ; accuracy: 0.7366666666666667; loss: 1.7505043745040894\n",
      "Training epoch 1219 ; accuracy: 0.9; loss: 0.19459283351898193\n",
      "Validation epoch 1219 ; accuracy: 0.7366666666666667; loss: 1.7509117126464844\n",
      "Training epoch 1220 ; accuracy: 0.9; loss: 0.19459274411201477\n",
      "Validation epoch 1220 ; accuracy: 0.7366666666666667; loss: 1.7513055801391602\n",
      "Training epoch 1221 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1221 ; accuracy: 0.7366666666666667; loss: 1.751705527305603\n",
      "Training epoch 1222 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1222 ; accuracy: 0.7366666666666667; loss: 1.7521051168441772\n",
      "Training epoch 1223 ; accuracy: 0.9; loss: 0.1945921629667282\n",
      "Validation epoch 1223 ; accuracy: 0.7366666666666667; loss: 1.7524935007095337\n",
      "Training epoch 1224 ; accuracy: 0.9; loss: 0.1945921778678894\n",
      "Validation epoch 1224 ; accuracy: 0.7366666666666667; loss: 1.7528778314590454\n",
      "Training epoch 1225 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1225 ; accuracy: 0.7366666666666667; loss: 1.7532564401626587\n",
      "Training epoch 1226 ; accuracy: 0.9; loss: 0.19459238648414612\n",
      "Validation epoch 1226 ; accuracy: 0.7366666666666667; loss: 1.7536448240280151\n",
      "Training epoch 1227 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 1227 ; accuracy: 0.7366666666666667; loss: 1.7540429830551147\n",
      "Training epoch 1228 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1228 ; accuracy: 0.7366666666666667; loss: 1.7544440031051636\n",
      "Training epoch 1229 ; accuracy: 0.9; loss: 0.19459252059459686\n",
      "Validation epoch 1229 ; accuracy: 0.7366666666666667; loss: 1.7548421621322632\n",
      "Training epoch 1230 ; accuracy: 0.9; loss: 0.1945922076702118\n",
      "Validation epoch 1230 ; accuracy: 0.7366666666666667; loss: 1.7552369832992554\n",
      "Training epoch 1231 ; accuracy: 0.9; loss: 0.19459234178066254\n",
      "Validation epoch 1231 ; accuracy: 0.7366666666666667; loss: 1.7556241750717163\n",
      "Training epoch 1232 ; accuracy: 0.9; loss: 0.1945924460887909\n",
      "Validation epoch 1232 ; accuracy: 0.7366666666666667; loss: 1.7560237646102905\n",
      "Training epoch 1233 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1233 ; accuracy: 0.7366666666666667; loss: 1.756418228149414\n",
      "Training epoch 1234 ; accuracy: 0.9; loss: 0.1945924609899521\n",
      "Validation epoch 1234 ; accuracy: 0.7366666666666667; loss: 1.7568304538726807\n",
      "Training epoch 1235 ; accuracy: 0.9; loss: 0.19459205865859985\n",
      "Validation epoch 1235 ; accuracy: 0.7366666666666667; loss: 1.7572304010391235\n",
      "Training epoch 1236 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1236 ; accuracy: 0.7366666666666667; loss: 1.7576297521591187\n",
      "Training epoch 1237 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1237 ; accuracy: 0.7366666666666667; loss: 1.758036494255066\n",
      "Training epoch 1238 ; accuracy: 0.9; loss: 0.19459205865859985\n",
      "Validation epoch 1238 ; accuracy: 0.7366666666666667; loss: 1.7584266662597656\n",
      "Training epoch 1239 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1239 ; accuracy: 0.7366666666666667; loss: 1.7588218450546265\n",
      "Training epoch 1240 ; accuracy: 0.9; loss: 0.19459210336208344\n",
      "Validation epoch 1240 ; accuracy: 0.7366666666666667; loss: 1.759216070175171\n",
      "Training epoch 1241 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1241 ; accuracy: 0.7366666666666667; loss: 1.7596023082733154\n",
      "Training epoch 1242 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1242 ; accuracy: 0.7366666666666667; loss: 1.759962558746338\n",
      "Training epoch 1243 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1243 ; accuracy: 0.7366666666666667; loss: 1.7603238821029663\n",
      "Training epoch 1244 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 1244 ; accuracy: 0.7366666666666667; loss: 1.7606886625289917\n",
      "Training epoch 1245 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1245 ; accuracy: 0.7366666666666667; loss: 1.7610499858856201\n",
      "Training epoch 1246 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1246 ; accuracy: 0.7366666666666667; loss: 1.7614028453826904\n",
      "Training epoch 1247 ; accuracy: 0.9; loss: 0.19459214806556702\n",
      "Validation epoch 1247 ; accuracy: 0.7366666666666667; loss: 1.761745572090149\n",
      "Training epoch 1248 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1248 ; accuracy: 0.7366666666666667; loss: 1.7620930671691895\n",
      "Training epoch 1249 ; accuracy: 0.9; loss: 0.19459211826324463\n",
      "Validation epoch 1249 ; accuracy: 0.7366666666666667; loss: 1.7624422311782837\n",
      "Training epoch 1250 ; accuracy: 0.9; loss: 0.1945921778678894\n",
      "Validation epoch 1250 ; accuracy: 0.7366666666666667; loss: 1.762787938117981\n",
      "Training epoch 1251 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1251 ; accuracy: 0.7366666666666667; loss: 1.763132095336914\n",
      "Training epoch 1252 ; accuracy: 0.9; loss: 0.19459225237369537\n",
      "Validation epoch 1252 ; accuracy: 0.7366666666666667; loss: 1.7634814977645874\n",
      "Training epoch 1253 ; accuracy: 0.9; loss: 0.19459205865859985\n",
      "Validation epoch 1253 ; accuracy: 0.7366666666666667; loss: 1.7638261318206787\n",
      "Training epoch 1254 ; accuracy: 0.9; loss: 0.1945922076702118\n",
      "Validation epoch 1254 ; accuracy: 0.7366666666666667; loss: 1.7641687393188477\n",
      "Training epoch 1255 ; accuracy: 0.9; loss: 0.19459228217601776\n",
      "Validation epoch 1255 ; accuracy: 0.7366666666666667; loss: 1.764522671699524\n",
      "Training epoch 1256 ; accuracy: 0.9; loss: 0.19459186494350433\n",
      "Validation epoch 1256 ; accuracy: 0.7366666666666667; loss: 1.7648714780807495\n",
      "Training epoch 1257 ; accuracy: 0.9; loss: 0.19459202885627747\n",
      "Validation epoch 1257 ; accuracy: 0.7366666666666667; loss: 1.7652099132537842\n",
      "Training epoch 1258 ; accuracy: 0.9; loss: 0.19459211826324463\n",
      "Validation epoch 1258 ; accuracy: 0.7366666666666667; loss: 1.7655469179153442\n",
      "Training epoch 1259 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1259 ; accuracy: 0.7366666666666667; loss: 1.7658764123916626\n",
      "Training epoch 1260 ; accuracy: 0.9; loss: 0.1945921927690506\n",
      "Validation epoch 1260 ; accuracy: 0.7366666666666667; loss: 1.7661982774734497\n",
      "Training epoch 1261 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1261 ; accuracy: 0.7366666666666667; loss: 1.76653254032135\n",
      "Training epoch 1262 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1262 ; accuracy: 0.7366666666666667; loss: 1.766872525215149\n",
      "Training epoch 1263 ; accuracy: 0.9; loss: 0.19459204375743866\n",
      "Validation epoch 1263 ; accuracy: 0.7366666666666667; loss: 1.7672007083892822\n",
      "Training epoch 1264 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1264 ; accuracy: 0.7366666666666667; loss: 1.7675315141677856\n",
      "Training epoch 1265 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1265 ; accuracy: 0.7366666666666667; loss: 1.767850399017334\n",
      "Training epoch 1266 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 1266 ; accuracy: 0.7366666666666667; loss: 1.768190860748291\n",
      "Training epoch 1267 ; accuracy: 0.9; loss: 0.19459186494350433\n",
      "Validation epoch 1267 ; accuracy: 0.7366666666666667; loss: 1.7685294151306152\n",
      "Training epoch 1268 ; accuracy: 0.9; loss: 0.19459204375743866\n",
      "Validation epoch 1268 ; accuracy: 0.7366666666666667; loss: 1.7688724994659424\n",
      "Training epoch 1269 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1269 ; accuracy: 0.7366666666666667; loss: 1.7692160606384277\n",
      "Training epoch 1270 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 1270 ; accuracy: 0.7366666666666667; loss: 1.769534707069397\n",
      "Training epoch 1271 ; accuracy: 0.9; loss: 0.19459204375743866\n",
      "Validation epoch 1271 ; accuracy: 0.7366666666666667; loss: 1.769843339920044\n",
      "Training epoch 1272 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 1272 ; accuracy: 0.7366666666666667; loss: 1.7701557874679565\n",
      "Training epoch 1273 ; accuracy: 0.9; loss: 0.1945921927690506\n",
      "Validation epoch 1273 ; accuracy: 0.7366666666666667; loss: 1.770474910736084\n",
      "Training epoch 1274 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1274 ; accuracy: 0.7366666666666667; loss: 1.7707946300506592\n",
      "Training epoch 1275 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 1275 ; accuracy: 0.7366666666666667; loss: 1.771111249923706\n",
      "Training epoch 1276 ; accuracy: 0.9; loss: 0.19459222257137299\n",
      "Validation epoch 1276 ; accuracy: 0.7366666666666667; loss: 1.7714215517044067\n",
      "Training epoch 1277 ; accuracy: 0.9; loss: 0.1945922076702118\n",
      "Validation epoch 1277 ; accuracy: 0.7366666666666667; loss: 1.7717489004135132\n",
      "Training epoch 1278 ; accuracy: 0.9; loss: 0.19459225237369537\n",
      "Validation epoch 1278 ; accuracy: 0.7366666666666667; loss: 1.7720927000045776\n",
      "Training epoch 1279 ; accuracy: 0.9; loss: 0.19459211826324463\n",
      "Validation epoch 1279 ; accuracy: 0.7366666666666667; loss: 1.7724419832229614\n",
      "Training epoch 1280 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 1280 ; accuracy: 0.7366666666666667; loss: 1.7727879285812378\n",
      "Training epoch 1281 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1281 ; accuracy: 0.7366666666666667; loss: 1.773138165473938\n",
      "Training epoch 1282 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1282 ; accuracy: 0.7366666666666667; loss: 1.773478388786316\n",
      "Training epoch 1283 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1283 ; accuracy: 0.7366666666666667; loss: 1.773817539215088\n",
      "Training epoch 1284 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1284 ; accuracy: 0.7366666666666667; loss: 1.7741440534591675\n",
      "Training epoch 1285 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 1285 ; accuracy: 0.7366666666666667; loss: 1.774462103843689\n",
      "Training epoch 1286 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 1286 ; accuracy: 0.7366666666666667; loss: 1.7747912406921387\n",
      "Training epoch 1287 ; accuracy: 0.9; loss: 0.1945919692516327\n",
      "Validation epoch 1287 ; accuracy: 0.7366666666666667; loss: 1.775116205215454\n",
      "Training epoch 1288 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1288 ; accuracy: 0.7366666666666667; loss: 1.7754374742507935\n",
      "Training epoch 1289 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1289 ; accuracy: 0.7366666666666667; loss: 1.775750756263733\n",
      "Training epoch 1290 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1290 ; accuracy: 0.7366666666666667; loss: 1.776068925857544\n",
      "Training epoch 1291 ; accuracy: 0.9; loss: 0.19459202885627747\n",
      "Validation epoch 1291 ; accuracy: 0.7366666666666667; loss: 1.7763786315917969\n",
      "Training epoch 1292 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1292 ; accuracy: 0.7366666666666667; loss: 1.7766882181167603\n",
      "Training epoch 1293 ; accuracy: 0.9; loss: 0.19459207355976105\n",
      "Validation epoch 1293 ; accuracy: 0.7366666666666667; loss: 1.7770015001296997\n",
      "Training epoch 1294 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1294 ; accuracy: 0.7366666666666667; loss: 1.777309775352478\n",
      "Training epoch 1295 ; accuracy: 0.9; loss: 0.19459207355976105\n",
      "Validation epoch 1295 ; accuracy: 0.7366666666666667; loss: 1.7776281833648682\n",
      "Training epoch 1296 ; accuracy: 0.9; loss: 0.19459211826324463\n",
      "Validation epoch 1296 ; accuracy: 0.7366666666666667; loss: 1.7779500484466553\n",
      "Training epoch 1297 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1297 ; accuracy: 0.7366666666666667; loss: 1.7782702445983887\n",
      "Training epoch 1298 ; accuracy: 0.9; loss: 0.19459202885627747\n",
      "Validation epoch 1298 ; accuracy: 0.7366666666666667; loss: 1.7785855531692505\n",
      "Training epoch 1299 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1299 ; accuracy: 0.7366666666666667; loss: 1.7788923978805542\n",
      "Training epoch 1300 ; accuracy: 0.9; loss: 0.19459199905395508\n",
      "Validation epoch 1300 ; accuracy: 0.7366666666666667; loss: 1.779199242591858\n",
      "Training epoch 1301 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1301 ; accuracy: 0.7366666666666667; loss: 1.779499888420105\n",
      "Training epoch 1302 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1302 ; accuracy: 0.7366666666666667; loss: 1.779801607131958\n",
      "Training epoch 1303 ; accuracy: 0.9; loss: 0.19459205865859985\n",
      "Validation epoch 1303 ; accuracy: 0.7366666666666667; loss: 1.7800880670547485\n",
      "Training epoch 1304 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1304 ; accuracy: 0.7366666666666667; loss: 1.7803704738616943\n",
      "Training epoch 1305 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1305 ; accuracy: 0.7366666666666667; loss: 1.7806540727615356\n",
      "Training epoch 1306 ; accuracy: 0.9; loss: 0.1945919692516327\n",
      "Validation epoch 1306 ; accuracy: 0.7366666666666667; loss: 1.780928373336792\n",
      "Training epoch 1307 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1307 ; accuracy: 0.7366666666666667; loss: 1.7811976671218872\n",
      "Training epoch 1308 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1308 ; accuracy: 0.7366666666666667; loss: 1.781466007232666\n",
      "Training epoch 1309 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1309 ; accuracy: 0.7366666666666667; loss: 1.781744360923767\n",
      "Training epoch 1310 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 1310 ; accuracy: 0.7366666666666667; loss: 1.782020092010498\n",
      "Training epoch 1311 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1311 ; accuracy: 0.7366666666666667; loss: 1.7822835445404053\n",
      "Training epoch 1312 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1312 ; accuracy: 0.7366666666666667; loss: 1.782537817955017\n",
      "Training epoch 1313 ; accuracy: 0.9; loss: 0.19459186494350433\n",
      "Validation epoch 1313 ; accuracy: 0.7366666666666667; loss: 1.7827903032302856\n",
      "Training epoch 1314 ; accuracy: 0.9; loss: 0.19459183514118195\n",
      "Validation epoch 1314 ; accuracy: 0.7366666666666667; loss: 1.7830384969711304\n",
      "Training epoch 1315 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1315 ; accuracy: 0.7366666666666667; loss: 1.7832800149917603\n",
      "Training epoch 1316 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1316 ; accuracy: 0.7366666666666667; loss: 1.7835239171981812\n",
      "Training epoch 1317 ; accuracy: 0.9; loss: 0.19459198415279388\n",
      "Validation epoch 1317 ; accuracy: 0.7366666666666667; loss: 1.7837756872177124\n",
      "Training epoch 1318 ; accuracy: 0.9; loss: 0.19459213316440582\n",
      "Validation epoch 1318 ; accuracy: 0.7366666666666667; loss: 1.7840372323989868\n",
      "Training epoch 1319 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1319 ; accuracy: 0.7366666666666667; loss: 1.7843058109283447\n",
      "Training epoch 1320 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1320 ; accuracy: 0.7366666666666667; loss: 1.7845747470855713\n",
      "Training epoch 1321 ; accuracy: 0.9; loss: 0.19459199905395508\n",
      "Validation epoch 1321 ; accuracy: 0.7366666666666667; loss: 1.7848433256149292\n",
      "Training epoch 1322 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1322 ; accuracy: 0.7366666666666667; loss: 1.7851108312606812\n",
      "Training epoch 1323 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 1323 ; accuracy: 0.7366666666666667; loss: 1.7853761911392212\n",
      "Training epoch 1324 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 1324 ; accuracy: 0.7366666666666667; loss: 1.7856351137161255\n",
      "Training epoch 1325 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1325 ; accuracy: 0.7366666666666667; loss: 1.78589928150177\n",
      "Training epoch 1326 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1326 ; accuracy: 0.7366666666666667; loss: 1.786167860031128\n",
      "Training epoch 1327 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1327 ; accuracy: 0.7366666666666667; loss: 1.7864428758621216\n",
      "Training epoch 1328 ; accuracy: 0.9; loss: 0.19459198415279388\n",
      "Validation epoch 1328 ; accuracy: 0.7366666666666667; loss: 1.7867106199264526\n",
      "Training epoch 1329 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1329 ; accuracy: 0.7366666666666667; loss: 1.786960482597351\n",
      "Training epoch 1330 ; accuracy: 0.9; loss: 0.19459198415279388\n",
      "Validation epoch 1330 ; accuracy: 0.7366666666666667; loss: 1.7872170209884644\n",
      "Training epoch 1331 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1331 ; accuracy: 0.7366666666666667; loss: 1.787465214729309\n",
      "Training epoch 1332 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1332 ; accuracy: 0.7366666666666667; loss: 1.787713885307312\n",
      "Training epoch 1333 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1333 ; accuracy: 0.7366666666666667; loss: 1.7879788875579834\n",
      "Training epoch 1334 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1334 ; accuracy: 0.7366666666666667; loss: 1.788234829902649\n",
      "Training epoch 1335 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1335 ; accuracy: 0.7366666666666667; loss: 1.7885111570358276\n",
      "Training epoch 1336 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 1336 ; accuracy: 0.7366666666666667; loss: 1.788777470588684\n",
      "Training epoch 1337 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1337 ; accuracy: 0.7366666666666667; loss: 1.7890383005142212\n",
      "Training epoch 1338 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1338 ; accuracy: 0.7366666666666667; loss: 1.7892940044403076\n",
      "Training epoch 1339 ; accuracy: 0.9; loss: 0.19459202885627747\n",
      "Validation epoch 1339 ; accuracy: 0.7366666666666667; loss: 1.789557695388794\n",
      "Training epoch 1340 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1340 ; accuracy: 0.7366666666666667; loss: 1.7898203134536743\n",
      "Training epoch 1341 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1341 ; accuracy: 0.7366666666666667; loss: 1.7900811433792114\n",
      "Training epoch 1342 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1342 ; accuracy: 0.7366666666666667; loss: 1.7903356552124023\n",
      "Training epoch 1343 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1343 ; accuracy: 0.7366666666666667; loss: 1.7905932664871216\n",
      "Training epoch 1344 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 1344 ; accuracy: 0.7366666666666667; loss: 1.7908443212509155\n",
      "Training epoch 1345 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1345 ; accuracy: 0.7366666666666667; loss: 1.791090726852417\n",
      "Training epoch 1346 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1346 ; accuracy: 0.7366666666666667; loss: 1.7913295030593872\n",
      "Training epoch 1347 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1347 ; accuracy: 0.7366666666666667; loss: 1.7915832996368408\n",
      "Training epoch 1348 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1348 ; accuracy: 0.7366666666666667; loss: 1.7918490171432495\n",
      "Training epoch 1349 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1349 ; accuracy: 0.7366666666666667; loss: 1.7921090126037598\n",
      "Training epoch 1350 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1350 ; accuracy: 0.7366666666666667; loss: 1.7923734188079834\n",
      "Training epoch 1351 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1351 ; accuracy: 0.7366666666666667; loss: 1.7926301956176758\n",
      "Training epoch 1352 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 1352 ; accuracy: 0.7366666666666667; loss: 1.7928757667541504\n",
      "Training epoch 1353 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 1353 ; accuracy: 0.7366666666666667; loss: 1.793122410774231\n",
      "Training epoch 1354 ; accuracy: 0.9; loss: 0.19459198415279388\n",
      "Validation epoch 1354 ; accuracy: 0.7366666666666667; loss: 1.7933727502822876\n",
      "Training epoch 1355 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 1355 ; accuracy: 0.7366666666666667; loss: 1.7936173677444458\n",
      "Training epoch 1356 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1356 ; accuracy: 0.7366666666666667; loss: 1.7938661575317383\n",
      "Training epoch 1357 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1357 ; accuracy: 0.7366666666666667; loss: 1.7941166162490845\n",
      "Training epoch 1358 ; accuracy: 0.9; loss: 0.19459183514118195\n",
      "Validation epoch 1358 ; accuracy: 0.7366666666666667; loss: 1.7943625450134277\n",
      "Training epoch 1359 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1359 ; accuracy: 0.7366666666666667; loss: 1.7946085929870605\n",
      "Training epoch 1360 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1360 ; accuracy: 0.7366666666666667; loss: 1.794854760169983\n",
      "Training epoch 1361 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 1361 ; accuracy: 0.7366666666666667; loss: 1.795131802558899\n",
      "Training epoch 1362 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1362 ; accuracy: 0.7366666666666667; loss: 1.7954081296920776\n",
      "Training epoch 1363 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1363 ; accuracy: 0.7366666666666667; loss: 1.7956825494766235\n",
      "Training epoch 1364 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1364 ; accuracy: 0.7366666666666667; loss: 1.7959686517715454\n",
      "Training epoch 1365 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 1365 ; accuracy: 0.7366666666666667; loss: 1.7962510585784912\n",
      "Training epoch 1366 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1366 ; accuracy: 0.7366666666666667; loss: 1.7965167760849\n",
      "Training epoch 1367 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1367 ; accuracy: 0.7366666666666667; loss: 1.7967811822891235\n",
      "Training epoch 1368 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1368 ; accuracy: 0.7366666666666667; loss: 1.7970404624938965\n",
      "Training epoch 1369 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 1369 ; accuracy: 0.7366666666666667; loss: 1.797296166419983\n",
      "Training epoch 1370 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1370 ; accuracy: 0.7366666666666667; loss: 1.7975435256958008\n",
      "Training epoch 1371 ; accuracy: 0.9; loss: 0.19459183514118195\n",
      "Validation epoch 1371 ; accuracy: 0.7366666666666667; loss: 1.7977864742279053\n",
      "Training epoch 1372 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1372 ; accuracy: 0.7366666666666667; loss: 1.798025131225586\n",
      "Training epoch 1373 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1373 ; accuracy: 0.7366666666666667; loss: 1.798261284828186\n",
      "Training epoch 1374 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1374 ; accuracy: 0.7366666666666667; loss: 1.7984976768493652\n",
      "Training epoch 1375 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1375 ; accuracy: 0.7366666666666667; loss: 1.7987349033355713\n",
      "Training epoch 1376 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1376 ; accuracy: 0.7366666666666667; loss: 1.798960566520691\n",
      "Training epoch 1377 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1377 ; accuracy: 0.7366666666666667; loss: 1.799181342124939\n",
      "Training epoch 1378 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1378 ; accuracy: 0.7366666666666667; loss: 1.799398422241211\n",
      "Training epoch 1379 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1379 ; accuracy: 0.7366666666666667; loss: 1.799614667892456\n",
      "Training epoch 1380 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1380 ; accuracy: 0.7366666666666667; loss: 1.7998300790786743\n",
      "Training epoch 1381 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1381 ; accuracy: 0.7366666666666667; loss: 1.800048828125\n",
      "Training epoch 1382 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 1382 ; accuracy: 0.7366666666666667; loss: 1.8002593517303467\n",
      "Training epoch 1383 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1383 ; accuracy: 0.7366666666666667; loss: 1.8004685640335083\n",
      "Training epoch 1384 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1384 ; accuracy: 0.7366666666666667; loss: 1.8006784915924072\n",
      "Training epoch 1385 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1385 ; accuracy: 0.7366666666666667; loss: 1.8008906841278076\n",
      "Training epoch 1386 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1386 ; accuracy: 0.7366666666666667; loss: 1.8011088371276855\n",
      "Training epoch 1387 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1387 ; accuracy: 0.7366666666666667; loss: 1.8013242483139038\n",
      "Training epoch 1388 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1388 ; accuracy: 0.7366666666666667; loss: 1.8015356063842773\n",
      "Training epoch 1389 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1389 ; accuracy: 0.7366666666666667; loss: 1.801743984222412\n",
      "Training epoch 1390 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1390 ; accuracy: 0.7366666666666667; loss: 1.8019565343856812\n",
      "Training epoch 1391 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1391 ; accuracy: 0.7366666666666667; loss: 1.8021610975265503\n",
      "Training epoch 1392 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1392 ; accuracy: 0.7366666666666667; loss: 1.8023730516433716\n",
      "Training epoch 1393 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1393 ; accuracy: 0.7366666666666667; loss: 1.8025795221328735\n",
      "Training epoch 1394 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1394 ; accuracy: 0.7366666666666667; loss: 1.8027839660644531\n",
      "Training epoch 1395 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1395 ; accuracy: 0.7366666666666667; loss: 1.8029825687408447\n",
      "Training epoch 1396 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1396 ; accuracy: 0.7366666666666667; loss: 1.8032037019729614\n",
      "Training epoch 1397 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1397 ; accuracy: 0.7366666666666667; loss: 1.803419828414917\n",
      "Training epoch 1398 ; accuracy: 0.9; loss: 0.19459204375743866\n",
      "Validation epoch 1398 ; accuracy: 0.7366666666666667; loss: 1.803632378578186\n",
      "Training epoch 1399 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1399 ; accuracy: 0.7366666666666667; loss: 1.803839921951294\n",
      "Training epoch 1400 ; accuracy: 0.9; loss: 0.19459198415279388\n",
      "Validation epoch 1400 ; accuracy: 0.7366666666666667; loss: 1.8040454387664795\n",
      "Training epoch 1401 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1401 ; accuracy: 0.7366666666666667; loss: 1.8042418956756592\n",
      "Training epoch 1402 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1402 ; accuracy: 0.7366666666666667; loss: 1.8044439554214478\n",
      "Training epoch 1403 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1403 ; accuracy: 0.7366666666666667; loss: 1.8046541213989258\n",
      "Training epoch 1404 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1404 ; accuracy: 0.7366666666666667; loss: 1.8048683404922485\n",
      "Training epoch 1405 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1405 ; accuracy: 0.7366666666666667; loss: 1.805080771446228\n",
      "Training epoch 1406 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1406 ; accuracy: 0.7366666666666667; loss: 1.805290937423706\n",
      "Training epoch 1407 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1407 ; accuracy: 0.7366666666666667; loss: 1.8054980039596558\n",
      "Training epoch 1408 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1408 ; accuracy: 0.7366666666666667; loss: 1.805701494216919\n",
      "Training epoch 1409 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1409 ; accuracy: 0.7366666666666667; loss: 1.805903673171997\n",
      "Training epoch 1410 ; accuracy: 0.9; loss: 0.19459183514118195\n",
      "Validation epoch 1410 ; accuracy: 0.7366666666666667; loss: 1.8061065673828125\n",
      "Training epoch 1411 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1411 ; accuracy: 0.7366666666666667; loss: 1.8063089847564697\n",
      "Training epoch 1412 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1412 ; accuracy: 0.7366666666666667; loss: 1.8065177202224731\n",
      "Training epoch 1413 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1413 ; accuracy: 0.7366666666666667; loss: 1.8067333698272705\n",
      "Training epoch 1414 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1414 ; accuracy: 0.7366666666666667; loss: 1.8069435358047485\n",
      "Training epoch 1415 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1415 ; accuracy: 0.7366666666666667; loss: 1.807159185409546\n",
      "Training epoch 1416 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1416 ; accuracy: 0.7366666666666667; loss: 1.8073744773864746\n",
      "Training epoch 1417 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1417 ; accuracy: 0.7366666666666667; loss: 1.8075939416885376\n",
      "Training epoch 1418 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1418 ; accuracy: 0.7366666666666667; loss: 1.8078100681304932\n",
      "Training epoch 1419 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1419 ; accuracy: 0.7366666666666667; loss: 1.8080323934555054\n",
      "Training epoch 1420 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1420 ; accuracy: 0.7366666666666667; loss: 1.8082618713378906\n",
      "Training epoch 1421 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1421 ; accuracy: 0.7366666666666667; loss: 1.8084748983383179\n",
      "Training epoch 1422 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1422 ; accuracy: 0.7366666666666667; loss: 1.8086974620819092\n",
      "Training epoch 1423 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1423 ; accuracy: 0.7366666666666667; loss: 1.8089089393615723\n",
      "Training epoch 1424 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1424 ; accuracy: 0.7366666666666667; loss: 1.8091235160827637\n",
      "Training epoch 1425 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1425 ; accuracy: 0.7366666666666667; loss: 1.8093394041061401\n",
      "Training epoch 1426 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1426 ; accuracy: 0.7366666666666667; loss: 1.8095588684082031\n",
      "Training epoch 1427 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1427 ; accuracy: 0.7366666666666667; loss: 1.809791922569275\n",
      "Training epoch 1428 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1428 ; accuracy: 0.7366666666666667; loss: 1.8100146055221558\n",
      "Training epoch 1429 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1429 ; accuracy: 0.7366666666666667; loss: 1.8102335929870605\n",
      "Training epoch 1430 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1430 ; accuracy: 0.7366666666666667; loss: 1.8104472160339355\n",
      "Training epoch 1431 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1431 ; accuracy: 0.7366666666666667; loss: 1.810655951499939\n",
      "Training epoch 1432 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1432 ; accuracy: 0.7366666666666667; loss: 1.8108524084091187\n",
      "Training epoch 1433 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1433 ; accuracy: 0.7366666666666667; loss: 1.8110451698303223\n",
      "Training epoch 1434 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1434 ; accuracy: 0.7366666666666667; loss: 1.811232328414917\n",
      "Training epoch 1435 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1435 ; accuracy: 0.7366666666666667; loss: 1.811415433883667\n",
      "Training epoch 1436 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1436 ; accuracy: 0.7366666666666667; loss: 1.811601161956787\n",
      "Training epoch 1437 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1437 ; accuracy: 0.7366666666666667; loss: 1.8117949962615967\n",
      "Training epoch 1438 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1438 ; accuracy: 0.7366666666666667; loss: 1.8119913339614868\n",
      "Training epoch 1439 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1439 ; accuracy: 0.7366666666666667; loss: 1.8121970891952515\n",
      "Training epoch 1440 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1440 ; accuracy: 0.7366666666666667; loss: 1.8123983144760132\n",
      "Training epoch 1441 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 1441 ; accuracy: 0.7366666666666667; loss: 1.8126064538955688\n",
      "Training epoch 1442 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1442 ; accuracy: 0.7366666666666667; loss: 1.8128137588500977\n",
      "Training epoch 1443 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1443 ; accuracy: 0.7366666666666667; loss: 1.8130462169647217\n",
      "Training epoch 1444 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1444 ; accuracy: 0.7366666666666667; loss: 1.8132951259613037\n",
      "Training epoch 1445 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1445 ; accuracy: 0.7366666666666667; loss: 1.8135372400283813\n",
      "Training epoch 1446 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1446 ; accuracy: 0.74; loss: 1.8137710094451904\n",
      "Training epoch 1447 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 1447 ; accuracy: 0.74; loss: 1.8140079975128174\n",
      "Training epoch 1448 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1448 ; accuracy: 0.74; loss: 1.8142366409301758\n",
      "Training epoch 1449 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1449 ; accuracy: 0.74; loss: 1.814459204673767\n",
      "Training epoch 1450 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1450 ; accuracy: 0.74; loss: 1.814680576324463\n",
      "Training epoch 1451 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1451 ; accuracy: 0.74; loss: 1.81489098072052\n",
      "Training epoch 1452 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1452 ; accuracy: 0.74; loss: 1.8151005506515503\n",
      "Training epoch 1453 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1453 ; accuracy: 0.74; loss: 1.8153088092803955\n",
      "Training epoch 1454 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1454 ; accuracy: 0.74; loss: 1.8155134916305542\n",
      "Training epoch 1455 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1455 ; accuracy: 0.74; loss: 1.8157267570495605\n",
      "Training epoch 1456 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1456 ; accuracy: 0.74; loss: 1.815954566001892\n",
      "Training epoch 1457 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1457 ; accuracy: 0.74; loss: 1.8161735534667969\n",
      "Training epoch 1458 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1458 ; accuracy: 0.74; loss: 1.8163942098617554\n",
      "Training epoch 1459 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1459 ; accuracy: 0.74; loss: 1.8166080713272095\n",
      "Training epoch 1460 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1460 ; accuracy: 0.74; loss: 1.8168171644210815\n",
      "Training epoch 1461 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1461 ; accuracy: 0.74; loss: 1.8170312643051147\n",
      "Training epoch 1462 ; accuracy: 0.9; loss: 0.19459190964698792\n",
      "Validation epoch 1462 ; accuracy: 0.74; loss: 1.8172584772109985\n",
      "Training epoch 1463 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1463 ; accuracy: 0.74; loss: 1.8174879550933838\n",
      "Training epoch 1464 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1464 ; accuracy: 0.74; loss: 1.817725658416748\n",
      "Training epoch 1465 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1465 ; accuracy: 0.74; loss: 1.817955493927002\n",
      "Training epoch 1466 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1466 ; accuracy: 0.74; loss: 1.818178653717041\n",
      "Training epoch 1467 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1467 ; accuracy: 0.74; loss: 1.8183979988098145\n",
      "Training epoch 1468 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1468 ; accuracy: 0.74; loss: 1.8186124563217163\n",
      "Training epoch 1469 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1469 ; accuracy: 0.74; loss: 1.8188179731369019\n",
      "Training epoch 1470 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1470 ; accuracy: 0.74; loss: 1.8190118074417114\n",
      "Training epoch 1471 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1471 ; accuracy: 0.74; loss: 1.8192158937454224\n",
      "Training epoch 1472 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1472 ; accuracy: 0.74; loss: 1.8194220066070557\n",
      "Training epoch 1473 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1473 ; accuracy: 0.74; loss: 1.8196266889572144\n",
      "Training epoch 1474 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1474 ; accuracy: 0.74; loss: 1.819831132888794\n",
      "Training epoch 1475 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1475 ; accuracy: 0.74; loss: 1.8200337886810303\n",
      "Training epoch 1476 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1476 ; accuracy: 0.74; loss: 1.8202295303344727\n",
      "Training epoch 1477 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1477 ; accuracy: 0.74; loss: 1.8204357624053955\n",
      "Training epoch 1478 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1478 ; accuracy: 0.74; loss: 1.820641040802002\n",
      "Training epoch 1479 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1479 ; accuracy: 0.7433333333333333; loss: 1.8208496570587158\n",
      "Training epoch 1480 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 1480 ; accuracy: 0.7433333333333333; loss: 1.8210632801055908\n",
      "Training epoch 1481 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1481 ; accuracy: 0.7433333333333333; loss: 1.8212686777114868\n",
      "Training epoch 1482 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1482 ; accuracy: 0.7433333333333333; loss: 1.8214632272720337\n",
      "Training epoch 1483 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1483 ; accuracy: 0.7433333333333333; loss: 1.821656346321106\n",
      "Training epoch 1484 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1484 ; accuracy: 0.7433333333333333; loss: 1.821842074394226\n",
      "Training epoch 1485 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1485 ; accuracy: 0.7433333333333333; loss: 1.8220326900482178\n",
      "Training epoch 1486 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1486 ; accuracy: 0.7433333333333333; loss: 1.8222233057022095\n",
      "Training epoch 1487 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1487 ; accuracy: 0.7433333333333333; loss: 1.8224139213562012\n",
      "Training epoch 1488 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1488 ; accuracy: 0.7433333333333333; loss: 1.8226078748703003\n",
      "Training epoch 1489 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 1489 ; accuracy: 0.7433333333333333; loss: 1.82281494140625\n",
      "Training epoch 1490 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1490 ; accuracy: 0.7433333333333333; loss: 1.8230150938034058\n",
      "Training epoch 1491 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1491 ; accuracy: 0.7433333333333333; loss: 1.8232049942016602\n",
      "Training epoch 1492 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1492 ; accuracy: 0.7433333333333333; loss: 1.8233896493911743\n",
      "Training epoch 1493 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1493 ; accuracy: 0.7433333333333333; loss: 1.8235691785812378\n",
      "Training epoch 1494 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1494 ; accuracy: 0.7433333333333333; loss: 1.8237593173980713\n",
      "Training epoch 1495 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1495 ; accuracy: 0.7433333333333333; loss: 1.8239548206329346\n",
      "Training epoch 1496 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1496 ; accuracy: 0.7433333333333333; loss: 1.8241536617279053\n",
      "Training epoch 1497 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1497 ; accuracy: 0.7433333333333333; loss: 1.8243489265441895\n",
      "Training epoch 1498 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1498 ; accuracy: 0.7433333333333333; loss: 1.8245478868484497\n",
      "Training epoch 1499 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1499 ; accuracy: 0.7433333333333333; loss: 1.8247482776641846\n",
      "Training epoch 1500 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1500 ; accuracy: 0.7433333333333333; loss: 1.8249377012252808\n",
      "Training epoch 1501 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1501 ; accuracy: 0.7433333333333333; loss: 1.8251270055770874\n",
      "Training epoch 1502 ; accuracy: 0.9; loss: 0.19459182024002075\n",
      "Validation epoch 1502 ; accuracy: 0.7433333333333333; loss: 1.8252811431884766\n",
      "Training epoch 1503 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1503 ; accuracy: 0.7433333333333333; loss: 1.8254300355911255\n",
      "Training epoch 1504 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1504 ; accuracy: 0.7433333333333333; loss: 1.825590968132019\n",
      "Training epoch 1505 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1505 ; accuracy: 0.7433333333333333; loss: 1.8257898092269897\n",
      "Training epoch 1506 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1506 ; accuracy: 0.7433333333333333; loss: 1.825981616973877\n",
      "Training epoch 1507 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1507 ; accuracy: 0.7433333333333333; loss: 1.8261722326278687\n",
      "Training epoch 1508 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1508 ; accuracy: 0.7433333333333333; loss: 1.8263567686080933\n",
      "Training epoch 1509 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1509 ; accuracy: 0.7433333333333333; loss: 1.826540470123291\n",
      "Training epoch 1510 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1510 ; accuracy: 0.7433333333333333; loss: 1.8267189264297485\n",
      "Training epoch 1511 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1511 ; accuracy: 0.7433333333333333; loss: 1.826890468597412\n",
      "Training epoch 1512 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1512 ; accuracy: 0.7433333333333333; loss: 1.8270801305770874\n",
      "Training epoch 1513 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1513 ; accuracy: 0.7433333333333333; loss: 1.8272678852081299\n",
      "Training epoch 1514 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1514 ; accuracy: 0.7433333333333333; loss: 1.8274515867233276\n",
      "Training epoch 1515 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1515 ; accuracy: 0.7433333333333333; loss: 1.8276294469833374\n",
      "Training epoch 1516 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1516 ; accuracy: 0.7433333333333333; loss: 1.8278005123138428\n",
      "Training epoch 1517 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1517 ; accuracy: 0.7433333333333333; loss: 1.827976107597351\n",
      "Training epoch 1518 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1518 ; accuracy: 0.7433333333333333; loss: 1.8281365633010864\n",
      "Training epoch 1519 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1519 ; accuracy: 0.7433333333333333; loss: 1.8282946348190308\n",
      "Training epoch 1520 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1520 ; accuracy: 0.7433333333333333; loss: 1.8284494876861572\n",
      "Training epoch 1521 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1521 ; accuracy: 0.7433333333333333; loss: 1.8285980224609375\n",
      "Training epoch 1522 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1522 ; accuracy: 0.7433333333333333; loss: 1.8287488222122192\n",
      "Training epoch 1523 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1523 ; accuracy: 0.7433333333333333; loss: 1.82891047000885\n",
      "Training epoch 1524 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1524 ; accuracy: 0.7433333333333333; loss: 1.829077124595642\n",
      "Training epoch 1525 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1525 ; accuracy: 0.7433333333333333; loss: 1.829250454902649\n",
      "Training epoch 1526 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1526 ; accuracy: 0.7433333333333333; loss: 1.8294211626052856\n",
      "Training epoch 1527 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1527 ; accuracy: 0.7433333333333333; loss: 1.8295961618423462\n",
      "Training epoch 1528 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 1528 ; accuracy: 0.7433333333333333; loss: 1.8297796249389648\n",
      "Training epoch 1529 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1529 ; accuracy: 0.7433333333333333; loss: 1.8299607038497925\n",
      "Training epoch 1530 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1530 ; accuracy: 0.7433333333333333; loss: 1.8301334381103516\n",
      "Training epoch 1531 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1531 ; accuracy: 0.7433333333333333; loss: 1.830295443534851\n",
      "Training epoch 1532 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1532 ; accuracy: 0.7433333333333333; loss: 1.830451250076294\n",
      "Training epoch 1533 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1533 ; accuracy: 0.7433333333333333; loss: 1.8306111097335815\n",
      "Training epoch 1534 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1534 ; accuracy: 0.7433333333333333; loss: 1.8307714462280273\n",
      "Training epoch 1535 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1535 ; accuracy: 0.7433333333333333; loss: 1.8309366703033447\n",
      "Training epoch 1536 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1536 ; accuracy: 0.7433333333333333; loss: 1.8311001062393188\n",
      "Training epoch 1537 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1537 ; accuracy: 0.7433333333333333; loss: 1.8312697410583496\n",
      "Training epoch 1538 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 1538 ; accuracy: 0.7433333333333333; loss: 1.8314450979232788\n",
      "Training epoch 1539 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1539 ; accuracy: 0.7433333333333333; loss: 1.831623911857605\n",
      "Training epoch 1540 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1540 ; accuracy: 0.7433333333333333; loss: 1.8318103551864624\n",
      "Training epoch 1541 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1541 ; accuracy: 0.7433333333333333; loss: 1.832003116607666\n",
      "Training epoch 1542 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1542 ; accuracy: 0.7433333333333333; loss: 1.832196831703186\n",
      "Training epoch 1543 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1543 ; accuracy: 0.7433333333333333; loss: 1.8323888778686523\n",
      "Training epoch 1544 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1544 ; accuracy: 0.7433333333333333; loss: 1.8325785398483276\n",
      "Training epoch 1545 ; accuracy: 0.9; loss: 0.19459202885627747\n",
      "Validation epoch 1545 ; accuracy: 0.7433333333333333; loss: 1.832854151725769\n",
      "Training epoch 1546 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1546 ; accuracy: 0.7433333333333333; loss: 1.8331172466278076\n",
      "Training epoch 1547 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1547 ; accuracy: 0.7433333333333333; loss: 1.8333731889724731\n",
      "Training epoch 1548 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 1548 ; accuracy: 0.7433333333333333; loss: 1.8336268663406372\n",
      "Training epoch 1549 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1549 ; accuracy: 0.7433333333333333; loss: 1.8338793516159058\n",
      "Training epoch 1550 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1550 ; accuracy: 0.7433333333333333; loss: 1.8341237306594849\n",
      "Training epoch 1551 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1551 ; accuracy: 0.7433333333333333; loss: 1.8343591690063477\n",
      "Training epoch 1552 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1552 ; accuracy: 0.7433333333333333; loss: 1.8345867395401\n",
      "Training epoch 1553 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1553 ; accuracy: 0.7433333333333333; loss: 1.8348156213760376\n",
      "Training epoch 1554 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1554 ; accuracy: 0.7433333333333333; loss: 1.8350352048873901\n",
      "Training epoch 1555 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1555 ; accuracy: 0.7433333333333333; loss: 1.835252046585083\n",
      "Training epoch 1556 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1556 ; accuracy: 0.7433333333333333; loss: 1.8354679346084595\n",
      "Training epoch 1557 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 1557 ; accuracy: 0.7433333333333333; loss: 1.8357088565826416\n",
      "Training epoch 1558 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1558 ; accuracy: 0.7433333333333333; loss: 1.8359452486038208\n",
      "Training epoch 1559 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1559 ; accuracy: 0.7433333333333333; loss: 1.8361709117889404\n",
      "Training epoch 1560 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1560 ; accuracy: 0.7433333333333333; loss: 1.8363946676254272\n",
      "Training epoch 1561 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1561 ; accuracy: 0.7433333333333333; loss: 1.8366140127182007\n",
      "Training epoch 1562 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1562 ; accuracy: 0.7433333333333333; loss: 1.8368297815322876\n",
      "Training epoch 1563 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1563 ; accuracy: 0.7433333333333333; loss: 1.837039828300476\n",
      "Training epoch 1564 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1564 ; accuracy: 0.7433333333333333; loss: 1.8372479677200317\n",
      "Training epoch 1565 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1565 ; accuracy: 0.7433333333333333; loss: 1.8374524116516113\n",
      "Training epoch 1566 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1566 ; accuracy: 0.7433333333333333; loss: 1.8376588821411133\n",
      "Training epoch 1567 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1567 ; accuracy: 0.7433333333333333; loss: 1.8378690481185913\n",
      "Training epoch 1568 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1568 ; accuracy: 0.7433333333333333; loss: 1.8380810022354126\n",
      "Training epoch 1569 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1569 ; accuracy: 0.7433333333333333; loss: 1.8382844924926758\n",
      "Training epoch 1570 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1570 ; accuracy: 0.7433333333333333; loss: 1.8384912014007568\n",
      "Training epoch 1571 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1571 ; accuracy: 0.7433333333333333; loss: 1.8386956453323364\n",
      "Training epoch 1572 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1572 ; accuracy: 0.7433333333333333; loss: 1.8389028310775757\n",
      "Training epoch 1573 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1573 ; accuracy: 0.7433333333333333; loss: 1.839106798171997\n",
      "Training epoch 1574 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1574 ; accuracy: 0.7433333333333333; loss: 1.8393058776855469\n",
      "Training epoch 1575 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1575 ; accuracy: 0.7433333333333333; loss: 1.839495062828064\n",
      "Training epoch 1576 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1576 ; accuracy: 0.7433333333333333; loss: 1.839691162109375\n",
      "Training epoch 1577 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1577 ; accuracy: 0.7433333333333333; loss: 1.8398854732513428\n",
      "Training epoch 1578 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1578 ; accuracy: 0.7433333333333333; loss: 1.8400688171386719\n",
      "Training epoch 1579 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1579 ; accuracy: 0.7433333333333333; loss: 1.8402467966079712\n",
      "Training epoch 1580 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1580 ; accuracy: 0.7433333333333333; loss: 1.8404183387756348\n",
      "Training epoch 1581 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1581 ; accuracy: 0.7433333333333333; loss: 1.840582251548767\n",
      "Training epoch 1582 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1582 ; accuracy: 0.7433333333333333; loss: 1.8407373428344727\n",
      "Training epoch 1583 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1583 ; accuracy: 0.7433333333333333; loss: 1.8408982753753662\n",
      "Training epoch 1584 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1584 ; accuracy: 0.7433333333333333; loss: 1.8410457372665405\n",
      "Training epoch 1585 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1585 ; accuracy: 0.7433333333333333; loss: 1.8412107229232788\n",
      "Training epoch 1586 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1586 ; accuracy: 0.7433333333333333; loss: 1.8413687944412231\n",
      "Training epoch 1587 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1587 ; accuracy: 0.7433333333333333; loss: 1.8415228128433228\n",
      "Training epoch 1588 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1588 ; accuracy: 0.7433333333333333; loss: 1.841681957244873\n",
      "Training epoch 1589 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1589 ; accuracy: 0.7433333333333333; loss: 1.8418354988098145\n",
      "Training epoch 1590 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1590 ; accuracy: 0.7433333333333333; loss: 1.8419803380966187\n",
      "Training epoch 1591 ; accuracy: 0.9; loss: 0.19459165632724762\n",
      "Validation epoch 1591 ; accuracy: 0.7433333333333333; loss: 1.8421130180358887\n",
      "Training epoch 1592 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1592 ; accuracy: 0.7433333333333333; loss: 1.8422436714172363\n",
      "Training epoch 1593 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1593 ; accuracy: 0.7433333333333333; loss: 1.8423677682876587\n",
      "Training epoch 1594 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1594 ; accuracy: 0.7433333333333333; loss: 1.8424890041351318\n",
      "Training epoch 1595 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1595 ; accuracy: 0.7433333333333333; loss: 1.8426185846328735\n",
      "Training epoch 1596 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1596 ; accuracy: 0.7433333333333333; loss: 1.8427481651306152\n",
      "Training epoch 1597 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1597 ; accuracy: 0.7433333333333333; loss: 1.842878818511963\n",
      "Training epoch 1598 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1598 ; accuracy: 0.7433333333333333; loss: 1.8430172204971313\n",
      "Training epoch 1599 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1599 ; accuracy: 0.7433333333333333; loss: 1.8431532382965088\n",
      "Training epoch 1600 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1600 ; accuracy: 0.7433333333333333; loss: 1.843288540840149\n",
      "Training epoch 1601 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1601 ; accuracy: 0.7433333333333333; loss: 1.8434306383132935\n",
      "Training epoch 1602 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1602 ; accuracy: 0.7433333333333333; loss: 1.843580961227417\n",
      "Training epoch 1603 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1603 ; accuracy: 0.7433333333333333; loss: 1.8437318801879883\n",
      "Training epoch 1604 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1604 ; accuracy: 0.7433333333333333; loss: 1.8438783884048462\n",
      "Training epoch 1605 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1605 ; accuracy: 0.7433333333333333; loss: 1.8440269231796265\n",
      "Training epoch 1606 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1606 ; accuracy: 0.7433333333333333; loss: 1.844173789024353\n",
      "Training epoch 1607 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1607 ; accuracy: 0.7433333333333333; loss: 1.844315767288208\n",
      "Training epoch 1608 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1608 ; accuracy: 0.7433333333333333; loss: 1.844447374343872\n",
      "Training epoch 1609 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1609 ; accuracy: 0.7433333333333333; loss: 1.8445786237716675\n",
      "Training epoch 1610 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1610 ; accuracy: 0.7433333333333333; loss: 1.8447104692459106\n",
      "Training epoch 1611 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1611 ; accuracy: 0.7433333333333333; loss: 1.8448374271392822\n",
      "Training epoch 1612 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1612 ; accuracy: 0.7433333333333333; loss: 1.844966173171997\n",
      "Training epoch 1613 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1613 ; accuracy: 0.7433333333333333; loss: 1.8451049327850342\n",
      "Training epoch 1614 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1614 ; accuracy: 0.7433333333333333; loss: 1.845238208770752\n",
      "Training epoch 1615 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1615 ; accuracy: 0.7433333333333333; loss: 1.845373511314392\n",
      "Training epoch 1616 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1616 ; accuracy: 0.7433333333333333; loss: 1.84552001953125\n",
      "Training epoch 1617 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1617 ; accuracy: 0.7433333333333333; loss: 1.8456677198410034\n",
      "Training epoch 1618 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1618 ; accuracy: 0.7433333333333333; loss: 1.8458048105239868\n",
      "Training epoch 1619 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1619 ; accuracy: 0.7433333333333333; loss: 1.8459407091140747\n",
      "Training epoch 1620 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1620 ; accuracy: 0.7433333333333333; loss: 1.8460785150527954\n",
      "Training epoch 1621 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1621 ; accuracy: 0.7433333333333333; loss: 1.8462245464324951\n",
      "Training epoch 1622 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1622 ; accuracy: 0.7433333333333333; loss: 1.8463681936264038\n",
      "Training epoch 1623 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 1623 ; accuracy: 0.7433333333333333; loss: 1.8465242385864258\n",
      "Training epoch 1624 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1624 ; accuracy: 0.7433333333333333; loss: 1.8466837406158447\n",
      "Training epoch 1625 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1625 ; accuracy: 0.7433333333333333; loss: 1.8468449115753174\n",
      "Training epoch 1626 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1626 ; accuracy: 0.7433333333333333; loss: 1.8470064401626587\n",
      "Training epoch 1627 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1627 ; accuracy: 0.7433333333333333; loss: 1.8471769094467163\n",
      "Training epoch 1628 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1628 ; accuracy: 0.7433333333333333; loss: 1.84734046459198\n",
      "Training epoch 1629 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1629 ; accuracy: 0.7433333333333333; loss: 1.8475033044815063\n",
      "Training epoch 1630 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1630 ; accuracy: 0.7433333333333333; loss: 1.8476656675338745\n",
      "Training epoch 1631 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1631 ; accuracy: 0.7433333333333333; loss: 1.847825527191162\n",
      "Training epoch 1632 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1632 ; accuracy: 0.7433333333333333; loss: 1.8479927778244019\n",
      "Training epoch 1633 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1633 ; accuracy: 0.7433333333333333; loss: 1.8481594324111938\n",
      "Training epoch 1634 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1634 ; accuracy: 0.7433333333333333; loss: 1.848334550857544\n",
      "Training epoch 1635 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1635 ; accuracy: 0.7433333333333333; loss: 1.8485015630722046\n",
      "Training epoch 1636 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1636 ; accuracy: 0.7433333333333333; loss: 1.848667025566101\n",
      "Training epoch 1637 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1637 ; accuracy: 0.7433333333333333; loss: 1.8488378524780273\n",
      "Training epoch 1638 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1638 ; accuracy: 0.7433333333333333; loss: 1.8490217924118042\n",
      "Training epoch 1639 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1639 ; accuracy: 0.7433333333333333; loss: 1.8492060899734497\n",
      "Training epoch 1640 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1640 ; accuracy: 0.7433333333333333; loss: 1.8493845462799072\n",
      "Training epoch 1641 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1641 ; accuracy: 0.7433333333333333; loss: 1.849562168121338\n",
      "Training epoch 1642 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1642 ; accuracy: 0.7433333333333333; loss: 1.849718451499939\n",
      "Training epoch 1643 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1643 ; accuracy: 0.7433333333333333; loss: 1.8498718738555908\n",
      "Training epoch 1644 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1644 ; accuracy: 0.7433333333333333; loss: 1.8500244617462158\n",
      "Training epoch 1645 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1645 ; accuracy: 0.7433333333333333; loss: 1.8501710891723633\n",
      "Training epoch 1646 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1646 ; accuracy: 0.7433333333333333; loss: 1.8503096103668213\n",
      "Training epoch 1647 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1647 ; accuracy: 0.7433333333333333; loss: 1.8504557609558105\n",
      "Training epoch 1648 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1648 ; accuracy: 0.7433333333333333; loss: 1.8506050109863281\n",
      "Training epoch 1649 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1649 ; accuracy: 0.7433333333333333; loss: 1.8507492542266846\n",
      "Training epoch 1650 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1650 ; accuracy: 0.7433333333333333; loss: 1.8508907556533813\n",
      "Training epoch 1651 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1651 ; accuracy: 0.7433333333333333; loss: 1.8510382175445557\n",
      "Training epoch 1652 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1652 ; accuracy: 0.7433333333333333; loss: 1.8511881828308105\n",
      "Training epoch 1653 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1653 ; accuracy: 0.7433333333333333; loss: 1.8513413667678833\n",
      "Training epoch 1654 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1654 ; accuracy: 0.7433333333333333; loss: 1.851505160331726\n",
      "Training epoch 1655 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1655 ; accuracy: 0.7433333333333333; loss: 1.8516736030578613\n",
      "Training epoch 1656 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 1656 ; accuracy: 0.7433333333333333; loss: 1.851908564567566\n",
      "Training epoch 1657 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1657 ; accuracy: 0.7433333333333333; loss: 1.852128267288208\n",
      "Training epoch 1658 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1658 ; accuracy: 0.7433333333333333; loss: 1.8523297309875488\n",
      "Training epoch 1659 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1659 ; accuracy: 0.7433333333333333; loss: 1.8525183200836182\n",
      "Training epoch 1660 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1660 ; accuracy: 0.7433333333333333; loss: 1.8526939153671265\n",
      "Training epoch 1661 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1661 ; accuracy: 0.7433333333333333; loss: 1.8528670072555542\n",
      "Training epoch 1662 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1662 ; accuracy: 0.7433333333333333; loss: 1.8530322313308716\n",
      "Training epoch 1663 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1663 ; accuracy: 0.7433333333333333; loss: 1.8531997203826904\n",
      "Training epoch 1664 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1664 ; accuracy: 0.7433333333333333; loss: 1.8533656597137451\n",
      "Training epoch 1665 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1665 ; accuracy: 0.7433333333333333; loss: 1.853522777557373\n",
      "Training epoch 1666 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1666 ; accuracy: 0.7433333333333333; loss: 1.8536763191223145\n",
      "Training epoch 1667 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1667 ; accuracy: 0.7433333333333333; loss: 1.8538224697113037\n",
      "Training epoch 1668 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1668 ; accuracy: 0.7433333333333333; loss: 1.853962779045105\n",
      "Training epoch 1669 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1669 ; accuracy: 0.7433333333333333; loss: 1.8541069030761719\n",
      "Training epoch 1670 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1670 ; accuracy: 0.7433333333333333; loss: 1.8542661666870117\n",
      "Training epoch 1671 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1671 ; accuracy: 0.7433333333333333; loss: 1.8544210195541382\n",
      "Training epoch 1672 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1672 ; accuracy: 0.7433333333333333; loss: 1.8545745611190796\n",
      "Training epoch 1673 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1673 ; accuracy: 0.7433333333333333; loss: 1.8547308444976807\n",
      "Training epoch 1674 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1674 ; accuracy: 0.7433333333333333; loss: 1.8548885583877563\n",
      "Training epoch 1675 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1675 ; accuracy: 0.7433333333333333; loss: 1.8550466299057007\n",
      "Training epoch 1676 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1676 ; accuracy: 0.7433333333333333; loss: 1.8552024364471436\n",
      "Training epoch 1677 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1677 ; accuracy: 0.7433333333333333; loss: 1.8553481101989746\n",
      "Training epoch 1678 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1678 ; accuracy: 0.7433333333333333; loss: 1.855485439300537\n",
      "Training epoch 1679 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1679 ; accuracy: 0.7433333333333333; loss: 1.8556221723556519\n",
      "Training epoch 1680 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1680 ; accuracy: 0.7433333333333333; loss: 1.8557578325271606\n",
      "Training epoch 1681 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1681 ; accuracy: 0.7433333333333333; loss: 1.8558982610702515\n",
      "Training epoch 1682 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1682 ; accuracy: 0.7433333333333333; loss: 1.8560410737991333\n",
      "Training epoch 1683 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1683 ; accuracy: 0.7433333333333333; loss: 1.8561822175979614\n",
      "Training epoch 1684 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1684 ; accuracy: 0.7433333333333333; loss: 1.8563235998153687\n",
      "Training epoch 1685 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1685 ; accuracy: 0.7433333333333333; loss: 1.8564651012420654\n",
      "Training epoch 1686 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1686 ; accuracy: 0.7433333333333333; loss: 1.856608271598816\n",
      "Training epoch 1687 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1687 ; accuracy: 0.7433333333333333; loss: 1.856756329536438\n",
      "Training epoch 1688 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1688 ; accuracy: 0.7433333333333333; loss: 1.856902837753296\n",
      "Training epoch 1689 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1689 ; accuracy: 0.7433333333333333; loss: 1.8570462465286255\n",
      "Training epoch 1690 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 1690 ; accuracy: 0.7433333333333333; loss: 1.8571815490722656\n",
      "Training epoch 1691 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1691 ; accuracy: 0.7433333333333333; loss: 1.8573142290115356\n",
      "Training epoch 1692 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1692 ; accuracy: 0.7433333333333333; loss: 1.8574589490890503\n",
      "Training epoch 1693 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1693 ; accuracy: 0.7433333333333333; loss: 1.8576053380966187\n",
      "Training epoch 1694 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1694 ; accuracy: 0.7433333333333333; loss: 1.8577429056167603\n",
      "Training epoch 1695 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1695 ; accuracy: 0.7433333333333333; loss: 1.8578770160675049\n",
      "Training epoch 1696 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1696 ; accuracy: 0.7433333333333333; loss: 1.8580131530761719\n",
      "Training epoch 1697 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 1697 ; accuracy: 0.7433333333333333; loss: 1.8581571578979492\n",
      "Training epoch 1698 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1698 ; accuracy: 0.7433333333333333; loss: 1.8582991361618042\n",
      "Training epoch 1699 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1699 ; accuracy: 0.7433333333333333; loss: 1.858435869216919\n",
      "Training epoch 1700 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1700 ; accuracy: 0.7433333333333333; loss: 1.8585622310638428\n",
      "Training epoch 1701 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1701 ; accuracy: 0.7433333333333333; loss: 1.8586782217025757\n",
      "Training epoch 1702 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1702 ; accuracy: 0.7433333333333333; loss: 1.8587923049926758\n",
      "Training epoch 1703 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1703 ; accuracy: 0.7433333333333333; loss: 1.8589134216308594\n",
      "Training epoch 1704 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1704 ; accuracy: 0.7433333333333333; loss: 1.8590298891067505\n",
      "Training epoch 1705 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1705 ; accuracy: 0.7433333333333333; loss: 1.8591558933258057\n",
      "Training epoch 1706 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1706 ; accuracy: 0.7433333333333333; loss: 1.8592782020568848\n",
      "Training epoch 1707 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1707 ; accuracy: 0.7433333333333333; loss: 1.8594032526016235\n",
      "Training epoch 1708 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1708 ; accuracy: 0.7433333333333333; loss: 1.859526515007019\n",
      "Training epoch 1709 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1709 ; accuracy: 0.7433333333333333; loss: 1.8596488237380981\n",
      "Training epoch 1710 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1710 ; accuracy: 0.7433333333333333; loss: 1.8597824573516846\n",
      "Training epoch 1711 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1711 ; accuracy: 0.7433333333333333; loss: 1.8599128723144531\n",
      "Training epoch 1712 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1712 ; accuracy: 0.7433333333333333; loss: 1.8600478172302246\n",
      "Training epoch 1713 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1713 ; accuracy: 0.7433333333333333; loss: 1.8601808547973633\n",
      "Training epoch 1714 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1714 ; accuracy: 0.7433333333333333; loss: 1.8603153228759766\n",
      "Training epoch 1715 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1715 ; accuracy: 0.7433333333333333; loss: 1.860439419746399\n",
      "Training epoch 1716 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1716 ; accuracy: 0.7433333333333333; loss: 1.860560655593872\n",
      "Training epoch 1717 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1717 ; accuracy: 0.7433333333333333; loss: 1.8606783151626587\n",
      "Training epoch 1718 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1718 ; accuracy: 0.7433333333333333; loss: 1.8607958555221558\n",
      "Training epoch 1719 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1719 ; accuracy: 0.7433333333333333; loss: 1.8609219789505005\n",
      "Training epoch 1720 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1720 ; accuracy: 0.7433333333333333; loss: 1.8610467910766602\n",
      "Training epoch 1721 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1721 ; accuracy: 0.7433333333333333; loss: 1.861169695854187\n",
      "Training epoch 1722 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1722 ; accuracy: 0.7433333333333333; loss: 1.8612847328186035\n",
      "Training epoch 1723 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1723 ; accuracy: 0.7433333333333333; loss: 1.8613981008529663\n",
      "Training epoch 1724 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1724 ; accuracy: 0.7433333333333333; loss: 1.8615100383758545\n",
      "Training epoch 1725 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1725 ; accuracy: 0.7433333333333333; loss: 1.8616255521774292\n",
      "Training epoch 1726 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1726 ; accuracy: 0.7433333333333333; loss: 1.861725091934204\n",
      "Training epoch 1727 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1727 ; accuracy: 0.7433333333333333; loss: 1.8618216514587402\n",
      "Training epoch 1728 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1728 ; accuracy: 0.7433333333333333; loss: 1.861924648284912\n",
      "Training epoch 1729 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1729 ; accuracy: 0.7433333333333333; loss: 1.862030029296875\n",
      "Training epoch 1730 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1730 ; accuracy: 0.7433333333333333; loss: 1.8621357679367065\n",
      "Training epoch 1731 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1731 ; accuracy: 0.7433333333333333; loss: 1.8622456789016724\n",
      "Training epoch 1732 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1732 ; accuracy: 0.7433333333333333; loss: 1.8623571395874023\n",
      "Training epoch 1733 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1733 ; accuracy: 0.7433333333333333; loss: 1.8624690771102905\n",
      "Training epoch 1734 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1734 ; accuracy: 0.7433333333333333; loss: 1.8625762462615967\n",
      "Training epoch 1735 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1735 ; accuracy: 0.7433333333333333; loss: 1.8626872301101685\n",
      "Training epoch 1736 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1736 ; accuracy: 0.7433333333333333; loss: 1.8628013134002686\n",
      "Training epoch 1737 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1737 ; accuracy: 0.7433333333333333; loss: 1.8629190921783447\n",
      "Training epoch 1738 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1738 ; accuracy: 0.7433333333333333; loss: 1.8630353212356567\n",
      "Training epoch 1739 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1739 ; accuracy: 0.7433333333333333; loss: 1.8631491661071777\n",
      "Training epoch 1740 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1740 ; accuracy: 0.7433333333333333; loss: 1.8632580041885376\n",
      "Training epoch 1741 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1741 ; accuracy: 0.7433333333333333; loss: 1.8633641004562378\n",
      "Training epoch 1742 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1742 ; accuracy: 0.7433333333333333; loss: 1.8634690046310425\n",
      "Training epoch 1743 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1743 ; accuracy: 0.7433333333333333; loss: 1.8635843992233276\n",
      "Training epoch 1744 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1744 ; accuracy: 0.7433333333333333; loss: 1.8636960983276367\n",
      "Training epoch 1745 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1745 ; accuracy: 0.7433333333333333; loss: 1.8638087511062622\n",
      "Training epoch 1746 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1746 ; accuracy: 0.7433333333333333; loss: 1.8639183044433594\n",
      "Training epoch 1747 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1747 ; accuracy: 0.7433333333333333; loss: 1.8640291690826416\n",
      "Training epoch 1748 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1748 ; accuracy: 0.7433333333333333; loss: 1.8641291856765747\n",
      "Training epoch 1749 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1749 ; accuracy: 0.7433333333333333; loss: 1.864231824874878\n",
      "Training epoch 1750 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 1750 ; accuracy: 0.7433333333333333; loss: 1.8643467426300049\n",
      "Training epoch 1751 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1751 ; accuracy: 0.7433333333333333; loss: 1.8644691705703735\n",
      "Training epoch 1752 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1752 ; accuracy: 0.7433333333333333; loss: 1.8645943403244019\n",
      "Training epoch 1753 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1753 ; accuracy: 0.7433333333333333; loss: 1.8647446632385254\n",
      "Training epoch 1754 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1754 ; accuracy: 0.7433333333333333; loss: 1.864891529083252\n",
      "Training epoch 1755 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 1755 ; accuracy: 0.7433333333333333; loss: 1.8650370836257935\n",
      "Training epoch 1756 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1756 ; accuracy: 0.7433333333333333; loss: 1.865175724029541\n",
      "Training epoch 1757 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1757 ; accuracy: 0.7433333333333333; loss: 1.8653181791305542\n",
      "Training epoch 1758 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1758 ; accuracy: 0.7433333333333333; loss: 1.865452527999878\n",
      "Training epoch 1759 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1759 ; accuracy: 0.7433333333333333; loss: 1.8655856847763062\n",
      "Training epoch 1760 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1760 ; accuracy: 0.7433333333333333; loss: 1.8657186031341553\n",
      "Training epoch 1761 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1761 ; accuracy: 0.7433333333333333; loss: 1.865848422050476\n",
      "Training epoch 1762 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1762 ; accuracy: 0.7433333333333333; loss: 1.8659708499908447\n",
      "Training epoch 1763 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1763 ; accuracy: 0.7433333333333333; loss: 1.8660857677459717\n",
      "Training epoch 1764 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1764 ; accuracy: 0.7433333333333333; loss: 1.8662028312683105\n",
      "Training epoch 1765 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1765 ; accuracy: 0.7433333333333333; loss: 1.8663206100463867\n",
      "Training epoch 1766 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1766 ; accuracy: 0.7433333333333333; loss: 1.8664422035217285\n",
      "Training epoch 1767 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 1767 ; accuracy: 0.7433333333333333; loss: 1.8665720224380493\n",
      "Training epoch 1768 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1768 ; accuracy: 0.7433333333333333; loss: 1.8667012453079224\n",
      "Training epoch 1769 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1769 ; accuracy: 0.7433333333333333; loss: 1.8668347597122192\n",
      "Training epoch 1770 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1770 ; accuracy: 0.7433333333333333; loss: 1.8669604063034058\n",
      "Training epoch 1771 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1771 ; accuracy: 0.7433333333333333; loss: 1.8670881986618042\n",
      "Training epoch 1772 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1772 ; accuracy: 0.7433333333333333; loss: 1.8672168254852295\n",
      "Training epoch 1773 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1773 ; accuracy: 0.7433333333333333; loss: 1.8673490285873413\n",
      "Training epoch 1774 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1774 ; accuracy: 0.7433333333333333; loss: 1.8674851655960083\n",
      "Training epoch 1775 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1775 ; accuracy: 0.7433333333333333; loss: 1.8676259517669678\n",
      "Training epoch 1776 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1776 ; accuracy: 0.7433333333333333; loss: 1.8677676916122437\n",
      "Training epoch 1777 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 1777 ; accuracy: 0.7433333333333333; loss: 1.867914080619812\n",
      "Training epoch 1778 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1778 ; accuracy: 0.7433333333333333; loss: 1.8680388927459717\n",
      "Training epoch 1779 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1779 ; accuracy: 0.7433333333333333; loss: 1.8681628704071045\n",
      "Training epoch 1780 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1780 ; accuracy: 0.7433333333333333; loss: 1.8682852983474731\n",
      "Training epoch 1781 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1781 ; accuracy: 0.7433333333333333; loss: 1.868414044380188\n",
      "Training epoch 1782 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1782 ; accuracy: 0.7433333333333333; loss: 1.8685463666915894\n",
      "Training epoch 1783 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1783 ; accuracy: 0.7433333333333333; loss: 1.8686779737472534\n",
      "Training epoch 1784 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1784 ; accuracy: 0.7433333333333333; loss: 1.8688006401062012\n",
      "Training epoch 1785 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1785 ; accuracy: 0.7433333333333333; loss: 1.8689197301864624\n",
      "Training epoch 1786 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 1786 ; accuracy: 0.7433333333333333; loss: 1.8690435886383057\n",
      "Training epoch 1787 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1787 ; accuracy: 0.7433333333333333; loss: 1.8691776990890503\n",
      "Training epoch 1788 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1788 ; accuracy: 0.7433333333333333; loss: 1.8693066835403442\n",
      "Training epoch 1789 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1789 ; accuracy: 0.7433333333333333; loss: 1.869431734085083\n",
      "Training epoch 1790 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1790 ; accuracy: 0.7433333333333333; loss: 1.8695597648620605\n",
      "Training epoch 1791 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1791 ; accuracy: 0.7433333333333333; loss: 1.8696852922439575\n",
      "Training epoch 1792 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1792 ; accuracy: 0.7433333333333333; loss: 1.8698174953460693\n",
      "Training epoch 1793 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1793 ; accuracy: 0.7433333333333333; loss: 1.8699496984481812\n",
      "Training epoch 1794 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1794 ; accuracy: 0.7433333333333333; loss: 1.8700835704803467\n",
      "Training epoch 1795 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1795 ; accuracy: 0.7433333333333333; loss: 1.8702176809310913\n",
      "Training epoch 1796 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1796 ; accuracy: 0.7433333333333333; loss: 1.8703434467315674\n",
      "Training epoch 1797 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1797 ; accuracy: 0.7433333333333333; loss: 1.870465874671936\n",
      "Training epoch 1798 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1798 ; accuracy: 0.7433333333333333; loss: 1.870587944984436\n",
      "Training epoch 1799 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1799 ; accuracy: 0.7433333333333333; loss: 1.8707120418548584\n",
      "Training epoch 1800 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1800 ; accuracy: 0.7433333333333333; loss: 1.8708248138427734\n",
      "Training epoch 1801 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1801 ; accuracy: 0.7433333333333333; loss: 1.8709417581558228\n",
      "Training epoch 1802 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1802 ; accuracy: 0.7433333333333333; loss: 1.8710699081420898\n",
      "Training epoch 1803 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1803 ; accuracy: 0.7433333333333333; loss: 1.8711849451065063\n",
      "Training epoch 1804 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1804 ; accuracy: 0.7433333333333333; loss: 1.8713091611862183\n",
      "Training epoch 1805 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1805 ; accuracy: 0.7433333333333333; loss: 1.8714423179626465\n",
      "Training epoch 1806 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1806 ; accuracy: 0.7433333333333333; loss: 1.8715722560882568\n",
      "Training epoch 1807 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1807 ; accuracy: 0.7433333333333333; loss: 1.8717052936553955\n",
      "Training epoch 1808 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1808 ; accuracy: 0.7433333333333333; loss: 1.8718359470367432\n",
      "Training epoch 1809 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1809 ; accuracy: 0.7433333333333333; loss: 1.8719614744186401\n",
      "Training epoch 1810 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1810 ; accuracy: 0.7433333333333333; loss: 1.8720866441726685\n",
      "Training epoch 1811 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1811 ; accuracy: 0.7433333333333333; loss: 1.8722110986709595\n",
      "Training epoch 1812 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 1812 ; accuracy: 0.7433333333333333; loss: 1.872340440750122\n",
      "Training epoch 1813 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1813 ; accuracy: 0.7433333333333333; loss: 1.8724751472473145\n",
      "Training epoch 1814 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1814 ; accuracy: 0.7433333333333333; loss: 1.8726130723953247\n",
      "Training epoch 1815 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1815 ; accuracy: 0.7433333333333333; loss: 1.872747540473938\n",
      "Training epoch 1816 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1816 ; accuracy: 0.7433333333333333; loss: 1.8728855848312378\n",
      "Training epoch 1817 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1817 ; accuracy: 0.7433333333333333; loss: 1.8730175495147705\n",
      "Training epoch 1818 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1818 ; accuracy: 0.7433333333333333; loss: 1.873145341873169\n",
      "Training epoch 1819 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1819 ; accuracy: 0.7433333333333333; loss: 1.8732712268829346\n",
      "Training epoch 1820 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1820 ; accuracy: 0.7433333333333333; loss: 1.8733980655670166\n",
      "Training epoch 1821 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1821 ; accuracy: 0.7433333333333333; loss: 1.8735251426696777\n",
      "Training epoch 1822 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1822 ; accuracy: 0.7433333333333333; loss: 1.8736472129821777\n",
      "Training epoch 1823 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1823 ; accuracy: 0.7433333333333333; loss: 1.8737540245056152\n",
      "Training epoch 1824 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1824 ; accuracy: 0.7433333333333333; loss: 1.8738576173782349\n",
      "Training epoch 1825 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1825 ; accuracy: 0.7433333333333333; loss: 1.8739715814590454\n",
      "Training epoch 1826 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1826 ; accuracy: 0.7433333333333333; loss: 1.874083399772644\n",
      "Training epoch 1827 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1827 ; accuracy: 0.7433333333333333; loss: 1.8741929531097412\n",
      "Training epoch 1828 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1828 ; accuracy: 0.7433333333333333; loss: 1.874300479888916\n",
      "Training epoch 1829 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1829 ; accuracy: 0.7433333333333333; loss: 1.8744109869003296\n",
      "Training epoch 1830 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1830 ; accuracy: 0.7433333333333333; loss: 1.874531865119934\n",
      "Training epoch 1831 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1831 ; accuracy: 0.7433333333333333; loss: 1.8746452331542969\n",
      "Training epoch 1832 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1832 ; accuracy: 0.7433333333333333; loss: 1.874760389328003\n",
      "Training epoch 1833 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1833 ; accuracy: 0.7433333333333333; loss: 1.8748914003372192\n",
      "Training epoch 1834 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1834 ; accuracy: 0.7433333333333333; loss: 1.8750183582305908\n",
      "Training epoch 1835 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1835 ; accuracy: 0.7433333333333333; loss: 1.8751448392868042\n",
      "Training epoch 1836 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1836 ; accuracy: 0.7433333333333333; loss: 1.8752694129943848\n",
      "Training epoch 1837 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1837 ; accuracy: 0.7433333333333333; loss: 1.8754068613052368\n",
      "Training epoch 1838 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1838 ; accuracy: 0.7433333333333333; loss: 1.8755481243133545\n",
      "Training epoch 1839 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1839 ; accuracy: 0.7433333333333333; loss: 1.875678300857544\n",
      "Training epoch 1840 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1840 ; accuracy: 0.7433333333333333; loss: 1.875806450843811\n",
      "Training epoch 1841 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1841 ; accuracy: 0.7433333333333333; loss: 1.8759256601333618\n",
      "Training epoch 1842 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1842 ; accuracy: 0.7433333333333333; loss: 1.8760615587234497\n",
      "Training epoch 1843 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1843 ; accuracy: 0.7433333333333333; loss: 1.876196265220642\n",
      "Training epoch 1844 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1844 ; accuracy: 0.7433333333333333; loss: 1.876326322555542\n",
      "Training epoch 1845 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1845 ; accuracy: 0.7433333333333333; loss: 1.876454472541809\n",
      "Training epoch 1846 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1846 ; accuracy: 0.7433333333333333; loss: 1.876580834388733\n",
      "Training epoch 1847 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1847 ; accuracy: 0.7433333333333333; loss: 1.8767049312591553\n",
      "Training epoch 1848 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1848 ; accuracy: 0.7433333333333333; loss: 1.8768365383148193\n",
      "Training epoch 1849 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1849 ; accuracy: 0.7433333333333333; loss: 1.8769716024398804\n",
      "Training epoch 1850 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1850 ; accuracy: 0.7433333333333333; loss: 1.8770999908447266\n",
      "Training epoch 1851 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1851 ; accuracy: 0.7433333333333333; loss: 1.8772261142730713\n",
      "Training epoch 1852 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1852 ; accuracy: 0.7433333333333333; loss: 1.877353310585022\n",
      "Training epoch 1853 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1853 ; accuracy: 0.7433333333333333; loss: 1.877477765083313\n",
      "Training epoch 1854 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1854 ; accuracy: 0.7433333333333333; loss: 1.877600908279419\n",
      "Training epoch 1855 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1855 ; accuracy: 0.7433333333333333; loss: 1.8777223825454712\n",
      "Training epoch 1856 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1856 ; accuracy: 0.7433333333333333; loss: 1.8778438568115234\n",
      "Training epoch 1857 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1857 ; accuracy: 0.7433333333333333; loss: 1.877967357635498\n",
      "Training epoch 1858 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1858 ; accuracy: 0.7433333333333333; loss: 1.8780877590179443\n",
      "Training epoch 1859 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1859 ; accuracy: 0.7433333333333333; loss: 1.878204584121704\n",
      "Training epoch 1860 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1860 ; accuracy: 0.7433333333333333; loss: 1.8783178329467773\n",
      "Training epoch 1861 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1861 ; accuracy: 0.7433333333333333; loss: 1.878430962562561\n",
      "Training epoch 1862 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1862 ; accuracy: 0.7433333333333333; loss: 1.8785412311553955\n",
      "Training epoch 1863 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1863 ; accuracy: 0.7433333333333333; loss: 1.8786578178405762\n",
      "Training epoch 1864 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1864 ; accuracy: 0.7433333333333333; loss: 1.8787740468978882\n",
      "Training epoch 1865 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1865 ; accuracy: 0.7433333333333333; loss: 1.8789005279541016\n",
      "Training epoch 1866 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1866 ; accuracy: 0.7433333333333333; loss: 1.8790256977081299\n",
      "Training epoch 1867 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1867 ; accuracy: 0.7433333333333333; loss: 1.8791512250900269\n",
      "Training epoch 1868 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1868 ; accuracy: 0.7433333333333333; loss: 1.8792773485183716\n",
      "Training epoch 1869 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1869 ; accuracy: 0.7433333333333333; loss: 1.8794069290161133\n",
      "Training epoch 1870 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1870 ; accuracy: 0.7433333333333333; loss: 1.8795353174209595\n",
      "Training epoch 1871 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1871 ; accuracy: 0.7433333333333333; loss: 1.8796665668487549\n",
      "Training epoch 1872 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1872 ; accuracy: 0.7433333333333333; loss: 1.8797961473464966\n",
      "Training epoch 1873 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1873 ; accuracy: 0.7433333333333333; loss: 1.8799248933792114\n",
      "Training epoch 1874 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1874 ; accuracy: 0.7433333333333333; loss: 1.8800547122955322\n",
      "Training epoch 1875 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1875 ; accuracy: 0.7433333333333333; loss: 1.8801730871200562\n",
      "Training epoch 1876 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1876 ; accuracy: 0.7433333333333333; loss: 1.880290150642395\n",
      "Training epoch 1877 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1877 ; accuracy: 0.7433333333333333; loss: 1.8804044723510742\n",
      "Training epoch 1878 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1878 ; accuracy: 0.7433333333333333; loss: 1.8805181980133057\n",
      "Training epoch 1879 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1879 ; accuracy: 0.7433333333333333; loss: 1.8806368112564087\n",
      "Training epoch 1880 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1880 ; accuracy: 0.7433333333333333; loss: 1.8807566165924072\n",
      "Training epoch 1881 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1881 ; accuracy: 0.7433333333333333; loss: 1.8808882236480713\n",
      "Training epoch 1882 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 1882 ; accuracy: 0.7433333333333333; loss: 1.8810296058654785\n",
      "Training epoch 1883 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 1883 ; accuracy: 0.7433333333333333; loss: 1.881170630455017\n",
      "Training epoch 1884 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1884 ; accuracy: 0.7433333333333333; loss: 1.8813116550445557\n",
      "Training epoch 1885 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1885 ; accuracy: 0.7433333333333333; loss: 1.8814524412155151\n",
      "Training epoch 1886 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1886 ; accuracy: 0.7433333333333333; loss: 1.881595492362976\n",
      "Training epoch 1887 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1887 ; accuracy: 0.7433333333333333; loss: 1.8817307949066162\n",
      "Training epoch 1888 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1888 ; accuracy: 0.7433333333333333; loss: 1.8818656206130981\n",
      "Training epoch 1889 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1889 ; accuracy: 0.7433333333333333; loss: 1.8819905519485474\n",
      "Training epoch 1890 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1890 ; accuracy: 0.7433333333333333; loss: 1.8821200132369995\n",
      "Training epoch 1891 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1891 ; accuracy: 0.7433333333333333; loss: 1.8822418451309204\n",
      "Training epoch 1892 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1892 ; accuracy: 0.7433333333333333; loss: 1.8823661804199219\n",
      "Training epoch 1893 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1893 ; accuracy: 0.7433333333333333; loss: 1.882484793663025\n",
      "Training epoch 1894 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1894 ; accuracy: 0.7433333333333333; loss: 1.8826086521148682\n",
      "Training epoch 1895 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1895 ; accuracy: 0.7433333333333333; loss: 1.8827393054962158\n",
      "Training epoch 1896 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1896 ; accuracy: 0.7433333333333333; loss: 1.8828800916671753\n",
      "Training epoch 1897 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1897 ; accuracy: 0.7433333333333333; loss: 1.8830111026763916\n",
      "Training epoch 1898 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1898 ; accuracy: 0.7433333333333333; loss: 1.8831435441970825\n",
      "Training epoch 1899 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1899 ; accuracy: 0.7433333333333333; loss: 1.883280634880066\n",
      "Training epoch 1900 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1900 ; accuracy: 0.7433333333333333; loss: 1.8834093809127808\n",
      "Training epoch 1901 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 1901 ; accuracy: 0.7433333333333333; loss: 1.8835316896438599\n",
      "Training epoch 1902 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1902 ; accuracy: 0.7433333333333333; loss: 1.8836370706558228\n",
      "Training epoch 1903 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1903 ; accuracy: 0.7433333333333333; loss: 1.8837294578552246\n",
      "Training epoch 1904 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1904 ; accuracy: 0.7433333333333333; loss: 1.883826732635498\n",
      "Training epoch 1905 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1905 ; accuracy: 0.7433333333333333; loss: 1.8839237689971924\n",
      "Training epoch 1906 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1906 ; accuracy: 0.7433333333333333; loss: 1.8840384483337402\n",
      "Training epoch 1907 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1907 ; accuracy: 0.7433333333333333; loss: 1.884158730506897\n",
      "Training epoch 1908 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 1908 ; accuracy: 0.7433333333333333; loss: 1.8842663764953613\n",
      "Training epoch 1909 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1909 ; accuracy: 0.7433333333333333; loss: 1.884369969367981\n",
      "Training epoch 1910 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1910 ; accuracy: 0.7433333333333333; loss: 1.8844679594039917\n",
      "Training epoch 1911 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1911 ; accuracy: 0.7433333333333333; loss: 1.8845562934875488\n",
      "Training epoch 1912 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1912 ; accuracy: 0.7433333333333333; loss: 1.8846369981765747\n",
      "Training epoch 1913 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1913 ; accuracy: 0.7433333333333333; loss: 1.8847278356552124\n",
      "Training epoch 1914 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1914 ; accuracy: 0.7433333333333333; loss: 1.8848168849945068\n",
      "Training epoch 1915 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1915 ; accuracy: 0.7433333333333333; loss: 1.8849133253097534\n",
      "Training epoch 1916 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1916 ; accuracy: 0.7433333333333333; loss: 1.8850140571594238\n",
      "Training epoch 1917 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1917 ; accuracy: 0.7433333333333333; loss: 1.8851182460784912\n",
      "Training epoch 1918 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1918 ; accuracy: 0.7433333333333333; loss: 1.8852092027664185\n",
      "Training epoch 1919 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1919 ; accuracy: 0.7433333333333333; loss: 1.8853033781051636\n",
      "Training epoch 1920 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1920 ; accuracy: 0.7433333333333333; loss: 1.885404109954834\n",
      "Training epoch 1921 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1921 ; accuracy: 0.7433333333333333; loss: 1.8855078220367432\n",
      "Training epoch 1922 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1922 ; accuracy: 0.7433333333333333; loss: 1.8856174945831299\n",
      "Training epoch 1923 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 1923 ; accuracy: 0.7433333333333333; loss: 1.8857336044311523\n",
      "Training epoch 1924 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1924 ; accuracy: 0.7433333333333333; loss: 1.885839819908142\n",
      "Training epoch 1925 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1925 ; accuracy: 0.7433333333333333; loss: 1.885952353477478\n",
      "Training epoch 1926 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1926 ; accuracy: 0.7433333333333333; loss: 1.886060357093811\n",
      "Training epoch 1927 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1927 ; accuracy: 0.7433333333333333; loss: 1.8861737251281738\n",
      "Training epoch 1928 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1928 ; accuracy: 0.7433333333333333; loss: 1.8862926959991455\n",
      "Training epoch 1929 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1929 ; accuracy: 0.7433333333333333; loss: 1.88641357421875\n",
      "Training epoch 1930 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1930 ; accuracy: 0.7433333333333333; loss: 1.8865325450897217\n",
      "Training epoch 1931 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1931 ; accuracy: 0.7433333333333333; loss: 1.8866522312164307\n",
      "Training epoch 1932 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1932 ; accuracy: 0.7433333333333333; loss: 1.886770248413086\n",
      "Training epoch 1933 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1933 ; accuracy: 0.7433333333333333; loss: 1.886888027191162\n",
      "Training epoch 1934 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1934 ; accuracy: 0.7433333333333333; loss: 1.8870058059692383\n",
      "Training epoch 1935 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 1935 ; accuracy: 0.7433333333333333; loss: 1.887123465538025\n",
      "Training epoch 1936 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1936 ; accuracy: 0.7433333333333333; loss: 1.8872407674789429\n",
      "Training epoch 1937 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1937 ; accuracy: 0.7433333333333333; loss: 1.8873553276062012\n",
      "Training epoch 1938 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1938 ; accuracy: 0.7433333333333333; loss: 1.8874716758728027\n",
      "Training epoch 1939 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1939 ; accuracy: 0.7433333333333333; loss: 1.8875927925109863\n",
      "Training epoch 1940 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1940 ; accuracy: 0.7433333333333333; loss: 1.8877201080322266\n",
      "Training epoch 1941 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1941 ; accuracy: 0.7433333333333333; loss: 1.887843370437622\n",
      "Training epoch 1942 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1942 ; accuracy: 0.7433333333333333; loss: 1.8879634141921997\n",
      "Training epoch 1943 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1943 ; accuracy: 0.7433333333333333; loss: 1.8880831003189087\n",
      "Training epoch 1944 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1944 ; accuracy: 0.7433333333333333; loss: 1.8881956338882446\n",
      "Training epoch 1945 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 1945 ; accuracy: 0.7433333333333333; loss: 1.8883068561553955\n",
      "Training epoch 1946 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1946 ; accuracy: 0.7433333333333333; loss: 1.888427495956421\n",
      "Training epoch 1947 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 1947 ; accuracy: 0.7433333333333333; loss: 1.8885529041290283\n",
      "Training epoch 1948 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1948 ; accuracy: 0.7433333333333333; loss: 1.8886680603027344\n",
      "Training epoch 1949 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1949 ; accuracy: 0.7433333333333333; loss: 1.8887825012207031\n",
      "Training epoch 1950 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1950 ; accuracy: 0.7433333333333333; loss: 1.8889074325561523\n",
      "Training epoch 1951 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1951 ; accuracy: 0.7433333333333333; loss: 1.8890384435653687\n",
      "Training epoch 1952 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1952 ; accuracy: 0.7433333333333333; loss: 1.8891682624816895\n",
      "Training epoch 1953 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 1953 ; accuracy: 0.7433333333333333; loss: 1.889296054840088\n",
      "Training epoch 1954 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1954 ; accuracy: 0.7433333333333333; loss: 1.889417290687561\n",
      "Training epoch 1955 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1955 ; accuracy: 0.7433333333333333; loss: 1.8895211219787598\n",
      "Training epoch 1956 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1956 ; accuracy: 0.7433333333333333; loss: 1.8896098136901855\n",
      "Training epoch 1957 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1957 ; accuracy: 0.7433333333333333; loss: 1.8896870613098145\n",
      "Training epoch 1958 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1958 ; accuracy: 0.7433333333333333; loss: 1.8897711038589478\n",
      "Training epoch 1959 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1959 ; accuracy: 0.7433333333333333; loss: 1.8898502588272095\n",
      "Training epoch 1960 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1960 ; accuracy: 0.7433333333333333; loss: 1.8899308443069458\n",
      "Training epoch 1961 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 1961 ; accuracy: 0.7433333333333333; loss: 1.8900238275527954\n",
      "Training epoch 1962 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 1962 ; accuracy: 0.7433333333333333; loss: 1.8901231288909912\n",
      "Training epoch 1963 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1963 ; accuracy: 0.7433333333333333; loss: 1.8902239799499512\n",
      "Training epoch 1964 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1964 ; accuracy: 0.7433333333333333; loss: 1.89031183719635\n",
      "Training epoch 1965 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1965 ; accuracy: 0.7433333333333333; loss: 1.890403389930725\n",
      "Training epoch 1966 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1966 ; accuracy: 0.7433333333333333; loss: 1.8905041217803955\n",
      "Training epoch 1967 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 1967 ; accuracy: 0.7433333333333333; loss: 1.8905985355377197\n",
      "Training epoch 1968 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1968 ; accuracy: 0.7433333333333333; loss: 1.8906872272491455\n",
      "Training epoch 1969 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1969 ; accuracy: 0.7433333333333333; loss: 1.8907827138900757\n",
      "Training epoch 1970 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 1970 ; accuracy: 0.7433333333333333; loss: 1.8908805847167969\n",
      "Training epoch 1971 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 1971 ; accuracy: 0.7433333333333333; loss: 1.8909834623336792\n",
      "Training epoch 1972 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 1972 ; accuracy: 0.7433333333333333; loss: 1.8910870552062988\n",
      "Training epoch 1973 ; accuracy: 0.9; loss: 0.19476942718029022\n",
      "Validation epoch 1973 ; accuracy: 0.7366666666666667; loss: 1.843721866607666\n",
      "Training epoch 1974 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 1974 ; accuracy: 0.7366666666666667; loss: 1.8079540729522705\n",
      "Training epoch 1975 ; accuracy: 0.9; loss: 0.19459235668182373\n",
      "Validation epoch 1975 ; accuracy: 0.7366666666666667; loss: 1.781396508216858\n",
      "Training epoch 1976 ; accuracy: 0.9; loss: 0.19459371268749237\n",
      "Validation epoch 1976 ; accuracy: 0.74; loss: 1.759846568107605\n",
      "Training epoch 1977 ; accuracy: 0.9; loss: 0.19460399448871613\n",
      "Validation epoch 1977 ; accuracy: 0.7333333333333333; loss: 1.7437968254089355\n",
      "Training epoch 1978 ; accuracy: 0.9; loss: 0.19465371966362\n",
      "Validation epoch 1978 ; accuracy: 0.7366666666666667; loss: 1.7326875925064087\n",
      "Training epoch 1979 ; accuracy: 0.9; loss: 0.19471147656440735\n",
      "Validation epoch 1979 ; accuracy: 0.74; loss: 1.7284897565841675\n",
      "Training epoch 1980 ; accuracy: 0.9; loss: 0.19469785690307617\n",
      "Validation epoch 1980 ; accuracy: 0.74; loss: 1.7288625240325928\n",
      "Training epoch 1981 ; accuracy: 0.9; loss: 0.1946452260017395\n",
      "Validation epoch 1981 ; accuracy: 0.74; loss: 1.731360673904419\n",
      "Training epoch 1982 ; accuracy: 0.9; loss: 0.19474607706069946\n",
      "Validation epoch 1982 ; accuracy: 0.7333333333333333; loss: 1.7395139932632446\n",
      "Training epoch 1983 ; accuracy: 0.9; loss: 0.19472014904022217\n",
      "Validation epoch 1983 ; accuracy: 0.7333333333333333; loss: 1.753557562828064\n",
      "Training epoch 1984 ; accuracy: 0.9; loss: 0.19461973011493683\n",
      "Validation epoch 1984 ; accuracy: 0.73; loss: 1.7686681747436523\n",
      "Training epoch 1985 ; accuracy: 0.9; loss: 0.1946256309747696\n",
      "Validation epoch 1985 ; accuracy: 0.7366666666666667; loss: 1.785942792892456\n",
      "Training epoch 1986 ; accuracy: 0.9; loss: 0.19460338354110718\n",
      "Validation epoch 1986 ; accuracy: 0.7366666666666667; loss: 1.8032238483428955\n",
      "Training epoch 1987 ; accuracy: 0.9; loss: 0.1945943385362625\n",
      "Validation epoch 1987 ; accuracy: 0.7366666666666667; loss: 1.819922924041748\n",
      "Training epoch 1988 ; accuracy: 0.9; loss: 0.1945974975824356\n",
      "Validation epoch 1988 ; accuracy: 0.7366666666666667; loss: 1.8362852334976196\n",
      "Training epoch 1989 ; accuracy: 0.9; loss: 0.19459323585033417\n",
      "Validation epoch 1989 ; accuracy: 0.7333333333333333; loss: 1.8524106740951538\n",
      "Training epoch 1990 ; accuracy: 0.9; loss: 0.19459447264671326\n",
      "Validation epoch 1990 ; accuracy: 0.7333333333333333; loss: 1.8680866956710815\n",
      "Training epoch 1991 ; accuracy: 0.9; loss: 0.19459357857704163\n",
      "Validation epoch 1991 ; accuracy: 0.7333333333333333; loss: 1.8833552598953247\n",
      "Training epoch 1992 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 1992 ; accuracy: 0.7366666666666667; loss: 1.898007869720459\n",
      "Training epoch 1993 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 1993 ; accuracy: 0.7333333333333333; loss: 1.912043809890747\n",
      "Training epoch 1994 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 1994 ; accuracy: 0.7366666666666667; loss: 1.9248757362365723\n",
      "Training epoch 1995 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 1995 ; accuracy: 0.7333333333333333; loss: 1.936916470527649\n",
      "Training epoch 1996 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 1996 ; accuracy: 0.7333333333333333; loss: 1.9482232332229614\n",
      "Training epoch 1997 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 1997 ; accuracy: 0.7333333333333333; loss: 1.9587808847427368\n",
      "Training epoch 1998 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 1998 ; accuracy: 0.73; loss: 1.968601107597351\n",
      "Training epoch 1999 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 1999 ; accuracy: 0.73; loss: 1.9776861667633057\n",
      "Training epoch 2000 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2000 ; accuracy: 0.73; loss: 1.9860221147537231\n",
      "Training epoch 2001 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 2001 ; accuracy: 0.73; loss: 1.993656873703003\n",
      "Training epoch 2002 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 2002 ; accuracy: 0.73; loss: 2.0006103515625\n",
      "Training epoch 2003 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2003 ; accuracy: 0.7266666666666667; loss: 2.0069148540496826\n",
      "Training epoch 2004 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 2004 ; accuracy: 0.7266666666666667; loss: 2.0126075744628906\n",
      "Training epoch 2005 ; accuracy: 0.9; loss: 0.19459183514118195\n",
      "Validation epoch 2005 ; accuracy: 0.7266666666666667; loss: 2.0177395343780518\n",
      "Training epoch 2006 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 2006 ; accuracy: 0.7266666666666667; loss: 2.022338390350342\n",
      "Training epoch 2007 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 2007 ; accuracy: 0.7266666666666667; loss: 2.0264368057250977\n",
      "Training epoch 2008 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 2008 ; accuracy: 0.7266666666666667; loss: 2.0300791263580322\n",
      "Training epoch 2009 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 2009 ; accuracy: 0.7233333333333334; loss: 2.033311605453491\n",
      "Training epoch 2010 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 2010 ; accuracy: 0.7233333333333334; loss: 2.036144495010376\n",
      "Training epoch 2011 ; accuracy: 0.9; loss: 0.19459180533885956\n",
      "Validation epoch 2011 ; accuracy: 0.7233333333333334; loss: 2.0386276245117188\n",
      "Training epoch 2012 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 2012 ; accuracy: 0.7233333333333334; loss: 2.0407373905181885\n",
      "Training epoch 2013 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 2013 ; accuracy: 0.7233333333333334; loss: 2.0425469875335693\n",
      "Training epoch 2014 ; accuracy: 0.9; loss: 0.19459208846092224\n",
      "Validation epoch 2014 ; accuracy: 0.7233333333333334; loss: 2.0440354347229004\n",
      "Training epoch 2015 ; accuracy: 0.9; loss: 0.19459199905395508\n",
      "Validation epoch 2015 ; accuracy: 0.7233333333333334; loss: 2.0452373027801514\n",
      "Training epoch 2016 ; accuracy: 0.9; loss: 0.19459223747253418\n",
      "Validation epoch 2016 ; accuracy: 0.7233333333333334; loss: 2.0462279319763184\n",
      "Training epoch 2017 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2017 ; accuracy: 0.7233333333333334; loss: 2.047081232070923\n",
      "Training epoch 2018 ; accuracy: 0.9; loss: 0.19459213316440582\n",
      "Validation epoch 2018 ; accuracy: 0.7233333333333334; loss: 2.0477075576782227\n",
      "Training epoch 2019 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 2019 ; accuracy: 0.7233333333333334; loss: 2.048165798187256\n",
      "Training epoch 2020 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 2020 ; accuracy: 0.7233333333333334; loss: 2.0485122203826904\n",
      "Training epoch 2021 ; accuracy: 0.9; loss: 0.19459189474582672\n",
      "Validation epoch 2021 ; accuracy: 0.7233333333333334; loss: 2.0486996173858643\n",
      "Training epoch 2022 ; accuracy: 0.9; loss: 0.1945919692516327\n",
      "Validation epoch 2022 ; accuracy: 0.7233333333333334; loss: 2.048736095428467\n",
      "Training epoch 2023 ; accuracy: 0.9; loss: 0.19459177553653717\n",
      "Validation epoch 2023 ; accuracy: 0.7233333333333334; loss: 2.048696279525757\n",
      "Training epoch 2024 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2024 ; accuracy: 0.7233333333333334; loss: 2.0486109256744385\n",
      "Training epoch 2025 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 2025 ; accuracy: 0.7233333333333334; loss: 2.0484557151794434\n",
      "Training epoch 2026 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 2026 ; accuracy: 0.7233333333333334; loss: 2.0482563972473145\n",
      "Training epoch 2027 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 2027 ; accuracy: 0.7233333333333334; loss: 2.048006057739258\n",
      "Training epoch 2028 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2028 ; accuracy: 0.7233333333333334; loss: 2.047720432281494\n",
      "Training epoch 2029 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 2029 ; accuracy: 0.7266666666666667; loss: 2.047398567199707\n",
      "Training epoch 2030 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 2030 ; accuracy: 0.7266666666666667; loss: 2.0470705032348633\n",
      "Training epoch 2031 ; accuracy: 0.9; loss: 0.1945919692516327\n",
      "Validation epoch 2031 ; accuracy: 0.7266666666666667; loss: 2.046656608581543\n",
      "Training epoch 2032 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2032 ; accuracy: 0.7266666666666667; loss: 2.0462281703948975\n",
      "Training epoch 2033 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2033 ; accuracy: 0.7266666666666667; loss: 2.0457990169525146\n",
      "Training epoch 2034 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2034 ; accuracy: 0.7266666666666667; loss: 2.045365333557129\n",
      "Training epoch 2035 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2035 ; accuracy: 0.73; loss: 2.0449185371398926\n",
      "Training epoch 2036 ; accuracy: 0.9; loss: 0.19459207355976105\n",
      "Validation epoch 2036 ; accuracy: 0.73; loss: 2.0444211959838867\n",
      "Training epoch 2037 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 2037 ; accuracy: 0.73; loss: 2.0439066886901855\n",
      "Training epoch 2038 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2038 ; accuracy: 0.73; loss: 2.043396472930908\n",
      "Training epoch 2039 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2039 ; accuracy: 0.73; loss: 2.0428965091705322\n",
      "Training epoch 2040 ; accuracy: 0.9; loss: 0.1945919692516327\n",
      "Validation epoch 2040 ; accuracy: 0.73; loss: 2.042409896850586\n",
      "Training epoch 2041 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2041 ; accuracy: 0.73; loss: 2.0419442653656006\n",
      "Training epoch 2042 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2042 ; accuracy: 0.73; loss: 2.0414912700653076\n",
      "Training epoch 2043 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 2043 ; accuracy: 0.73; loss: 2.041018486022949\n",
      "Training epoch 2044 ; accuracy: 0.9; loss: 0.19459168612957\n",
      "Validation epoch 2044 ; accuracy: 0.73; loss: 2.040529489517212\n",
      "Training epoch 2045 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2045 ; accuracy: 0.73; loss: 2.040064811706543\n",
      "Training epoch 2046 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2046 ; accuracy: 0.73; loss: 2.0396111011505127\n",
      "Training epoch 2047 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2047 ; accuracy: 0.73; loss: 2.039156913757324\n",
      "Training epoch 2048 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2048 ; accuracy: 0.73; loss: 2.038715362548828\n",
      "Training epoch 2049 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 2049 ; accuracy: 0.73; loss: 2.038282632827759\n",
      "Training epoch 2050 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2050 ; accuracy: 0.73; loss: 2.03786301612854\n",
      "Training epoch 2051 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2051 ; accuracy: 0.7333333333333333; loss: 2.0374674797058105\n",
      "Training epoch 2052 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2052 ; accuracy: 0.7333333333333333; loss: 2.037128210067749\n",
      "Training epoch 2053 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2053 ; accuracy: 0.7333333333333333; loss: 2.036797046661377\n",
      "Training epoch 2054 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2054 ; accuracy: 0.7333333333333333; loss: 2.036480188369751\n",
      "Training epoch 2055 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2055 ; accuracy: 0.7333333333333333; loss: 2.03617262840271\n",
      "Training epoch 2056 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2056 ; accuracy: 0.7333333333333333; loss: 2.0358588695526123\n",
      "Training epoch 2057 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2057 ; accuracy: 0.7333333333333333; loss: 2.035548686981201\n",
      "Training epoch 2058 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 2058 ; accuracy: 0.7333333333333333; loss: 2.035231828689575\n",
      "Training epoch 2059 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2059 ; accuracy: 0.7333333333333333; loss: 2.034910202026367\n",
      "Training epoch 2060 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2060 ; accuracy: 0.7333333333333333; loss: 2.034609079360962\n",
      "Training epoch 2061 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2061 ; accuracy: 0.7333333333333333; loss: 2.0343008041381836\n",
      "Training epoch 2062 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2062 ; accuracy: 0.7333333333333333; loss: 2.0339853763580322\n",
      "Training epoch 2063 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2063 ; accuracy: 0.7333333333333333; loss: 2.0336790084838867\n",
      "Training epoch 2064 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2064 ; accuracy: 0.7333333333333333; loss: 2.0333855152130127\n",
      "Training epoch 2065 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2065 ; accuracy: 0.7333333333333333; loss: 2.033099412918091\n",
      "Training epoch 2066 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2066 ; accuracy: 0.7333333333333333; loss: 2.032822608947754\n",
      "Training epoch 2067 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2067 ; accuracy: 0.7333333333333333; loss: 2.032540798187256\n",
      "Training epoch 2068 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2068 ; accuracy: 0.7333333333333333; loss: 2.0322701930999756\n",
      "Training epoch 2069 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2069 ; accuracy: 0.7333333333333333; loss: 2.0320205688476562\n",
      "Training epoch 2070 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2070 ; accuracy: 0.7333333333333333; loss: 2.0317773818969727\n",
      "Training epoch 2071 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2071 ; accuracy: 0.7333333333333333; loss: 2.031533718109131\n",
      "Training epoch 2072 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2072 ; accuracy: 0.7333333333333333; loss: 2.0313007831573486\n",
      "Training epoch 2073 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2073 ; accuracy: 0.7333333333333333; loss: 2.0310704708099365\n",
      "Training epoch 2074 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2074 ; accuracy: 0.7333333333333333; loss: 2.030841827392578\n",
      "Training epoch 2075 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2075 ; accuracy: 0.7333333333333333; loss: 2.03061842918396\n",
      "Training epoch 2076 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2076 ; accuracy: 0.7333333333333333; loss: 2.0303871631622314\n",
      "Training epoch 2077 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2077 ; accuracy: 0.7333333333333333; loss: 2.0301589965820312\n",
      "Training epoch 2078 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2078 ; accuracy: 0.7333333333333333; loss: 2.0299267768859863\n",
      "Training epoch 2079 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2079 ; accuracy: 0.7333333333333333; loss: 2.0297136306762695\n",
      "Training epoch 2080 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2080 ; accuracy: 0.7333333333333333; loss: 2.0295052528381348\n",
      "Training epoch 2081 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2081 ; accuracy: 0.7333333333333333; loss: 2.0293052196502686\n",
      "Training epoch 2082 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2082 ; accuracy: 0.7333333333333333; loss: 2.029146432876587\n",
      "Training epoch 2083 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2083 ; accuracy: 0.7333333333333333; loss: 2.0289855003356934\n",
      "Training epoch 2084 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2084 ; accuracy: 0.7333333333333333; loss: 2.0288379192352295\n",
      "Training epoch 2085 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2085 ; accuracy: 0.7333333333333333; loss: 2.028684377670288\n",
      "Training epoch 2086 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2086 ; accuracy: 0.7333333333333333; loss: 2.028536319732666\n",
      "Training epoch 2087 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2087 ; accuracy: 0.7333333333333333; loss: 2.0283989906311035\n",
      "Training epoch 2088 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2088 ; accuracy: 0.7333333333333333; loss: 2.0282652378082275\n",
      "Training epoch 2089 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2089 ; accuracy: 0.7333333333333333; loss: 2.028135061264038\n",
      "Training epoch 2090 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 2090 ; accuracy: 0.7333333333333333; loss: 2.0280587673187256\n",
      "Training epoch 2091 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2091 ; accuracy: 0.7333333333333333; loss: 2.027987241744995\n",
      "Training epoch 2092 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2092 ; accuracy: 0.7333333333333333; loss: 2.027911901473999\n",
      "Training epoch 2093 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2093 ; accuracy: 0.7333333333333333; loss: 2.0278217792510986\n",
      "Training epoch 2094 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 2094 ; accuracy: 0.7333333333333333; loss: 2.0277445316314697\n",
      "Training epoch 2095 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2095 ; accuracy: 0.7333333333333333; loss: 2.0276637077331543\n",
      "Training epoch 2096 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2096 ; accuracy: 0.7333333333333333; loss: 2.0275886058807373\n",
      "Training epoch 2097 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2097 ; accuracy: 0.7333333333333333; loss: 2.027517795562744\n",
      "Training epoch 2098 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2098 ; accuracy: 0.7333333333333333; loss: 2.0274457931518555\n",
      "Training epoch 2099 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 2099 ; accuracy: 0.7333333333333333; loss: 2.027459144592285\n",
      "Training epoch 2100 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2100 ; accuracy: 0.7333333333333333; loss: 2.0274524688720703\n",
      "Training epoch 2101 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2101 ; accuracy: 0.7333333333333333; loss: 2.02744722366333\n",
      "Training epoch 2102 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2102 ; accuracy: 0.7333333333333333; loss: 2.0274393558502197\n",
      "Training epoch 2103 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2103 ; accuracy: 0.7333333333333333; loss: 2.027427911758423\n",
      "Training epoch 2104 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2104 ; accuracy: 0.7333333333333333; loss: 2.0274064540863037\n",
      "Training epoch 2105 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2105 ; accuracy: 0.7333333333333333; loss: 2.0273728370666504\n",
      "Training epoch 2106 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2106 ; accuracy: 0.7333333333333333; loss: 2.027329683303833\n",
      "Training epoch 2107 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2107 ; accuracy: 0.7333333333333333; loss: 2.0272786617279053\n",
      "Training epoch 2108 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2108 ; accuracy: 0.7333333333333333; loss: 2.0272154808044434\n",
      "Training epoch 2109 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2109 ; accuracy: 0.7333333333333333; loss: 2.027151584625244\n",
      "Training epoch 2110 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2110 ; accuracy: 0.7333333333333333; loss: 2.027066469192505\n",
      "Training epoch 2111 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2111 ; accuracy: 0.7333333333333333; loss: 2.026986598968506\n",
      "Training epoch 2112 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2112 ; accuracy: 0.7333333333333333; loss: 2.0269110202789307\n",
      "Training epoch 2113 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2113 ; accuracy: 0.7333333333333333; loss: 2.026834726333618\n",
      "Training epoch 2114 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2114 ; accuracy: 0.7333333333333333; loss: 2.026757001876831\n",
      "Training epoch 2115 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2115 ; accuracy: 0.7333333333333333; loss: 2.0266757011413574\n",
      "Training epoch 2116 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2116 ; accuracy: 0.7333333333333333; loss: 2.026590347290039\n",
      "Training epoch 2117 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2117 ; accuracy: 0.7333333333333333; loss: 2.0265085697174072\n",
      "Training epoch 2118 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2118 ; accuracy: 0.7333333333333333; loss: 2.0264432430267334\n",
      "Training epoch 2119 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2119 ; accuracy: 0.7333333333333333; loss: 2.0263826847076416\n",
      "Training epoch 2120 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2120 ; accuracy: 0.7333333333333333; loss: 2.026308298110962\n",
      "Training epoch 2121 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2121 ; accuracy: 0.7333333333333333; loss: 2.0262346267700195\n",
      "Training epoch 2122 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2122 ; accuracy: 0.7333333333333333; loss: 2.0261707305908203\n",
      "Training epoch 2123 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2123 ; accuracy: 0.7333333333333333; loss: 2.0260987281799316\n",
      "Training epoch 2124 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2124 ; accuracy: 0.7333333333333333; loss: 2.0260090827941895\n",
      "Training epoch 2125 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2125 ; accuracy: 0.7333333333333333; loss: 2.025928497314453\n",
      "Training epoch 2126 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2126 ; accuracy: 0.7333333333333333; loss: 2.0258255004882812\n",
      "Training epoch 2127 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2127 ; accuracy: 0.7333333333333333; loss: 2.025721549987793\n",
      "Training epoch 2128 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2128 ; accuracy: 0.7366666666666667; loss: 2.0256032943725586\n",
      "Training epoch 2129 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2129 ; accuracy: 0.7366666666666667; loss: 2.0254905223846436\n",
      "Training epoch 2130 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2130 ; accuracy: 0.7366666666666667; loss: 2.0253689289093018\n",
      "Training epoch 2131 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2131 ; accuracy: 0.7366666666666667; loss: 2.025235414505005\n",
      "Training epoch 2132 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2132 ; accuracy: 0.7366666666666667; loss: 2.0251097679138184\n",
      "Training epoch 2133 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2133 ; accuracy: 0.7366666666666667; loss: 2.024986505508423\n",
      "Training epoch 2134 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2134 ; accuracy: 0.7366666666666667; loss: 2.0248780250549316\n",
      "Training epoch 2135 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2135 ; accuracy: 0.7366666666666667; loss: 2.0247766971588135\n",
      "Training epoch 2136 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2136 ; accuracy: 0.7366666666666667; loss: 2.0246829986572266\n",
      "Training epoch 2137 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2137 ; accuracy: 0.7366666666666667; loss: 2.0246055126190186\n",
      "Training epoch 2138 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2138 ; accuracy: 0.7366666666666667; loss: 2.024538278579712\n",
      "Training epoch 2139 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2139 ; accuracy: 0.7366666666666667; loss: 2.0244710445404053\n",
      "Training epoch 2140 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2140 ; accuracy: 0.7366666666666667; loss: 2.024404287338257\n",
      "Training epoch 2141 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2141 ; accuracy: 0.7366666666666667; loss: 2.0243425369262695\n",
      "Training epoch 2142 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2142 ; accuracy: 0.7366666666666667; loss: 2.024282932281494\n",
      "Training epoch 2143 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2143 ; accuracy: 0.7366666666666667; loss: 2.0242128372192383\n",
      "Training epoch 2144 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2144 ; accuracy: 0.7366666666666667; loss: 2.024146556854248\n",
      "Training epoch 2145 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2145 ; accuracy: 0.7366666666666667; loss: 2.02408766746521\n",
      "Training epoch 2146 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2146 ; accuracy: 0.7366666666666667; loss: 2.024040460586548\n",
      "Training epoch 2147 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2147 ; accuracy: 0.74; loss: 2.0240097045898438\n",
      "Training epoch 2148 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2148 ; accuracy: 0.74; loss: 2.0239639282226562\n",
      "Training epoch 2149 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2149 ; accuracy: 0.74; loss: 2.0239219665527344\n",
      "Training epoch 2150 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2150 ; accuracy: 0.74; loss: 2.0238759517669678\n",
      "Training epoch 2151 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2151 ; accuracy: 0.74; loss: 2.0238304138183594\n",
      "Training epoch 2152 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2152 ; accuracy: 0.74; loss: 2.0237977504730225\n",
      "Training epoch 2153 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2153 ; accuracy: 0.74; loss: 2.0237555503845215\n",
      "Training epoch 2154 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2154 ; accuracy: 0.74; loss: 2.0237138271331787\n",
      "Training epoch 2155 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2155 ; accuracy: 0.74; loss: 2.0236642360687256\n",
      "Training epoch 2156 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2156 ; accuracy: 0.74; loss: 2.023613214492798\n",
      "Training epoch 2157 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2157 ; accuracy: 0.74; loss: 2.0235631465911865\n",
      "Training epoch 2158 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2158 ; accuracy: 0.74; loss: 2.0235180854797363\n",
      "Training epoch 2159 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2159 ; accuracy: 0.74; loss: 2.02347469329834\n",
      "Training epoch 2160 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2160 ; accuracy: 0.74; loss: 2.0234367847442627\n",
      "Training epoch 2161 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2161 ; accuracy: 0.74; loss: 2.0233936309814453\n",
      "Training epoch 2162 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2162 ; accuracy: 0.74; loss: 2.0233492851257324\n",
      "Training epoch 2163 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2163 ; accuracy: 0.74; loss: 2.023310661315918\n",
      "Training epoch 2164 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2164 ; accuracy: 0.74; loss: 2.023277997970581\n",
      "Training epoch 2165 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2165 ; accuracy: 0.74; loss: 2.0232632160186768\n",
      "Training epoch 2166 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2166 ; accuracy: 0.74; loss: 2.0232467651367188\n",
      "Training epoch 2167 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2167 ; accuracy: 0.74; loss: 2.023242950439453\n",
      "Training epoch 2168 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2168 ; accuracy: 0.74; loss: 2.02323317527771\n",
      "Training epoch 2169 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2169 ; accuracy: 0.74; loss: 2.0232138633728027\n",
      "Training epoch 2170 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2170 ; accuracy: 0.74; loss: 2.023200750350952\n",
      "Training epoch 2171 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2171 ; accuracy: 0.74; loss: 2.0231988430023193\n",
      "Training epoch 2172 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2172 ; accuracy: 0.74; loss: 2.0231945514678955\n",
      "Training epoch 2173 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2173 ; accuracy: 0.74; loss: 2.023183584213257\n",
      "Training epoch 2174 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2174 ; accuracy: 0.74; loss: 2.023176670074463\n",
      "Training epoch 2175 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2175 ; accuracy: 0.74; loss: 2.0231664180755615\n",
      "Training epoch 2176 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2176 ; accuracy: 0.74; loss: 2.023171901702881\n",
      "Training epoch 2177 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2177 ; accuracy: 0.74; loss: 2.0231831073760986\n",
      "Training epoch 2178 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2178 ; accuracy: 0.74; loss: 2.023200035095215\n",
      "Training epoch 2179 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2179 ; accuracy: 0.74; loss: 2.0231966972351074\n",
      "Training epoch 2180 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2180 ; accuracy: 0.74; loss: 2.0231897830963135\n",
      "Training epoch 2181 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2181 ; accuracy: 0.74; loss: 2.0231828689575195\n",
      "Training epoch 2182 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2182 ; accuracy: 0.74; loss: 2.0231785774230957\n",
      "Training epoch 2183 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2183 ; accuracy: 0.74; loss: 2.023172616958618\n",
      "Training epoch 2184 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2184 ; accuracy: 0.74; loss: 2.023169755935669\n",
      "Training epoch 2185 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2185 ; accuracy: 0.74; loss: 2.023167133331299\n",
      "Training epoch 2186 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2186 ; accuracy: 0.74; loss: 2.023162841796875\n",
      "Training epoch 2187 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2187 ; accuracy: 0.74; loss: 2.0231704711914062\n",
      "Training epoch 2188 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2188 ; accuracy: 0.74; loss: 2.023174524307251\n",
      "Training epoch 2189 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2189 ; accuracy: 0.74; loss: 2.0231831073760986\n",
      "Training epoch 2190 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2190 ; accuracy: 0.74; loss: 2.0231776237487793\n",
      "Training epoch 2191 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2191 ; accuracy: 0.74; loss: 2.02315092086792\n",
      "Training epoch 2192 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2192 ; accuracy: 0.74; loss: 2.023123264312744\n",
      "Training epoch 2193 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2193 ; accuracy: 0.74; loss: 2.023090362548828\n",
      "Training epoch 2194 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2194 ; accuracy: 0.74; loss: 2.023055076599121\n",
      "Training epoch 2195 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2195 ; accuracy: 0.74; loss: 2.023010492324829\n",
      "Training epoch 2196 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2196 ; accuracy: 0.74; loss: 2.0229647159576416\n",
      "Training epoch 2197 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2197 ; accuracy: 0.74; loss: 2.022923231124878\n",
      "Training epoch 2198 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2198 ; accuracy: 0.74; loss: 2.0228872299194336\n",
      "Training epoch 2199 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2199 ; accuracy: 0.74; loss: 2.0228421688079834\n",
      "Training epoch 2200 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2200 ; accuracy: 0.74; loss: 2.0228018760681152\n",
      "Training epoch 2201 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2201 ; accuracy: 0.74; loss: 2.0227694511413574\n",
      "Training epoch 2202 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2202 ; accuracy: 0.74; loss: 2.0227394104003906\n",
      "Training epoch 2203 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2203 ; accuracy: 0.74; loss: 2.022709369659424\n",
      "Training epoch 2204 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2204 ; accuracy: 0.74; loss: 2.0226807594299316\n",
      "Training epoch 2205 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2205 ; accuracy: 0.74; loss: 2.0226640701293945\n",
      "Training epoch 2206 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2206 ; accuracy: 0.74; loss: 2.022650957107544\n",
      "Training epoch 2207 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2207 ; accuracy: 0.74; loss: 2.0226423740386963\n",
      "Training epoch 2208 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2208 ; accuracy: 0.74; loss: 2.022636651992798\n",
      "Training epoch 2209 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2209 ; accuracy: 0.74; loss: 2.0226221084594727\n",
      "Training epoch 2210 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2210 ; accuracy: 0.74; loss: 2.022615909576416\n",
      "Training epoch 2211 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2211 ; accuracy: 0.74; loss: 2.0226128101348877\n",
      "Training epoch 2212 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2212 ; accuracy: 0.74; loss: 2.022599220275879\n",
      "Training epoch 2213 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2213 ; accuracy: 0.74; loss: 2.022592782974243\n",
      "Training epoch 2214 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2214 ; accuracy: 0.74; loss: 2.0225915908813477\n",
      "Training epoch 2215 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2215 ; accuracy: 0.74; loss: 2.022597074508667\n",
      "Training epoch 2216 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2216 ; accuracy: 0.74; loss: 2.0225985050201416\n",
      "Training epoch 2217 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2217 ; accuracy: 0.74; loss: 2.022597551345825\n",
      "Training epoch 2218 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2218 ; accuracy: 0.74; loss: 2.0225989818573\n",
      "Training epoch 2219 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2219 ; accuracy: 0.74; loss: 2.022587776184082\n",
      "Training epoch 2220 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2220 ; accuracy: 0.74; loss: 2.0225718021392822\n",
      "Training epoch 2221 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2221 ; accuracy: 0.74; loss: 2.0225577354431152\n",
      "Training epoch 2222 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2222 ; accuracy: 0.74; loss: 2.0225517749786377\n",
      "Training epoch 2223 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2223 ; accuracy: 0.74; loss: 2.0225508213043213\n",
      "Training epoch 2224 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2224 ; accuracy: 0.74; loss: 2.0225470066070557\n",
      "Training epoch 2225 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2225 ; accuracy: 0.74; loss: 2.0225324630737305\n",
      "Training epoch 2226 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2226 ; accuracy: 0.74; loss: 2.022517204284668\n",
      "Training epoch 2227 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2227 ; accuracy: 0.74; loss: 2.0224976539611816\n",
      "Training epoch 2228 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2228 ; accuracy: 0.74; loss: 2.022465229034424\n",
      "Training epoch 2229 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2229 ; accuracy: 0.74; loss: 2.022437572479248\n",
      "Training epoch 2230 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2230 ; accuracy: 0.74; loss: 2.02241849899292\n",
      "Training epoch 2231 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2231 ; accuracy: 0.74; loss: 2.022402048110962\n",
      "Training epoch 2232 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2232 ; accuracy: 0.74; loss: 2.022390127182007\n",
      "Training epoch 2233 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2233 ; accuracy: 0.74; loss: 2.0223851203918457\n",
      "Training epoch 2234 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2234 ; accuracy: 0.74; loss: 2.022379159927368\n",
      "Training epoch 2235 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2235 ; accuracy: 0.74; loss: 2.0223801136016846\n",
      "Training epoch 2236 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2236 ; accuracy: 0.74; loss: 2.0223867893218994\n",
      "Training epoch 2237 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2237 ; accuracy: 0.74; loss: 2.0223872661590576\n",
      "Training epoch 2238 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2238 ; accuracy: 0.74; loss: 2.0223922729492188\n",
      "Training epoch 2239 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2239 ; accuracy: 0.74; loss: 2.022397756576538\n",
      "Training epoch 2240 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2240 ; accuracy: 0.74; loss: 2.022413730621338\n",
      "Training epoch 2241 ; accuracy: 0.9; loss: 0.19472363591194153\n",
      "Validation epoch 2241 ; accuracy: 0.7333333333333333; loss: 2.0055480003356934\n",
      "Training epoch 2242 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2242 ; accuracy: 0.73; loss: 1.9984238147735596\n",
      "Training epoch 2243 ; accuracy: 0.9; loss: 0.19459187984466553\n",
      "Validation epoch 2243 ; accuracy: 0.73; loss: 1.9953397512435913\n",
      "Training epoch 2244 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2244 ; accuracy: 0.7266666666666667; loss: 1.9971235990524292\n",
      "Training epoch 2245 ; accuracy: 0.9; loss: 0.19459299743175507\n",
      "Validation epoch 2245 ; accuracy: 0.7233333333333334; loss: 2.00118088722229\n",
      "Training epoch 2246 ; accuracy: 0.9; loss: 0.1945945769548416\n",
      "Validation epoch 2246 ; accuracy: 0.7166666666666667; loss: 2.0056469440460205\n",
      "Training epoch 2247 ; accuracy: 0.9; loss: 0.19459855556488037\n",
      "Validation epoch 2247 ; accuracy: 0.7166666666666667; loss: 2.008549451828003\n",
      "Training epoch 2248 ; accuracy: 0.9; loss: 0.1946014016866684\n",
      "Validation epoch 2248 ; accuracy: 0.7166666666666667; loss: 2.010542631149292\n",
      "Training epoch 2249 ; accuracy: 0.9; loss: 0.19460967183113098\n",
      "Validation epoch 2249 ; accuracy: 0.7166666666666667; loss: 2.008402109146118\n",
      "Training epoch 2250 ; accuracy: 0.9; loss: 0.19461071491241455\n",
      "Validation epoch 2250 ; accuracy: 0.7166666666666667; loss: 2.003523588180542\n",
      "Training epoch 2251 ; accuracy: 0.9; loss: 0.19460925459861755\n",
      "Validation epoch 2251 ; accuracy: 0.7166666666666667; loss: 1.9978786706924438\n",
      "Training epoch 2252 ; accuracy: 0.9; loss: 0.19463405013084412\n",
      "Validation epoch 2252 ; accuracy: 0.7166666666666667; loss: 1.9859631061553955\n",
      "Training epoch 2253 ; accuracy: 0.9; loss: 0.19460195302963257\n",
      "Validation epoch 2253 ; accuracy: 0.7166666666666667; loss: 1.9749916791915894\n",
      "Training epoch 2254 ; accuracy: 0.9; loss: 0.1946057826280594\n",
      "Validation epoch 2254 ; accuracy: 0.7166666666666667; loss: 1.9649511575698853\n",
      "Training epoch 2255 ; accuracy: 0.9; loss: 0.19460539519786835\n",
      "Validation epoch 2255 ; accuracy: 0.7166666666666667; loss: 1.9560538530349731\n",
      "Training epoch 2256 ; accuracy: 0.9; loss: 0.1946035623550415\n",
      "Validation epoch 2256 ; accuracy: 0.7166666666666667; loss: 1.9483674764633179\n",
      "Training epoch 2257 ; accuracy: 0.9; loss: 0.19459977746009827\n",
      "Validation epoch 2257 ; accuracy: 0.72; loss: 1.9414430856704712\n",
      "Training epoch 2258 ; accuracy: 0.9; loss: 0.19460155069828033\n",
      "Validation epoch 2258 ; accuracy: 0.72; loss: 1.9352670907974243\n",
      "Training epoch 2259 ; accuracy: 0.9; loss: 0.19459852576255798\n",
      "Validation epoch 2259 ; accuracy: 0.7233333333333334; loss: 1.9299367666244507\n",
      "Training epoch 2260 ; accuracy: 0.9; loss: 0.19459640979766846\n",
      "Validation epoch 2260 ; accuracy: 0.7233333333333334; loss: 1.9256750345230103\n",
      "Training epoch 2261 ; accuracy: 0.9; loss: 0.19459421932697296\n",
      "Validation epoch 2261 ; accuracy: 0.7266666666666667; loss: 1.922135591506958\n",
      "Training epoch 2262 ; accuracy: 0.9; loss: 0.19459466636180878\n",
      "Validation epoch 2262 ; accuracy: 0.7233333333333334; loss: 1.9191174507141113\n",
      "Training epoch 2263 ; accuracy: 0.9; loss: 0.19459408521652222\n",
      "Validation epoch 2263 ; accuracy: 0.7233333333333334; loss: 1.916659951210022\n",
      "Training epoch 2264 ; accuracy: 0.9; loss: 0.19459302723407745\n",
      "Validation epoch 2264 ; accuracy: 0.7233333333333334; loss: 1.9147770404815674\n",
      "Training epoch 2265 ; accuracy: 0.9; loss: 0.1945926547050476\n",
      "Validation epoch 2265 ; accuracy: 0.7233333333333334; loss: 1.9133776426315308\n",
      "Training epoch 2266 ; accuracy: 0.9; loss: 0.19459226727485657\n",
      "Validation epoch 2266 ; accuracy: 0.7233333333333334; loss: 1.9123573303222656\n",
      "Training epoch 2267 ; accuracy: 0.9; loss: 0.1945926398038864\n",
      "Validation epoch 2267 ; accuracy: 0.7266666666666667; loss: 1.911657452583313\n",
      "Training epoch 2268 ; accuracy: 0.9; loss: 0.19459249079227448\n",
      "Validation epoch 2268 ; accuracy: 0.7266666666666667; loss: 1.9112329483032227\n",
      "Training epoch 2269 ; accuracy: 0.9; loss: 0.19459214806556702\n",
      "Validation epoch 2269 ; accuracy: 0.73; loss: 1.9110223054885864\n",
      "Training epoch 2270 ; accuracy: 0.9; loss: 0.19459229707717896\n",
      "Validation epoch 2270 ; accuracy: 0.73; loss: 1.9106923341751099\n",
      "Training epoch 2271 ; accuracy: 0.9; loss: 0.19459205865859985\n",
      "Validation epoch 2271 ; accuracy: 0.7333333333333333; loss: 1.910539150238037\n",
      "Training epoch 2272 ; accuracy: 0.9; loss: 0.1945919394493103\n",
      "Validation epoch 2272 ; accuracy: 0.7333333333333333; loss: 1.9105342626571655\n",
      "Training epoch 2273 ; accuracy: 0.9; loss: 0.19459186494350433\n",
      "Validation epoch 2273 ; accuracy: 0.73; loss: 1.9106390476226807\n",
      "Training epoch 2274 ; accuracy: 0.9; loss: 0.19459201395511627\n",
      "Validation epoch 2274 ; accuracy: 0.73; loss: 1.910854458808899\n",
      "Training epoch 2275 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 2275 ; accuracy: 0.73; loss: 1.9111512899398804\n",
      "Training epoch 2276 ; accuracy: 0.9; loss: 0.19459164142608643\n",
      "Validation epoch 2276 ; accuracy: 0.7333333333333333; loss: 1.9114679098129272\n",
      "Training epoch 2277 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 2277 ; accuracy: 0.7333333333333333; loss: 1.9117408990859985\n",
      "Training epoch 2278 ; accuracy: 0.9; loss: 0.19459162652492523\n",
      "Validation epoch 2278 ; accuracy: 0.7333333333333333; loss: 1.9120512008666992\n",
      "Training epoch 2279 ; accuracy: 0.9; loss: 0.1945919543504715\n",
      "Validation epoch 2279 ; accuracy: 0.7333333333333333; loss: 1.9123891592025757\n",
      "Training epoch 2280 ; accuracy: 0.9; loss: 0.19459176063537598\n",
      "Validation epoch 2280 ; accuracy: 0.7333333333333333; loss: 1.9127721786499023\n",
      "Training epoch 2281 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2281 ; accuracy: 0.7333333333333333; loss: 1.913171648979187\n",
      "Training epoch 2282 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 2282 ; accuracy: 0.7333333333333333; loss: 1.9135940074920654\n",
      "Training epoch 2283 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 2283 ; accuracy: 0.7333333333333333; loss: 1.914029598236084\n",
      "Training epoch 2284 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 2284 ; accuracy: 0.7333333333333333; loss: 1.9144575595855713\n",
      "Training epoch 2285 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2285 ; accuracy: 0.7333333333333333; loss: 1.9148763418197632\n",
      "Training epoch 2286 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2286 ; accuracy: 0.7333333333333333; loss: 1.9153037071228027\n",
      "Training epoch 2287 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2287 ; accuracy: 0.7333333333333333; loss: 1.9157294034957886\n",
      "Training epoch 2288 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 2288 ; accuracy: 0.7333333333333333; loss: 1.9161512851715088\n",
      "Training epoch 2289 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2289 ; accuracy: 0.7333333333333333; loss: 1.91655433177948\n",
      "Training epoch 2290 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2290 ; accuracy: 0.7333333333333333; loss: 1.9169559478759766\n",
      "Training epoch 2291 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 2291 ; accuracy: 0.7333333333333333; loss: 1.9173537492752075\n",
      "Training epoch 2292 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2292 ; accuracy: 0.7333333333333333; loss: 1.917755126953125\n",
      "Training epoch 2293 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2293 ; accuracy: 0.7333333333333333; loss: 1.9181610345840454\n",
      "Training epoch 2294 ; accuracy: 0.9; loss: 0.19459179043769836\n",
      "Validation epoch 2294 ; accuracy: 0.7333333333333333; loss: 1.918579339981079\n",
      "Training epoch 2295 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 2295 ; accuracy: 0.7333333333333333; loss: 1.9189904928207397\n",
      "Training epoch 2296 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2296 ; accuracy: 0.7333333333333333; loss: 1.9193941354751587\n",
      "Training epoch 2297 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2297 ; accuracy: 0.7333333333333333; loss: 1.9197776317596436\n",
      "Training epoch 2298 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 2298 ; accuracy: 0.7333333333333333; loss: 1.92015540599823\n",
      "Training epoch 2299 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2299 ; accuracy: 0.7333333333333333; loss: 1.9205137491226196\n",
      "Training epoch 2300 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 2300 ; accuracy: 0.7333333333333333; loss: 1.9208762645721436\n",
      "Training epoch 2301 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2301 ; accuracy: 0.7333333333333333; loss: 1.9212430715560913\n",
      "Training epoch 2302 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2302 ; accuracy: 0.7333333333333333; loss: 1.9215980768203735\n",
      "Training epoch 2303 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2303 ; accuracy: 0.7333333333333333; loss: 1.9219543933868408\n",
      "Training epoch 2304 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2304 ; accuracy: 0.7333333333333333; loss: 1.9222846031188965\n",
      "Training epoch 2305 ; accuracy: 0.9; loss: 0.19459152221679688\n",
      "Validation epoch 2305 ; accuracy: 0.7333333333333333; loss: 1.9226057529449463\n",
      "Training epoch 2306 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2306 ; accuracy: 0.7333333333333333; loss: 1.922919511795044\n",
      "Training epoch 2307 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2307 ; accuracy: 0.7333333333333333; loss: 1.9232310056686401\n",
      "Training epoch 2308 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2308 ; accuracy: 0.7333333333333333; loss: 1.923536777496338\n",
      "Training epoch 2309 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2309 ; accuracy: 0.7333333333333333; loss: 1.9238439798355103\n",
      "Training epoch 2310 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2310 ; accuracy: 0.7333333333333333; loss: 1.9241621494293213\n",
      "Training epoch 2311 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2311 ; accuracy: 0.7333333333333333; loss: 1.9244698286056519\n",
      "Training epoch 2312 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 2312 ; accuracy: 0.7333333333333333; loss: 1.924781084060669\n",
      "Training epoch 2313 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2313 ; accuracy: 0.7333333333333333; loss: 1.9250870943069458\n",
      "Training epoch 2314 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2314 ; accuracy: 0.7333333333333333; loss: 1.9253814220428467\n",
      "Training epoch 2315 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2315 ; accuracy: 0.7333333333333333; loss: 1.9256744384765625\n",
      "Training epoch 2316 ; accuracy: 0.9; loss: 0.1945917159318924\n",
      "Validation epoch 2316 ; accuracy: 0.7333333333333333; loss: 1.926008701324463\n",
      "Training epoch 2317 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2317 ; accuracy: 0.7333333333333333; loss: 1.926351547241211\n",
      "Training epoch 2318 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2318 ; accuracy: 0.7333333333333333; loss: 1.926690697669983\n",
      "Training epoch 2319 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2319 ; accuracy: 0.7333333333333333; loss: 1.9270203113555908\n",
      "Training epoch 2320 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 2320 ; accuracy: 0.7333333333333333; loss: 1.9273685216903687\n",
      "Training epoch 2321 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2321 ; accuracy: 0.7333333333333333; loss: 1.9277104139328003\n",
      "Training epoch 2322 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2322 ; accuracy: 0.7333333333333333; loss: 1.9280363321304321\n",
      "Training epoch 2323 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2323 ; accuracy: 0.7333333333333333; loss: 1.9283701181411743\n",
      "Training epoch 2324 ; accuracy: 0.9; loss: 0.1945917010307312\n",
      "Validation epoch 2324 ; accuracy: 0.7333333333333333; loss: 1.9287569522857666\n",
      "Training epoch 2325 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2325 ; accuracy: 0.7333333333333333; loss: 1.929131269454956\n",
      "Training epoch 2326 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2326 ; accuracy: 0.7333333333333333; loss: 1.9294942617416382\n",
      "Training epoch 2327 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2327 ; accuracy: 0.7333333333333333; loss: 1.9298596382141113\n",
      "Training epoch 2328 ; accuracy: 0.9; loss: 0.1945914924144745\n",
      "Validation epoch 2328 ; accuracy: 0.7333333333333333; loss: 1.9302195310592651\n",
      "Training epoch 2329 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2329 ; accuracy: 0.7333333333333333; loss: 1.9305696487426758\n",
      "Training epoch 2330 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2330 ; accuracy: 0.7333333333333333; loss: 1.930902123451233\n",
      "Training epoch 2331 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2331 ; accuracy: 0.7333333333333333; loss: 1.9312314987182617\n",
      "Training epoch 2332 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2332 ; accuracy: 0.7333333333333333; loss: 1.9315577745437622\n",
      "Training epoch 2333 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2333 ; accuracy: 0.7333333333333333; loss: 1.931878924369812\n",
      "Training epoch 2334 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2334 ; accuracy: 0.7333333333333333; loss: 1.932191014289856\n",
      "Training epoch 2335 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2335 ; accuracy: 0.7333333333333333; loss: 1.9325069189071655\n",
      "Training epoch 2336 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2336 ; accuracy: 0.7333333333333333; loss: 1.9328153133392334\n",
      "Training epoch 2337 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2337 ; accuracy: 0.7333333333333333; loss: 1.9331138134002686\n",
      "Training epoch 2338 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2338 ; accuracy: 0.7333333333333333; loss: 1.9334049224853516\n",
      "Training epoch 2339 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2339 ; accuracy: 0.7333333333333333; loss: 1.9336925745010376\n",
      "Training epoch 2340 ; accuracy: 0.9; loss: 0.19459158182144165\n",
      "Validation epoch 2340 ; accuracy: 0.7333333333333333; loss: 1.934000015258789\n",
      "Training epoch 2341 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2341 ; accuracy: 0.7333333333333333; loss: 1.9343029260635376\n",
      "Training epoch 2342 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2342 ; accuracy: 0.7333333333333333; loss: 1.934605360031128\n",
      "Training epoch 2343 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2343 ; accuracy: 0.7333333333333333; loss: 1.9349019527435303\n",
      "Training epoch 2344 ; accuracy: 0.9; loss: 0.1945919245481491\n",
      "Validation epoch 2344 ; accuracy: 0.7333333333333333; loss: 1.9352551698684692\n",
      "Training epoch 2345 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2345 ; accuracy: 0.7333333333333333; loss: 1.9355906248092651\n",
      "Training epoch 2346 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2346 ; accuracy: 0.7333333333333333; loss: 1.9359147548675537\n",
      "Training epoch 2347 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2347 ; accuracy: 0.7333333333333333; loss: 1.9362090826034546\n",
      "Training epoch 2348 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2348 ; accuracy: 0.7333333333333333; loss: 1.9364969730377197\n",
      "Training epoch 2349 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2349 ; accuracy: 0.7333333333333333; loss: 1.9367873668670654\n",
      "Training epoch 2350 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2350 ; accuracy: 0.7333333333333333; loss: 1.937073826789856\n",
      "Training epoch 2351 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2351 ; accuracy: 0.7333333333333333; loss: 1.9373583793640137\n",
      "Training epoch 2352 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2352 ; accuracy: 0.7333333333333333; loss: 1.9376355409622192\n",
      "Training epoch 2353 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2353 ; accuracy: 0.7333333333333333; loss: 1.937902808189392\n",
      "Training epoch 2354 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2354 ; accuracy: 0.7333333333333333; loss: 1.9381918907165527\n",
      "Training epoch 2355 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2355 ; accuracy: 0.7333333333333333; loss: 1.938477635383606\n",
      "Training epoch 2356 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2356 ; accuracy: 0.7333333333333333; loss: 1.9387552738189697\n",
      "Training epoch 2357 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2357 ; accuracy: 0.7333333333333333; loss: 1.939028263092041\n",
      "Training epoch 2358 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2358 ; accuracy: 0.7333333333333333; loss: 1.9393103122711182\n",
      "Training epoch 2359 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2359 ; accuracy: 0.7333333333333333; loss: 1.9395734071731567\n",
      "Training epoch 2360 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2360 ; accuracy: 0.7333333333333333; loss: 1.9398313760757446\n",
      "Training epoch 2361 ; accuracy: 0.9; loss: 0.1945914775133133\n",
      "Validation epoch 2361 ; accuracy: 0.7333333333333333; loss: 1.940066933631897\n",
      "Training epoch 2362 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2362 ; accuracy: 0.7333333333333333; loss: 1.940307855606079\n",
      "Training epoch 2363 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2363 ; accuracy: 0.7333333333333333; loss: 1.9405568838119507\n",
      "Training epoch 2364 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 2364 ; accuracy: 0.7333333333333333; loss: 1.9408079385757446\n",
      "Training epoch 2365 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2365 ; accuracy: 0.7333333333333333; loss: 1.9410587549209595\n",
      "Training epoch 2366 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2366 ; accuracy: 0.7333333333333333; loss: 1.9413092136383057\n",
      "Training epoch 2367 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2367 ; accuracy: 0.7333333333333333; loss: 1.9415464401245117\n",
      "Training epoch 2368 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2368 ; accuracy: 0.7333333333333333; loss: 1.94178307056427\n",
      "Training epoch 2369 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2369 ; accuracy: 0.7333333333333333; loss: 1.9419901371002197\n",
      "Training epoch 2370 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2370 ; accuracy: 0.7333333333333333; loss: 1.9421980381011963\n",
      "Training epoch 2371 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2371 ; accuracy: 0.7333333333333333; loss: 1.942417860031128\n",
      "Training epoch 2372 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2372 ; accuracy: 0.7333333333333333; loss: 1.942640781402588\n",
      "Training epoch 2373 ; accuracy: 0.9; loss: 0.19459161162376404\n",
      "Validation epoch 2373 ; accuracy: 0.7333333333333333; loss: 1.9429157972335815\n",
      "Training epoch 2374 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2374 ; accuracy: 0.7333333333333333; loss: 1.9431835412979126\n",
      "Training epoch 2375 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2375 ; accuracy: 0.7333333333333333; loss: 1.9434500932693481\n",
      "Training epoch 2376 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2376 ; accuracy: 0.7333333333333333; loss: 1.9437192678451538\n",
      "Training epoch 2377 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2377 ; accuracy: 0.7333333333333333; loss: 1.9439936876296997\n",
      "Training epoch 2378 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2378 ; accuracy: 0.7333333333333333; loss: 1.9442633390426636\n",
      "Training epoch 2379 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2379 ; accuracy: 0.7333333333333333; loss: 1.9445297718048096\n",
      "Training epoch 2380 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2380 ; accuracy: 0.7333333333333333; loss: 1.944811224937439\n",
      "Training epoch 2381 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2381 ; accuracy: 0.7333333333333333; loss: 1.945082187652588\n",
      "Training epoch 2382 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2382 ; accuracy: 0.7333333333333333; loss: 1.945357084274292\n",
      "Training epoch 2383 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2383 ; accuracy: 0.7333333333333333; loss: 1.9456415176391602\n",
      "Training epoch 2384 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2384 ; accuracy: 0.7333333333333333; loss: 1.9459196329116821\n",
      "Training epoch 2385 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2385 ; accuracy: 0.7333333333333333; loss: 1.9461885690689087\n",
      "Training epoch 2386 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2386 ; accuracy: 0.7333333333333333; loss: 1.9464569091796875\n",
      "Training epoch 2387 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2387 ; accuracy: 0.7333333333333333; loss: 1.9467185735702515\n",
      "Training epoch 2388 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2388 ; accuracy: 0.7333333333333333; loss: 1.946980595588684\n",
      "Training epoch 2389 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 2389 ; accuracy: 0.7333333333333333; loss: 1.9472509622573853\n",
      "Training epoch 2390 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2390 ; accuracy: 0.7333333333333333; loss: 1.9475150108337402\n",
      "Training epoch 2391 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2391 ; accuracy: 0.7333333333333333; loss: 1.947779893875122\n",
      "Training epoch 2392 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2392 ; accuracy: 0.7333333333333333; loss: 1.9480478763580322\n",
      "Training epoch 2393 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2393 ; accuracy: 0.7333333333333333; loss: 1.9483060836791992\n",
      "Training epoch 2394 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2394 ; accuracy: 0.7333333333333333; loss: 1.9485499858856201\n",
      "Training epoch 2395 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2395 ; accuracy: 0.7333333333333333; loss: 1.9487911462783813\n",
      "Training epoch 2396 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2396 ; accuracy: 0.7333333333333333; loss: 1.9490238428115845\n",
      "Training epoch 2397 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2397 ; accuracy: 0.7333333333333333; loss: 1.9492523670196533\n",
      "Training epoch 2398 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2398 ; accuracy: 0.7333333333333333; loss: 1.949483036994934\n",
      "Training epoch 2399 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2399 ; accuracy: 0.7333333333333333; loss: 1.9497127532958984\n",
      "Training epoch 2400 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2400 ; accuracy: 0.7333333333333333; loss: 1.9499393701553345\n",
      "Training epoch 2401 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2401 ; accuracy: 0.7333333333333333; loss: 1.9501676559448242\n",
      "Training epoch 2402 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2402 ; accuracy: 0.7333333333333333; loss: 1.9503979682922363\n",
      "Training epoch 2403 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2403 ; accuracy: 0.7333333333333333; loss: 1.9506314992904663\n",
      "Training epoch 2404 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2404 ; accuracy: 0.7333333333333333; loss: 1.950850009918213\n",
      "Training epoch 2405 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2405 ; accuracy: 0.7333333333333333; loss: 1.9510760307312012\n",
      "Training epoch 2406 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2406 ; accuracy: 0.7333333333333333; loss: 1.9513012170791626\n",
      "Training epoch 2407 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2407 ; accuracy: 0.7333333333333333; loss: 1.9515234231948853\n",
      "Training epoch 2408 ; accuracy: 0.9; loss: 0.19459138810634613\n",
      "Validation epoch 2408 ; accuracy: 0.7333333333333333; loss: 1.951711654663086\n",
      "Training epoch 2409 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2409 ; accuracy: 0.7333333333333333; loss: 1.9518985748291016\n",
      "Training epoch 2410 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2410 ; accuracy: 0.7333333333333333; loss: 1.9520885944366455\n",
      "Training epoch 2411 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2411 ; accuracy: 0.7333333333333333; loss: 1.9522745609283447\n",
      "Training epoch 2412 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2412 ; accuracy: 0.7333333333333333; loss: 1.952455997467041\n",
      "Training epoch 2413 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2413 ; accuracy: 0.7333333333333333; loss: 1.9526424407958984\n",
      "Training epoch 2414 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2414 ; accuracy: 0.7333333333333333; loss: 1.9528344869613647\n",
      "Training epoch 2415 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2415 ; accuracy: 0.7333333333333333; loss: 1.9530302286148071\n",
      "Training epoch 2416 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2416 ; accuracy: 0.7333333333333333; loss: 1.9532279968261719\n",
      "Training epoch 2417 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2417 ; accuracy: 0.7333333333333333; loss: 1.9534399509429932\n",
      "Training epoch 2418 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2418 ; accuracy: 0.7333333333333333; loss: 1.953654170036316\n",
      "Training epoch 2419 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 2419 ; accuracy: 0.7333333333333333; loss: 1.9538979530334473\n",
      "Training epoch 2420 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2420 ; accuracy: 0.7333333333333333; loss: 1.954142689704895\n",
      "Training epoch 2421 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 2421 ; accuracy: 0.7333333333333333; loss: 1.9543952941894531\n",
      "Training epoch 2422 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2422 ; accuracy: 0.7333333333333333; loss: 1.9546384811401367\n",
      "Training epoch 2423 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2423 ; accuracy: 0.7333333333333333; loss: 1.9548829793930054\n",
      "Training epoch 2424 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2424 ; accuracy: 0.7333333333333333; loss: 1.9551196098327637\n",
      "Training epoch 2425 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2425 ; accuracy: 0.7333333333333333; loss: 1.9553594589233398\n",
      "Training epoch 2426 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2426 ; accuracy: 0.7333333333333333; loss: 1.9556018114089966\n",
      "Training epoch 2427 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2427 ; accuracy: 0.7333333333333333; loss: 1.9558284282684326\n",
      "Training epoch 2428 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2428 ; accuracy: 0.7333333333333333; loss: 1.9560518264770508\n",
      "Training epoch 2429 ; accuracy: 0.9; loss: 0.19459135830402374\n",
      "Validation epoch 2429 ; accuracy: 0.7333333333333333; loss: 1.956278920173645\n",
      "Training epoch 2430 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2430 ; accuracy: 0.7333333333333333; loss: 1.9565030336380005\n",
      "Training epoch 2431 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2431 ; accuracy: 0.7333333333333333; loss: 1.9567151069641113\n",
      "Training epoch 2432 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2432 ; accuracy: 0.7333333333333333; loss: 1.9569382667541504\n",
      "Training epoch 2433 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2433 ; accuracy: 0.7333333333333333; loss: 1.9571661949157715\n",
      "Training epoch 2434 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2434 ; accuracy: 0.7333333333333333; loss: 1.957391381263733\n",
      "Training epoch 2435 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2435 ; accuracy: 0.7333333333333333; loss: 1.9576151371002197\n",
      "Training epoch 2436 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2436 ; accuracy: 0.7333333333333333; loss: 1.9578393697738647\n",
      "Training epoch 2437 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2437 ; accuracy: 0.7333333333333333; loss: 1.9580564498901367\n",
      "Training epoch 2438 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2438 ; accuracy: 0.7333333333333333; loss: 1.9582735300064087\n",
      "Training epoch 2439 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2439 ; accuracy: 0.7333333333333333; loss: 1.9584918022155762\n",
      "Training epoch 2440 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 2440 ; accuracy: 0.7333333333333333; loss: 1.9587500095367432\n",
      "Training epoch 2441 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2441 ; accuracy: 0.7333333333333333; loss: 1.9590083360671997\n",
      "Training epoch 2442 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2442 ; accuracy: 0.7333333333333333; loss: 1.959263563156128\n",
      "Training epoch 2443 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2443 ; accuracy: 0.7333333333333333; loss: 1.9595106840133667\n",
      "Training epoch 2444 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2444 ; accuracy: 0.7333333333333333; loss: 1.9597545862197876\n",
      "Training epoch 2445 ; accuracy: 0.9; loss: 0.19459141790866852\n",
      "Validation epoch 2445 ; accuracy: 0.7333333333333333; loss: 1.9600292444229126\n",
      "Training epoch 2446 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2446 ; accuracy: 0.7333333333333333; loss: 1.9602937698364258\n",
      "Training epoch 2447 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2447 ; accuracy: 0.7333333333333333; loss: 1.960544466972351\n",
      "Training epoch 2448 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2448 ; accuracy: 0.7333333333333333; loss: 1.9607857465744019\n",
      "Training epoch 2449 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2449 ; accuracy: 0.7333333333333333; loss: 1.961026668548584\n",
      "Training epoch 2450 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2450 ; accuracy: 0.7333333333333333; loss: 1.9612573385238647\n",
      "Training epoch 2451 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2451 ; accuracy: 0.7333333333333333; loss: 1.9614793062210083\n",
      "Training epoch 2452 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2452 ; accuracy: 0.73; loss: 1.9617071151733398\n",
      "Training epoch 2453 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2453 ; accuracy: 0.73; loss: 1.9619207382202148\n",
      "Training epoch 2454 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2454 ; accuracy: 0.73; loss: 1.9621315002441406\n",
      "Training epoch 2455 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2455 ; accuracy: 0.73; loss: 1.9623349905014038\n",
      "Training epoch 2456 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2456 ; accuracy: 0.73; loss: 1.9625437259674072\n",
      "Training epoch 2457 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2457 ; accuracy: 0.73; loss: 1.962747573852539\n",
      "Training epoch 2458 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2458 ; accuracy: 0.73; loss: 1.9629467725753784\n",
      "Training epoch 2459 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2459 ; accuracy: 0.73; loss: 1.9631459712982178\n",
      "Training epoch 2460 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2460 ; accuracy: 0.73; loss: 1.9633374214172363\n",
      "Training epoch 2461 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2461 ; accuracy: 0.73; loss: 1.9635239839553833\n",
      "Training epoch 2462 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2462 ; accuracy: 0.73; loss: 1.9637079238891602\n",
      "Training epoch 2463 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2463 ; accuracy: 0.73; loss: 1.9638938903808594\n",
      "Training epoch 2464 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2464 ; accuracy: 0.73; loss: 1.964076280593872\n",
      "Training epoch 2465 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2465 ; accuracy: 0.73; loss: 1.9642606973648071\n",
      "Training epoch 2466 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2466 ; accuracy: 0.73; loss: 1.9644439220428467\n",
      "Training epoch 2467 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2467 ; accuracy: 0.73; loss: 1.9646295309066772\n",
      "Training epoch 2468 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2468 ; accuracy: 0.73; loss: 1.9648100137710571\n",
      "Training epoch 2469 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2469 ; accuracy: 0.73; loss: 1.9649906158447266\n",
      "Training epoch 2470 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2470 ; accuracy: 0.73; loss: 1.9651676416397095\n",
      "Training epoch 2471 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2471 ; accuracy: 0.73; loss: 1.9653470516204834\n",
      "Training epoch 2472 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2472 ; accuracy: 0.73; loss: 1.9655286073684692\n",
      "Training epoch 2473 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2473 ; accuracy: 0.73; loss: 1.9657102823257446\n",
      "Training epoch 2474 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2474 ; accuracy: 0.73; loss: 1.9659037590026855\n",
      "Training epoch 2475 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2475 ; accuracy: 0.73; loss: 1.9660946130752563\n",
      "Training epoch 2476 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2476 ; accuracy: 0.73; loss: 1.9662861824035645\n",
      "Training epoch 2477 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2477 ; accuracy: 0.73; loss: 1.9664762020111084\n",
      "Training epoch 2478 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2478 ; accuracy: 0.73; loss: 1.9666601419448853\n",
      "Training epoch 2479 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2479 ; accuracy: 0.73; loss: 1.9668408632278442\n",
      "Training epoch 2480 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2480 ; accuracy: 0.73; loss: 1.9670119285583496\n",
      "Training epoch 2481 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2481 ; accuracy: 0.73; loss: 1.9671814441680908\n",
      "Training epoch 2482 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2482 ; accuracy: 0.73; loss: 1.967349886894226\n",
      "Training epoch 2483 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2483 ; accuracy: 0.73; loss: 1.9675252437591553\n",
      "Training epoch 2484 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2484 ; accuracy: 0.73; loss: 1.9676905870437622\n",
      "Training epoch 2485 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2485 ; accuracy: 0.73; loss: 1.9678707122802734\n",
      "Training epoch 2486 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2486 ; accuracy: 0.73; loss: 1.9680397510528564\n",
      "Training epoch 2487 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2487 ; accuracy: 0.73; loss: 1.9682196378707886\n",
      "Training epoch 2488 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2488 ; accuracy: 0.73; loss: 1.968401312828064\n",
      "Training epoch 2489 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2489 ; accuracy: 0.73; loss: 1.9685852527618408\n",
      "Training epoch 2490 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2490 ; accuracy: 0.73; loss: 1.9687646627426147\n",
      "Training epoch 2491 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2491 ; accuracy: 0.73; loss: 1.9689477682113647\n",
      "Training epoch 2492 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2492 ; accuracy: 0.73; loss: 1.9691253900527954\n",
      "Training epoch 2493 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2493 ; accuracy: 0.73; loss: 1.969306230545044\n",
      "Training epoch 2494 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2494 ; accuracy: 0.73; loss: 1.9694726467132568\n",
      "Training epoch 2495 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2495 ; accuracy: 0.73; loss: 1.969639539718628\n",
      "Training epoch 2496 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2496 ; accuracy: 0.73; loss: 1.969800591468811\n",
      "Training epoch 2497 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2497 ; accuracy: 0.73; loss: 1.9699554443359375\n",
      "Training epoch 2498 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2498 ; accuracy: 0.73; loss: 1.9701074361801147\n",
      "Training epoch 2499 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2499 ; accuracy: 0.73; loss: 1.9702597856521606\n",
      "Training epoch 2500 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2500 ; accuracy: 0.73; loss: 1.9704068899154663\n",
      "Training epoch 2501 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2501 ; accuracy: 0.73; loss: 1.9705578088760376\n",
      "Training epoch 2502 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2502 ; accuracy: 0.73; loss: 1.9707022905349731\n",
      "Training epoch 2503 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2503 ; accuracy: 0.73; loss: 1.9708573818206787\n",
      "Training epoch 2504 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2504 ; accuracy: 0.73; loss: 1.9710131883621216\n",
      "Training epoch 2505 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2505 ; accuracy: 0.73; loss: 1.9711710214614868\n",
      "Training epoch 2506 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2506 ; accuracy: 0.73; loss: 1.971330165863037\n",
      "Training epoch 2507 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2507 ; accuracy: 0.73; loss: 1.9714752435684204\n",
      "Training epoch 2508 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2508 ; accuracy: 0.73; loss: 1.9716218709945679\n",
      "Training epoch 2509 ; accuracy: 0.9; loss: 0.19459140300750732\n",
      "Validation epoch 2509 ; accuracy: 0.73; loss: 1.9717787504196167\n",
      "Training epoch 2510 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2510 ; accuracy: 0.73; loss: 1.9719336032867432\n",
      "Training epoch 2511 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2511 ; accuracy: 0.73; loss: 1.9720664024353027\n",
      "Training epoch 2512 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2512 ; accuracy: 0.73; loss: 1.9722168445587158\n",
      "Training epoch 2513 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2513 ; accuracy: 0.73; loss: 1.9723639488220215\n",
      "Training epoch 2514 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2514 ; accuracy: 0.73; loss: 1.9725370407104492\n",
      "Training epoch 2515 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2515 ; accuracy: 0.73; loss: 1.9727044105529785\n",
      "Training epoch 2516 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2516 ; accuracy: 0.73; loss: 1.9728745222091675\n",
      "Training epoch 2517 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2517 ; accuracy: 0.73; loss: 1.973042607307434\n",
      "Training epoch 2518 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2518 ; accuracy: 0.73; loss: 1.9731948375701904\n",
      "Training epoch 2519 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2519 ; accuracy: 0.73; loss: 1.9733532667160034\n",
      "Training epoch 2520 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2520 ; accuracy: 0.73; loss: 1.9735093116760254\n",
      "Training epoch 2521 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2521 ; accuracy: 0.73; loss: 1.973676323890686\n",
      "Training epoch 2522 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2522 ; accuracy: 0.73; loss: 1.9738447666168213\n",
      "Training epoch 2523 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2523 ; accuracy: 0.73; loss: 1.9740102291107178\n",
      "Training epoch 2524 ; accuracy: 0.9; loss: 0.19459134340286255\n",
      "Validation epoch 2524 ; accuracy: 0.73; loss: 1.9741770029067993\n",
      "Training epoch 2525 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2525 ; accuracy: 0.73; loss: 1.9743343591690063\n",
      "Training epoch 2526 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2526 ; accuracy: 0.73; loss: 1.9744688272476196\n",
      "Training epoch 2527 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2527 ; accuracy: 0.73; loss: 1.9746090173721313\n",
      "Training epoch 2528 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2528 ; accuracy: 0.73; loss: 1.9747493267059326\n",
      "Training epoch 2529 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2529 ; accuracy: 0.73; loss: 1.974885106086731\n",
      "Training epoch 2530 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2530 ; accuracy: 0.73; loss: 1.9750217199325562\n",
      "Training epoch 2531 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2531 ; accuracy: 0.73; loss: 1.9751635789871216\n",
      "Training epoch 2532 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2532 ; accuracy: 0.73; loss: 1.9753047227859497\n",
      "Training epoch 2533 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2533 ; accuracy: 0.73; loss: 1.9754503965377808\n",
      "Training epoch 2534 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 2534 ; accuracy: 0.73; loss: 1.975703477859497\n",
      "Training epoch 2535 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2535 ; accuracy: 0.73; loss: 1.9759421348571777\n",
      "Training epoch 2536 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2536 ; accuracy: 0.73; loss: 1.9761663675308228\n",
      "Training epoch 2537 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2537 ; accuracy: 0.73; loss: 1.9763779640197754\n",
      "Training epoch 2538 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2538 ; accuracy: 0.73; loss: 1.9765881299972534\n",
      "Training epoch 2539 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2539 ; accuracy: 0.73; loss: 1.9768025875091553\n",
      "Training epoch 2540 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2540 ; accuracy: 0.73; loss: 1.9770119190216064\n",
      "Training epoch 2541 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2541 ; accuracy: 0.73; loss: 1.9772272109985352\n",
      "Training epoch 2542 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2542 ; accuracy: 0.73; loss: 1.9774397611618042\n",
      "Training epoch 2543 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2543 ; accuracy: 0.73; loss: 1.9776482582092285\n",
      "Training epoch 2544 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2544 ; accuracy: 0.73; loss: 1.9778612852096558\n",
      "Training epoch 2545 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2545 ; accuracy: 0.73; loss: 1.9780664443969727\n",
      "Training epoch 2546 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2546 ; accuracy: 0.73; loss: 1.978258490562439\n",
      "Training epoch 2547 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2547 ; accuracy: 0.73; loss: 1.9784505367279053\n",
      "Training epoch 2548 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2548 ; accuracy: 0.73; loss: 1.9786415100097656\n",
      "Training epoch 2549 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2549 ; accuracy: 0.73; loss: 1.9788289070129395\n",
      "Training epoch 2550 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2550 ; accuracy: 0.73; loss: 1.9790208339691162\n",
      "Training epoch 2551 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2551 ; accuracy: 0.73; loss: 1.9792033433914185\n",
      "Training epoch 2552 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2552 ; accuracy: 0.73; loss: 1.9793782234191895\n",
      "Training epoch 2553 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2553 ; accuracy: 0.73; loss: 1.9795621633529663\n",
      "Training epoch 2554 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2554 ; accuracy: 0.73; loss: 1.9797444343566895\n",
      "Training epoch 2555 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2555 ; accuracy: 0.73; loss: 1.979915976524353\n",
      "Training epoch 2556 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2556 ; accuracy: 0.73; loss: 1.9800852537155151\n",
      "Training epoch 2557 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2557 ; accuracy: 0.73; loss: 1.9802398681640625\n",
      "Training epoch 2558 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2558 ; accuracy: 0.73; loss: 1.980386734008789\n",
      "Training epoch 2559 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2559 ; accuracy: 0.73; loss: 1.98053777217865\n",
      "Training epoch 2560 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2560 ; accuracy: 0.73; loss: 1.9806827306747437\n",
      "Training epoch 2561 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2561 ; accuracy: 0.73; loss: 1.9808361530303955\n",
      "Training epoch 2562 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2562 ; accuracy: 0.73; loss: 1.9809887409210205\n",
      "Training epoch 2563 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2563 ; accuracy: 0.73; loss: 1.9811413288116455\n",
      "Training epoch 2564 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2564 ; accuracy: 0.73; loss: 1.9812923669815063\n",
      "Training epoch 2565 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2565 ; accuracy: 0.73; loss: 1.981442928314209\n",
      "Training epoch 2566 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2566 ; accuracy: 0.73; loss: 1.981601357460022\n",
      "Training epoch 2567 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2567 ; accuracy: 0.73; loss: 1.9817547798156738\n",
      "Training epoch 2568 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2568 ; accuracy: 0.73; loss: 1.9819061756134033\n",
      "Training epoch 2569 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2569 ; accuracy: 0.73; loss: 1.9820475578308105\n",
      "Training epoch 2570 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2570 ; accuracy: 0.73; loss: 1.982183814048767\n",
      "Training epoch 2571 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2571 ; accuracy: 0.73; loss: 1.982316493988037\n",
      "Training epoch 2572 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2572 ; accuracy: 0.73; loss: 1.9824540615081787\n",
      "Training epoch 2573 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2573 ; accuracy: 0.73; loss: 1.9826116561889648\n",
      "Training epoch 2574 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2574 ; accuracy: 0.73; loss: 1.9827722311019897\n",
      "Training epoch 2575 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2575 ; accuracy: 0.73; loss: 1.9829291105270386\n",
      "Training epoch 2576 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2576 ; accuracy: 0.73; loss: 1.9830759763717651\n",
      "Training epoch 2577 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2577 ; accuracy: 0.73; loss: 1.9832199811935425\n",
      "Training epoch 2578 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2578 ; accuracy: 0.73; loss: 1.9833705425262451\n",
      "Training epoch 2579 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2579 ; accuracy: 0.7333333333333333; loss: 1.983516812324524\n",
      "Training epoch 2580 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2580 ; accuracy: 0.7333333333333333; loss: 1.9836567640304565\n",
      "Training epoch 2581 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2581 ; accuracy: 0.7333333333333333; loss: 1.9838043451309204\n",
      "Training epoch 2582 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2582 ; accuracy: 0.7333333333333333; loss: 1.9839774370193481\n",
      "Training epoch 2583 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2583 ; accuracy: 0.7333333333333333; loss: 1.9841469526290894\n",
      "Training epoch 2584 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2584 ; accuracy: 0.7333333333333333; loss: 1.984318494796753\n",
      "Training epoch 2585 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 2585 ; accuracy: 0.7333333333333333; loss: 1.984501600265503\n",
      "Training epoch 2586 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2586 ; accuracy: 0.7333333333333333; loss: 1.9846819639205933\n",
      "Training epoch 2587 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2587 ; accuracy: 0.7333333333333333; loss: 1.9848474264144897\n",
      "Training epoch 2588 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2588 ; accuracy: 0.7333333333333333; loss: 1.9850008487701416\n",
      "Training epoch 2589 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2589 ; accuracy: 0.7333333333333333; loss: 1.985161542892456\n",
      "Training epoch 2590 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2590 ; accuracy: 0.7333333333333333; loss: 1.9853230714797974\n",
      "Training epoch 2591 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2591 ; accuracy: 0.7333333333333333; loss: 1.9854856729507446\n",
      "Training epoch 2592 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2592 ; accuracy: 0.7333333333333333; loss: 1.9856441020965576\n",
      "Training epoch 2593 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2593 ; accuracy: 0.7333333333333333; loss: 1.9857969284057617\n",
      "Training epoch 2594 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2594 ; accuracy: 0.7333333333333333; loss: 1.9859435558319092\n",
      "Training epoch 2595 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2595 ; accuracy: 0.7333333333333333; loss: 1.9860990047454834\n",
      "Training epoch 2596 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2596 ; accuracy: 0.7333333333333333; loss: 1.9862487316131592\n",
      "Training epoch 2597 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2597 ; accuracy: 0.7333333333333333; loss: 1.9863883256912231\n",
      "Training epoch 2598 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2598 ; accuracy: 0.7333333333333333; loss: 1.9865247011184692\n",
      "Training epoch 2599 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2599 ; accuracy: 0.7333333333333333; loss: 1.9866780042648315\n",
      "Training epoch 2600 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2600 ; accuracy: 0.7333333333333333; loss: 1.9868253469467163\n",
      "Training epoch 2601 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2601 ; accuracy: 0.7333333333333333; loss: 1.9869589805603027\n",
      "Training epoch 2602 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2602 ; accuracy: 0.7333333333333333; loss: 1.987094759941101\n",
      "Training epoch 2603 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2603 ; accuracy: 0.7333333333333333; loss: 1.9872281551361084\n",
      "Training epoch 2604 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2604 ; accuracy: 0.7333333333333333; loss: 1.9873685836791992\n",
      "Training epoch 2605 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2605 ; accuracy: 0.7333333333333333; loss: 1.9875041246414185\n",
      "Training epoch 2606 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2606 ; accuracy: 0.7333333333333333; loss: 1.9876346588134766\n",
      "Training epoch 2607 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2607 ; accuracy: 0.7333333333333333; loss: 1.9877632856369019\n",
      "Training epoch 2608 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2608 ; accuracy: 0.7333333333333333; loss: 1.9878897666931152\n",
      "Training epoch 2609 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2609 ; accuracy: 0.7333333333333333; loss: 1.9880088567733765\n",
      "Training epoch 2610 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2610 ; accuracy: 0.7333333333333333; loss: 1.988128423690796\n",
      "Training epoch 2611 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2611 ; accuracy: 0.7333333333333333; loss: 1.9882519245147705\n",
      "Training epoch 2612 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2612 ; accuracy: 0.7333333333333333; loss: 1.988379716873169\n",
      "Training epoch 2613 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2613 ; accuracy: 0.7333333333333333; loss: 1.9885233640670776\n",
      "Training epoch 2614 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 2614 ; accuracy: 0.7333333333333333; loss: 1.9886879920959473\n",
      "Training epoch 2615 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2615 ; accuracy: 0.7333333333333333; loss: 1.9888458251953125\n",
      "Training epoch 2616 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2616 ; accuracy: 0.7333333333333333; loss: 1.9890007972717285\n",
      "Training epoch 2617 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2617 ; accuracy: 0.7333333333333333; loss: 1.9891639947891235\n",
      "Training epoch 2618 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2618 ; accuracy: 0.7333333333333333; loss: 1.989324927330017\n",
      "Training epoch 2619 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2619 ; accuracy: 0.7333333333333333; loss: 1.989491581916809\n",
      "Training epoch 2620 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2620 ; accuracy: 0.7333333333333333; loss: 1.989659070968628\n",
      "Training epoch 2621 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2621 ; accuracy: 0.7333333333333333; loss: 1.9898152351379395\n",
      "Training epoch 2622 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2622 ; accuracy: 0.7333333333333333; loss: 1.9899669885635376\n",
      "Training epoch 2623 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2623 ; accuracy: 0.7333333333333333; loss: 1.990115761756897\n",
      "Training epoch 2624 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2624 ; accuracy: 0.7333333333333333; loss: 1.9902628660202026\n",
      "Training epoch 2625 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2625 ; accuracy: 0.7333333333333333; loss: 1.9904062747955322\n",
      "Training epoch 2626 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2626 ; accuracy: 0.7333333333333333; loss: 1.9905415773391724\n",
      "Training epoch 2627 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2627 ; accuracy: 0.7333333333333333; loss: 1.990679144859314\n",
      "Training epoch 2628 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2628 ; accuracy: 0.7333333333333333; loss: 1.9907889366149902\n",
      "Training epoch 2629 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2629 ; accuracy: 0.7333333333333333; loss: 1.990902304649353\n",
      "Training epoch 2630 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2630 ; accuracy: 0.7333333333333333; loss: 1.9910162687301636\n",
      "Training epoch 2631 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2631 ; accuracy: 0.7333333333333333; loss: 1.9911521673202515\n",
      "Training epoch 2632 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2632 ; accuracy: 0.7333333333333333; loss: 1.9912937879562378\n",
      "Training epoch 2633 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2633 ; accuracy: 0.7333333333333333; loss: 1.991435170173645\n",
      "Training epoch 2634 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2634 ; accuracy: 0.7333333333333333; loss: 1.9915775060653687\n",
      "Training epoch 2635 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2635 ; accuracy: 0.7333333333333333; loss: 1.9917230606079102\n",
      "Training epoch 2636 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2636 ; accuracy: 0.7333333333333333; loss: 1.9918681383132935\n",
      "Training epoch 2637 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2637 ; accuracy: 0.7333333333333333; loss: 1.9920254945755005\n",
      "Training epoch 2638 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2638 ; accuracy: 0.7333333333333333; loss: 1.9921773672103882\n",
      "Training epoch 2639 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2639 ; accuracy: 0.7333333333333333; loss: 1.9923095703125\n",
      "Training epoch 2640 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2640 ; accuracy: 0.7333333333333333; loss: 1.9924529790878296\n",
      "Training epoch 2641 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2641 ; accuracy: 0.7333333333333333; loss: 1.9926069974899292\n",
      "Training epoch 2642 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2642 ; accuracy: 0.7333333333333333; loss: 1.9927546977996826\n",
      "Training epoch 2643 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2643 ; accuracy: 0.7333333333333333; loss: 1.9928988218307495\n",
      "Training epoch 2644 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2644 ; accuracy: 0.7333333333333333; loss: 1.9930576086044312\n",
      "Training epoch 2645 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2645 ; accuracy: 0.7333333333333333; loss: 1.9932094812393188\n",
      "Training epoch 2646 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2646 ; accuracy: 0.7333333333333333; loss: 1.9933538436889648\n",
      "Training epoch 2647 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2647 ; accuracy: 0.7333333333333333; loss: 1.9934911727905273\n",
      "Training epoch 2648 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2648 ; accuracy: 0.7333333333333333; loss: 1.9936332702636719\n",
      "Training epoch 2649 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2649 ; accuracy: 0.7333333333333333; loss: 1.9937824010849\n",
      "Training epoch 2650 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2650 ; accuracy: 0.7333333333333333; loss: 1.9939287900924683\n",
      "Training epoch 2651 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2651 ; accuracy: 0.7333333333333333; loss: 1.994072675704956\n",
      "Training epoch 2652 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2652 ; accuracy: 0.7333333333333333; loss: 1.9942069053649902\n",
      "Training epoch 2653 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2653 ; accuracy: 0.7333333333333333; loss: 1.9943424463272095\n",
      "Training epoch 2654 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2654 ; accuracy: 0.7333333333333333; loss: 1.9944771528244019\n",
      "Training epoch 2655 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2655 ; accuracy: 0.7333333333333333; loss: 1.9946147203445435\n",
      "Training epoch 2656 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2656 ; accuracy: 0.7333333333333333; loss: 1.9947489500045776\n",
      "Training epoch 2657 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2657 ; accuracy: 0.7333333333333333; loss: 1.9948782920837402\n",
      "Training epoch 2658 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2658 ; accuracy: 0.7333333333333333; loss: 1.995008111000061\n",
      "Training epoch 2659 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2659 ; accuracy: 0.7333333333333333; loss: 1.995144009590149\n",
      "Training epoch 2660 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2660 ; accuracy: 0.7333333333333333; loss: 1.995279312133789\n",
      "Training epoch 2661 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2661 ; accuracy: 0.7333333333333333; loss: 1.9954142570495605\n",
      "Training epoch 2662 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2662 ; accuracy: 0.7333333333333333; loss: 1.9955482482910156\n",
      "Training epoch 2663 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2663 ; accuracy: 0.7333333333333333; loss: 1.9956800937652588\n",
      "Training epoch 2664 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2664 ; accuracy: 0.7333333333333333; loss: 1.9958136081695557\n",
      "Training epoch 2665 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2665 ; accuracy: 0.7333333333333333; loss: 1.9959442615509033\n",
      "Training epoch 2666 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2666 ; accuracy: 0.7333333333333333; loss: 1.9960793256759644\n",
      "Training epoch 2667 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2667 ; accuracy: 0.7333333333333333; loss: 1.9962190389633179\n",
      "Training epoch 2668 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2668 ; accuracy: 0.7333333333333333; loss: 1.9963494539260864\n",
      "Training epoch 2669 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2669 ; accuracy: 0.7333333333333333; loss: 1.9964888095855713\n",
      "Training epoch 2670 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2670 ; accuracy: 0.7333333333333333; loss: 1.9966309070587158\n",
      "Training epoch 2671 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2671 ; accuracy: 0.7333333333333333; loss: 1.996801733970642\n",
      "Training epoch 2672 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2672 ; accuracy: 0.7333333333333333; loss: 1.996982216835022\n",
      "Training epoch 2673 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2673 ; accuracy: 0.7333333333333333; loss: 1.9971537590026855\n",
      "Training epoch 2674 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2674 ; accuracy: 0.7333333333333333; loss: 1.9973244667053223\n",
      "Training epoch 2675 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2675 ; accuracy: 0.7333333333333333; loss: 1.997497320175171\n",
      "Training epoch 2676 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2676 ; accuracy: 0.7333333333333333; loss: 1.9976643323898315\n",
      "Training epoch 2677 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2677 ; accuracy: 0.7333333333333333; loss: 1.9978344440460205\n",
      "Training epoch 2678 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2678 ; accuracy: 0.7333333333333333; loss: 1.9980049133300781\n",
      "Training epoch 2679 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2679 ; accuracy: 0.7333333333333333; loss: 1.9981648921966553\n",
      "Training epoch 2680 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2680 ; accuracy: 0.7333333333333333; loss: 1.9983206987380981\n",
      "Training epoch 2681 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2681 ; accuracy: 0.7333333333333333; loss: 1.9984816312789917\n",
      "Training epoch 2682 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2682 ; accuracy: 0.7333333333333333; loss: 1.9986408948898315\n",
      "Training epoch 2683 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2683 ; accuracy: 0.7333333333333333; loss: 1.9987943172454834\n",
      "Training epoch 2684 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2684 ; accuracy: 0.7333333333333333; loss: 1.9989385604858398\n",
      "Training epoch 2685 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2685 ; accuracy: 0.7333333333333333; loss: 1.9990792274475098\n",
      "Training epoch 2686 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2686 ; accuracy: 0.7333333333333333; loss: 1.9992589950561523\n",
      "Training epoch 2687 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2687 ; accuracy: 0.7333333333333333; loss: 1.9994374513626099\n",
      "Training epoch 2688 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2688 ; accuracy: 0.7333333333333333; loss: 1.9995981454849243\n",
      "Training epoch 2689 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2689 ; accuracy: 0.7333333333333333; loss: 1.9997576475143433\n",
      "Training epoch 2690 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2690 ; accuracy: 0.7333333333333333; loss: 1.9999240636825562\n",
      "Training epoch 2691 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2691 ; accuracy: 0.7333333333333333; loss: 2.000089168548584\n",
      "Training epoch 2692 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2692 ; accuracy: 0.7333333333333333; loss: 2.0002553462982178\n",
      "Training epoch 2693 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2693 ; accuracy: 0.7333333333333333; loss: 2.0004162788391113\n",
      "Training epoch 2694 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2694 ; accuracy: 0.7333333333333333; loss: 2.000567674636841\n",
      "Training epoch 2695 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2695 ; accuracy: 0.7333333333333333; loss: 2.000713586807251\n",
      "Training epoch 2696 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2696 ; accuracy: 0.7333333333333333; loss: 2.000856399536133\n",
      "Training epoch 2697 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2697 ; accuracy: 0.7333333333333333; loss: 2.0009913444519043\n",
      "Training epoch 2698 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2698 ; accuracy: 0.7333333333333333; loss: 2.001124143600464\n",
      "Training epoch 2699 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2699 ; accuracy: 0.7333333333333333; loss: 2.0012552738189697\n",
      "Training epoch 2700 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2700 ; accuracy: 0.7333333333333333; loss: 2.001375913619995\n",
      "Training epoch 2701 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2701 ; accuracy: 0.7333333333333333; loss: 2.0015017986297607\n",
      "Training epoch 2702 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2702 ; accuracy: 0.7333333333333333; loss: 2.0016255378723145\n",
      "Training epoch 2703 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2703 ; accuracy: 0.7333333333333333; loss: 2.0017552375793457\n",
      "Training epoch 2704 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2704 ; accuracy: 0.7333333333333333; loss: 2.0018880367279053\n",
      "Training epoch 2705 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2705 ; accuracy: 0.7333333333333333; loss: 2.0020318031311035\n",
      "Training epoch 2706 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2706 ; accuracy: 0.7333333333333333; loss: 2.0021772384643555\n",
      "Training epoch 2707 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 2707 ; accuracy: 0.7333333333333333; loss: 2.0023698806762695\n",
      "Training epoch 2708 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2708 ; accuracy: 0.7333333333333333; loss: 2.0025572776794434\n",
      "Training epoch 2709 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2709 ; accuracy: 0.7333333333333333; loss: 2.0027382373809814\n",
      "Training epoch 2710 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2710 ; accuracy: 0.7333333333333333; loss: 2.002906560897827\n",
      "Training epoch 2711 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2711 ; accuracy: 0.7333333333333333; loss: 2.003070831298828\n",
      "Training epoch 2712 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2712 ; accuracy: 0.7333333333333333; loss: 2.003220319747925\n",
      "Training epoch 2713 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2713 ; accuracy: 0.7333333333333333; loss: 2.003361463546753\n",
      "Training epoch 2714 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2714 ; accuracy: 0.7333333333333333; loss: 2.0035080909729004\n",
      "Training epoch 2715 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2715 ; accuracy: 0.7333333333333333; loss: 2.003647804260254\n",
      "Training epoch 2716 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2716 ; accuracy: 0.7333333333333333; loss: 2.0037906169891357\n",
      "Training epoch 2717 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2717 ; accuracy: 0.7333333333333333; loss: 2.003926992416382\n",
      "Training epoch 2718 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2718 ; accuracy: 0.7333333333333333; loss: 2.0040690898895264\n",
      "Training epoch 2719 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2719 ; accuracy: 0.7333333333333333; loss: 2.004209518432617\n",
      "Training epoch 2720 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2720 ; accuracy: 0.7333333333333333; loss: 2.004344940185547\n",
      "Training epoch 2721 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2721 ; accuracy: 0.7333333333333333; loss: 2.0044758319854736\n",
      "Training epoch 2722 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2722 ; accuracy: 0.7333333333333333; loss: 2.0046262741088867\n",
      "Training epoch 2723 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2723 ; accuracy: 0.7333333333333333; loss: 2.004770517349243\n",
      "Training epoch 2724 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2724 ; accuracy: 0.7333333333333333; loss: 2.0049092769622803\n",
      "Training epoch 2725 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2725 ; accuracy: 0.7333333333333333; loss: 2.0050508975982666\n",
      "Training epoch 2726 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2726 ; accuracy: 0.7333333333333333; loss: 2.0051980018615723\n",
      "Training epoch 2727 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2727 ; accuracy: 0.7333333333333333; loss: 2.0053420066833496\n",
      "Training epoch 2728 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2728 ; accuracy: 0.7333333333333333; loss: 2.005481004714966\n",
      "Training epoch 2729 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2729 ; accuracy: 0.7333333333333333; loss: 2.0056374073028564\n",
      "Training epoch 2730 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2730 ; accuracy: 0.7333333333333333; loss: 2.0057950019836426\n",
      "Training epoch 2731 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2731 ; accuracy: 0.7333333333333333; loss: 2.005958318710327\n",
      "Training epoch 2732 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2732 ; accuracy: 0.7333333333333333; loss: 2.0061161518096924\n",
      "Training epoch 2733 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2733 ; accuracy: 0.7333333333333333; loss: 2.00627064704895\n",
      "Training epoch 2734 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2734 ; accuracy: 0.7333333333333333; loss: 2.006416082382202\n",
      "Training epoch 2735 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2735 ; accuracy: 0.7333333333333333; loss: 2.0065605640411377\n",
      "Training epoch 2736 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2736 ; accuracy: 0.7333333333333333; loss: 2.0066914558410645\n",
      "Training epoch 2737 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2737 ; accuracy: 0.7333333333333333; loss: 2.0068154335021973\n",
      "Training epoch 2738 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2738 ; accuracy: 0.7333333333333333; loss: 2.006934881210327\n",
      "Training epoch 2739 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2739 ; accuracy: 0.7333333333333333; loss: 2.0070507526397705\n",
      "Training epoch 2740 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2740 ; accuracy: 0.7333333333333333; loss: 2.007167339324951\n",
      "Training epoch 2741 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2741 ; accuracy: 0.7333333333333333; loss: 2.007289171218872\n",
      "Training epoch 2742 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2742 ; accuracy: 0.7333333333333333; loss: 2.0074055194854736\n",
      "Training epoch 2743 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2743 ; accuracy: 0.7333333333333333; loss: 2.007519483566284\n",
      "Training epoch 2744 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2744 ; accuracy: 0.7333333333333333; loss: 2.0076286792755127\n",
      "Training epoch 2745 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2745 ; accuracy: 0.7333333333333333; loss: 2.0077359676361084\n",
      "Training epoch 2746 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2746 ; accuracy: 0.7333333333333333; loss: 2.007840633392334\n",
      "Training epoch 2747 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2747 ; accuracy: 0.7333333333333333; loss: 2.007946491241455\n",
      "Training epoch 2748 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2748 ; accuracy: 0.7333333333333333; loss: 2.0080530643463135\n",
      "Training epoch 2749 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2749 ; accuracy: 0.7333333333333333; loss: 2.0081660747528076\n",
      "Training epoch 2750 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2750 ; accuracy: 0.7333333333333333; loss: 2.0082926750183105\n",
      "Training epoch 2751 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2751 ; accuracy: 0.7333333333333333; loss: 2.0084218978881836\n",
      "Training epoch 2752 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2752 ; accuracy: 0.7333333333333333; loss: 2.008547782897949\n",
      "Training epoch 2753 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2753 ; accuracy: 0.7333333333333333; loss: 2.0086774826049805\n",
      "Training epoch 2754 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2754 ; accuracy: 0.7333333333333333; loss: 2.008801221847534\n",
      "Training epoch 2755 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2755 ; accuracy: 0.7333333333333333; loss: 2.0089375972747803\n",
      "Training epoch 2756 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2756 ; accuracy: 0.7333333333333333; loss: 2.0090694427490234\n",
      "Training epoch 2757 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2757 ; accuracy: 0.7333333333333333; loss: 2.0092105865478516\n",
      "Training epoch 2758 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2758 ; accuracy: 0.7333333333333333; loss: 2.009355068206787\n",
      "Training epoch 2759 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2759 ; accuracy: 0.7333333333333333; loss: 2.0094945430755615\n",
      "Training epoch 2760 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2760 ; accuracy: 0.7333333333333333; loss: 2.009641408920288\n",
      "Training epoch 2761 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2761 ; accuracy: 0.7333333333333333; loss: 2.0097811222076416\n",
      "Training epoch 2762 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2762 ; accuracy: 0.7333333333333333; loss: 2.00992488861084\n",
      "Training epoch 2763 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2763 ; accuracy: 0.7333333333333333; loss: 2.010077476501465\n",
      "Training epoch 2764 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2764 ; accuracy: 0.7333333333333333; loss: 2.010219097137451\n",
      "Training epoch 2765 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2765 ; accuracy: 0.7333333333333333; loss: 2.0103559494018555\n",
      "Training epoch 2766 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2766 ; accuracy: 0.7333333333333333; loss: 2.010496139526367\n",
      "Training epoch 2767 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2767 ; accuracy: 0.7333333333333333; loss: 2.0106289386749268\n",
      "Training epoch 2768 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2768 ; accuracy: 0.7333333333333333; loss: 2.0107619762420654\n",
      "Training epoch 2769 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2769 ; accuracy: 0.7333333333333333; loss: 2.0108838081359863\n",
      "Training epoch 2770 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2770 ; accuracy: 0.7333333333333333; loss: 2.0110039710998535\n",
      "Training epoch 2771 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2771 ; accuracy: 0.7333333333333333; loss: 2.011118173599243\n",
      "Training epoch 2772 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2772 ; accuracy: 0.7333333333333333; loss: 2.0112223625183105\n",
      "Training epoch 2773 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2773 ; accuracy: 0.7333333333333333; loss: 2.011367082595825\n",
      "Training epoch 2774 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2774 ; accuracy: 0.7333333333333333; loss: 2.0115063190460205\n",
      "Training epoch 2775 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2775 ; accuracy: 0.7333333333333333; loss: 2.011636734008789\n",
      "Training epoch 2776 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2776 ; accuracy: 0.7333333333333333; loss: 2.0117499828338623\n",
      "Training epoch 2777 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2777 ; accuracy: 0.7333333333333333; loss: 2.0118589401245117\n",
      "Training epoch 2778 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2778 ; accuracy: 0.7333333333333333; loss: 2.011965751647949\n",
      "Training epoch 2779 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2779 ; accuracy: 0.7333333333333333; loss: 2.012073278427124\n",
      "Training epoch 2780 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2780 ; accuracy: 0.7333333333333333; loss: 2.0121731758117676\n",
      "Training epoch 2781 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2781 ; accuracy: 0.7333333333333333; loss: 2.0122761726379395\n",
      "Training epoch 2782 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2782 ; accuracy: 0.7333333333333333; loss: 2.012382984161377\n",
      "Training epoch 2783 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2783 ; accuracy: 0.7333333333333333; loss: 2.0125277042388916\n",
      "Training epoch 2784 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 2784 ; accuracy: 0.7333333333333333; loss: 2.012676954269409\n",
      "Training epoch 2785 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2785 ; accuracy: 0.7333333333333333; loss: 2.0128254890441895\n",
      "Training epoch 2786 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2786 ; accuracy: 0.7333333333333333; loss: 2.012976884841919\n",
      "Training epoch 2787 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2787 ; accuracy: 0.7333333333333333; loss: 2.0131287574768066\n",
      "Training epoch 2788 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2788 ; accuracy: 0.7333333333333333; loss: 2.0132744312286377\n",
      "Training epoch 2789 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2789 ; accuracy: 0.7333333333333333; loss: 2.013416290283203\n",
      "Training epoch 2790 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2790 ; accuracy: 0.7333333333333333; loss: 2.013554096221924\n",
      "Training epoch 2791 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2791 ; accuracy: 0.7333333333333333; loss: 2.013688564300537\n",
      "Training epoch 2792 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2792 ; accuracy: 0.7333333333333333; loss: 2.0138256549835205\n",
      "Training epoch 2793 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2793 ; accuracy: 0.7333333333333333; loss: 2.013964891433716\n",
      "Training epoch 2794 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2794 ; accuracy: 0.7333333333333333; loss: 2.0140957832336426\n",
      "Training epoch 2795 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2795 ; accuracy: 0.7333333333333333; loss: 2.014244318008423\n",
      "Training epoch 2796 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2796 ; accuracy: 0.7333333333333333; loss: 2.0143840312957764\n",
      "Training epoch 2797 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2797 ; accuracy: 0.7333333333333333; loss: 2.0145223140716553\n",
      "Training epoch 2798 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2798 ; accuracy: 0.7333333333333333; loss: 2.0146563053131104\n",
      "Training epoch 2799 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2799 ; accuracy: 0.7333333333333333; loss: 2.014774799346924\n",
      "Training epoch 2800 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2800 ; accuracy: 0.7333333333333333; loss: 2.0148966312408447\n",
      "Training epoch 2801 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2801 ; accuracy: 0.7333333333333333; loss: 2.015017032623291\n",
      "Training epoch 2802 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2802 ; accuracy: 0.7333333333333333; loss: 2.015164852142334\n",
      "Training epoch 2803 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2803 ; accuracy: 0.7333333333333333; loss: 2.0153045654296875\n",
      "Training epoch 2804 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2804 ; accuracy: 0.7333333333333333; loss: 2.0154430866241455\n",
      "Training epoch 2805 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2805 ; accuracy: 0.7333333333333333; loss: 2.015585422515869\n",
      "Training epoch 2806 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2806 ; accuracy: 0.7333333333333333; loss: 2.0157296657562256\n",
      "Training epoch 2807 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2807 ; accuracy: 0.7333333333333333; loss: 2.01586651802063\n",
      "Training epoch 2808 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2808 ; accuracy: 0.7333333333333333; loss: 2.0159952640533447\n",
      "Training epoch 2809 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2809 ; accuracy: 0.7333333333333333; loss: 2.0161190032958984\n",
      "Training epoch 2810 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2810 ; accuracy: 0.7333333333333333; loss: 2.0162386894226074\n",
      "Training epoch 2811 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2811 ; accuracy: 0.7333333333333333; loss: 2.016352891921997\n",
      "Training epoch 2812 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2812 ; accuracy: 0.7333333333333333; loss: 2.0164642333984375\n",
      "Training epoch 2813 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2813 ; accuracy: 0.7333333333333333; loss: 2.016575813293457\n",
      "Training epoch 2814 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2814 ; accuracy: 0.7333333333333333; loss: 2.016684055328369\n",
      "Training epoch 2815 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2815 ; accuracy: 0.7333333333333333; loss: 2.0167899131774902\n",
      "Training epoch 2816 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2816 ; accuracy: 0.7333333333333333; loss: 2.016892910003662\n",
      "Training epoch 2817 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2817 ; accuracy: 0.7333333333333333; loss: 2.0169849395751953\n",
      "Training epoch 2818 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2818 ; accuracy: 0.7333333333333333; loss: 2.0170764923095703\n",
      "Training epoch 2819 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2819 ; accuracy: 0.7333333333333333; loss: 2.0171687602996826\n",
      "Training epoch 2820 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2820 ; accuracy: 0.7333333333333333; loss: 2.0172619819641113\n",
      "Training epoch 2821 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2821 ; accuracy: 0.7333333333333333; loss: 2.017350196838379\n",
      "Training epoch 2822 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2822 ; accuracy: 0.7333333333333333; loss: 2.01743483543396\n",
      "Training epoch 2823 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2823 ; accuracy: 0.7333333333333333; loss: 2.017524480819702\n",
      "Training epoch 2824 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2824 ; accuracy: 0.7333333333333333; loss: 2.0176122188568115\n",
      "Training epoch 2825 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2825 ; accuracy: 0.7333333333333333; loss: 2.0177032947540283\n",
      "Training epoch 2826 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2826 ; accuracy: 0.7333333333333333; loss: 2.017803430557251\n",
      "Training epoch 2827 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2827 ; accuracy: 0.7333333333333333; loss: 2.017911195755005\n",
      "Training epoch 2828 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2828 ; accuracy: 0.7333333333333333; loss: 2.0180253982543945\n",
      "Training epoch 2829 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2829 ; accuracy: 0.7333333333333333; loss: 2.0181355476379395\n",
      "Training epoch 2830 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2830 ; accuracy: 0.7333333333333333; loss: 2.0182418823242188\n",
      "Training epoch 2831 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2831 ; accuracy: 0.7333333333333333; loss: 2.018346071243286\n",
      "Training epoch 2832 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2832 ; accuracy: 0.7333333333333333; loss: 2.0184545516967773\n",
      "Training epoch 2833 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2833 ; accuracy: 0.7333333333333333; loss: 2.01855731010437\n",
      "Training epoch 2834 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2834 ; accuracy: 0.7333333333333333; loss: 2.0186562538146973\n",
      "Training epoch 2835 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2835 ; accuracy: 0.7333333333333333; loss: 2.018758773803711\n",
      "Training epoch 2836 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2836 ; accuracy: 0.7333333333333333; loss: 2.018852710723877\n",
      "Training epoch 2837 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2837 ; accuracy: 0.7333333333333333; loss: 2.018949270248413\n",
      "Training epoch 2838 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2838 ; accuracy: 0.7333333333333333; loss: 2.0190491676330566\n",
      "Training epoch 2839 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2839 ; accuracy: 0.7333333333333333; loss: 2.019153356552124\n",
      "Training epoch 2840 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2840 ; accuracy: 0.7333333333333333; loss: 2.019249200820923\n",
      "Training epoch 2841 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2841 ; accuracy: 0.7333333333333333; loss: 2.019343376159668\n",
      "Training epoch 2842 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2842 ; accuracy: 0.7333333333333333; loss: 2.019442558288574\n",
      "Training epoch 2843 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2843 ; accuracy: 0.7333333333333333; loss: 2.019545316696167\n",
      "Training epoch 2844 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2844 ; accuracy: 0.7333333333333333; loss: 2.0196359157562256\n",
      "Training epoch 2845 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2845 ; accuracy: 0.7333333333333333; loss: 2.019728660583496\n",
      "Training epoch 2846 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2846 ; accuracy: 0.7333333333333333; loss: 2.0198116302490234\n",
      "Training epoch 2847 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2847 ; accuracy: 0.7333333333333333; loss: 2.019902229309082\n",
      "Training epoch 2848 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2848 ; accuracy: 0.7333333333333333; loss: 2.0199897289276123\n",
      "Training epoch 2849 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2849 ; accuracy: 0.7333333333333333; loss: 2.0200862884521484\n",
      "Training epoch 2850 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2850 ; accuracy: 0.7333333333333333; loss: 2.0201821327209473\n",
      "Training epoch 2851 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2851 ; accuracy: 0.7333333333333333; loss: 2.020282506942749\n",
      "Training epoch 2852 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2852 ; accuracy: 0.7333333333333333; loss: 2.020388126373291\n",
      "Training epoch 2853 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2853 ; accuracy: 0.7333333333333333; loss: 2.0204832553863525\n",
      "Training epoch 2854 ; accuracy: 0.9; loss: 0.19459126889705658\n",
      "Validation epoch 2854 ; accuracy: 0.7333333333333333; loss: 2.020549774169922\n",
      "Training epoch 2855 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2855 ; accuracy: 0.74; loss: 2.0206117630004883\n",
      "Training epoch 2856 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2856 ; accuracy: 0.74; loss: 2.0206754207611084\n",
      "Training epoch 2857 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2857 ; accuracy: 0.74; loss: 2.020745038986206\n",
      "Training epoch 2858 ; accuracy: 0.9; loss: 0.19459131360054016\n",
      "Validation epoch 2858 ; accuracy: 0.74; loss: 2.0208635330200195\n",
      "Training epoch 2859 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2859 ; accuracy: 0.74; loss: 2.0209760665893555\n",
      "Training epoch 2860 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2860 ; accuracy: 0.74; loss: 2.0210952758789062\n",
      "Training epoch 2861 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2861 ; accuracy: 0.74; loss: 2.0212085247039795\n",
      "Training epoch 2862 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2862 ; accuracy: 0.74; loss: 2.021350860595703\n",
      "Training epoch 2863 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2863 ; accuracy: 0.74; loss: 2.0214855670928955\n",
      "Training epoch 2864 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2864 ; accuracy: 0.74; loss: 2.0216140747070312\n",
      "Training epoch 2865 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 2865 ; accuracy: 0.74; loss: 2.021749973297119\n",
      "Training epoch 2866 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2866 ; accuracy: 0.74; loss: 2.021878957748413\n",
      "Training epoch 2867 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2867 ; accuracy: 0.74; loss: 2.0220110416412354\n",
      "Training epoch 2868 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2868 ; accuracy: 0.7366666666666667; loss: 2.0221445560455322\n",
      "Training epoch 2869 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2869 ; accuracy: 0.7366666666666667; loss: 2.0222747325897217\n",
      "Training epoch 2870 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2870 ; accuracy: 0.7366666666666667; loss: 2.0223984718322754\n",
      "Training epoch 2871 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2871 ; accuracy: 0.7366666666666667; loss: 2.0225231647491455\n",
      "Training epoch 2872 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2872 ; accuracy: 0.7333333333333333; loss: 2.022641181945801\n",
      "Training epoch 2873 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2873 ; accuracy: 0.7333333333333333; loss: 2.0227530002593994\n",
      "Training epoch 2874 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2874 ; accuracy: 0.7333333333333333; loss: 2.0228593349456787\n",
      "Training epoch 2875 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2875 ; accuracy: 0.7333333333333333; loss: 2.022955894470215\n",
      "Training epoch 2876 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2876 ; accuracy: 0.7333333333333333; loss: 2.0230517387390137\n",
      "Training epoch 2877 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2877 ; accuracy: 0.7333333333333333; loss: 2.0231447219848633\n",
      "Training epoch 2878 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2878 ; accuracy: 0.7333333333333333; loss: 2.023240089416504\n",
      "Training epoch 2879 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2879 ; accuracy: 0.7333333333333333; loss: 2.0233335494995117\n",
      "Training epoch 2880 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2880 ; accuracy: 0.7333333333333333; loss: 2.023425340652466\n",
      "Training epoch 2881 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2881 ; accuracy: 0.7333333333333333; loss: 2.0235464572906494\n",
      "Training epoch 2882 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2882 ; accuracy: 0.7333333333333333; loss: 2.0236804485321045\n",
      "Training epoch 2883 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2883 ; accuracy: 0.7333333333333333; loss: 2.0238125324249268\n",
      "Training epoch 2884 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2884 ; accuracy: 0.7333333333333333; loss: 2.0239455699920654\n",
      "Training epoch 2885 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2885 ; accuracy: 0.7333333333333333; loss: 2.0240726470947266\n",
      "Training epoch 2886 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2886 ; accuracy: 0.7333333333333333; loss: 2.024191379547119\n",
      "Training epoch 2887 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2887 ; accuracy: 0.7333333333333333; loss: 2.0243117809295654\n",
      "Training epoch 2888 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2888 ; accuracy: 0.7333333333333333; loss: 2.024425506591797\n",
      "Training epoch 2889 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2889 ; accuracy: 0.7333333333333333; loss: 2.0245425701141357\n",
      "Training epoch 2890 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2890 ; accuracy: 0.7333333333333333; loss: 2.0246617794036865\n",
      "Training epoch 2891 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2891 ; accuracy: 0.7333333333333333; loss: 2.0247766971588135\n",
      "Training epoch 2892 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2892 ; accuracy: 0.7333333333333333; loss: 2.024885892868042\n",
      "Training epoch 2893 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2893 ; accuracy: 0.7333333333333333; loss: 2.024991750717163\n",
      "Training epoch 2894 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2894 ; accuracy: 0.7333333333333333; loss: 2.025097131729126\n",
      "Training epoch 2895 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2895 ; accuracy: 0.7333333333333333; loss: 2.025193214416504\n",
      "Training epoch 2896 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2896 ; accuracy: 0.7333333333333333; loss: 2.0252845287323\n",
      "Training epoch 2897 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2897 ; accuracy: 0.7333333333333333; loss: 2.025369882583618\n",
      "Training epoch 2898 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2898 ; accuracy: 0.7333333333333333; loss: 2.0254485607147217\n",
      "Training epoch 2899 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2899 ; accuracy: 0.7333333333333333; loss: 2.025524616241455\n",
      "Training epoch 2900 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2900 ; accuracy: 0.7333333333333333; loss: 2.0256035327911377\n",
      "Training epoch 2901 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2901 ; accuracy: 0.7333333333333333; loss: 2.0256781578063965\n",
      "Training epoch 2902 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2902 ; accuracy: 0.7333333333333333; loss: 2.0257582664489746\n",
      "Training epoch 2903 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2903 ; accuracy: 0.7366666666666667; loss: 2.025838613510132\n",
      "Training epoch 2904 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2904 ; accuracy: 0.7366666666666667; loss: 2.0259170532226562\n",
      "Training epoch 2905 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2905 ; accuracy: 0.7366666666666667; loss: 2.025996208190918\n",
      "Training epoch 2906 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2906 ; accuracy: 0.7366666666666667; loss: 2.0260751247406006\n",
      "Training epoch 2907 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2907 ; accuracy: 0.7366666666666667; loss: 2.026154041290283\n",
      "Training epoch 2908 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2908 ; accuracy: 0.7366666666666667; loss: 2.0262277126312256\n",
      "Training epoch 2909 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2909 ; accuracy: 0.7366666666666667; loss: 2.0263142585754395\n",
      "Training epoch 2910 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2910 ; accuracy: 0.7366666666666667; loss: 2.026406764984131\n",
      "Training epoch 2911 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2911 ; accuracy: 0.7366666666666667; loss: 2.026505947113037\n",
      "Training epoch 2912 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2912 ; accuracy: 0.7366666666666667; loss: 2.026601552963257\n",
      "Training epoch 2913 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2913 ; accuracy: 0.7366666666666667; loss: 2.0266993045806885\n",
      "Training epoch 2914 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2914 ; accuracy: 0.7366666666666667; loss: 2.026798963546753\n",
      "Training epoch 2915 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2915 ; accuracy: 0.7366666666666667; loss: 2.026902675628662\n",
      "Training epoch 2916 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2916 ; accuracy: 0.7366666666666667; loss: 2.026994466781616\n",
      "Training epoch 2917 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2917 ; accuracy: 0.7366666666666667; loss: 2.0270836353302\n",
      "Training epoch 2918 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 2918 ; accuracy: 0.7366666666666667; loss: 2.0271692276000977\n",
      "Training epoch 2919 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2919 ; accuracy: 0.7366666666666667; loss: 2.0272555351257324\n",
      "Training epoch 2920 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2920 ; accuracy: 0.7366666666666667; loss: 2.027336597442627\n",
      "Training epoch 2921 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2921 ; accuracy: 0.7366666666666667; loss: 2.0274100303649902\n",
      "Training epoch 2922 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2922 ; accuracy: 0.7366666666666667; loss: 2.0275089740753174\n",
      "Training epoch 2923 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2923 ; accuracy: 0.7366666666666667; loss: 2.0276153087615967\n",
      "Training epoch 2924 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2924 ; accuracy: 0.7366666666666667; loss: 2.0277135372161865\n",
      "Training epoch 2925 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2925 ; accuracy: 0.7366666666666667; loss: 2.0278165340423584\n",
      "Training epoch 2926 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2926 ; accuracy: 0.7366666666666667; loss: 2.0279109477996826\n",
      "Training epoch 2927 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2927 ; accuracy: 0.7366666666666667; loss: 2.0280017852783203\n",
      "Training epoch 2928 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2928 ; accuracy: 0.7366666666666667; loss: 2.0280914306640625\n",
      "Training epoch 2929 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2929 ; accuracy: 0.7366666666666667; loss: 2.0281805992126465\n",
      "Training epoch 2930 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2930 ; accuracy: 0.7366666666666667; loss: 2.0282645225524902\n",
      "Training epoch 2931 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2931 ; accuracy: 0.7366666666666667; loss: 2.0283455848693848\n",
      "Training epoch 2932 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2932 ; accuracy: 0.7366666666666667; loss: 2.028409004211426\n",
      "Training epoch 2933 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 2933 ; accuracy: 0.7366666666666667; loss: 2.0285208225250244\n",
      "Training epoch 2934 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2934 ; accuracy: 0.7366666666666667; loss: 2.0286386013031006\n",
      "Training epoch 2935 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2935 ; accuracy: 0.7366666666666667; loss: 2.0287487506866455\n",
      "Training epoch 2936 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2936 ; accuracy: 0.7366666666666667; loss: 2.0288679599761963\n",
      "Training epoch 2937 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2937 ; accuracy: 0.7366666666666667; loss: 2.028994083404541\n",
      "Training epoch 2938 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2938 ; accuracy: 0.7366666666666667; loss: 2.02911639213562\n",
      "Training epoch 2939 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2939 ; accuracy: 0.7366666666666667; loss: 2.029226779937744\n",
      "Training epoch 2940 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2940 ; accuracy: 0.7366666666666667; loss: 2.029334545135498\n",
      "Training epoch 2941 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2941 ; accuracy: 0.7366666666666667; loss: 2.0294363498687744\n",
      "Training epoch 2942 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2942 ; accuracy: 0.7366666666666667; loss: 2.0295310020446777\n",
      "Training epoch 2943 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2943 ; accuracy: 0.7366666666666667; loss: 2.0296223163604736\n",
      "Training epoch 2944 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2944 ; accuracy: 0.7366666666666667; loss: 2.0297110080718994\n",
      "Training epoch 2945 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2945 ; accuracy: 0.7366666666666667; loss: 2.0298025608062744\n",
      "Training epoch 2946 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 2946 ; accuracy: 0.7366666666666667; loss: 2.029892921447754\n",
      "Training epoch 2947 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2947 ; accuracy: 0.7366666666666667; loss: 2.0299768447875977\n",
      "Training epoch 2948 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2948 ; accuracy: 0.7366666666666667; loss: 2.0300638675689697\n",
      "Training epoch 2949 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2949 ; accuracy: 0.7366666666666667; loss: 2.030151844024658\n",
      "Training epoch 2950 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2950 ; accuracy: 0.7366666666666667; loss: 2.030238389968872\n",
      "Training epoch 2951 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2951 ; accuracy: 0.7366666666666667; loss: 2.0303328037261963\n",
      "Training epoch 2952 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2952 ; accuracy: 0.7366666666666667; loss: 2.030407667160034\n",
      "Training epoch 2953 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2953 ; accuracy: 0.7366666666666667; loss: 2.0304789543151855\n",
      "Training epoch 2954 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2954 ; accuracy: 0.7366666666666667; loss: 2.0305581092834473\n",
      "Training epoch 2955 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2955 ; accuracy: 0.7366666666666667; loss: 2.0306334495544434\n",
      "Training epoch 2956 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2956 ; accuracy: 0.7366666666666667; loss: 2.030707359313965\n",
      "Training epoch 2957 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2957 ; accuracy: 0.7366666666666667; loss: 2.0307774543762207\n",
      "Training epoch 2958 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2958 ; accuracy: 0.7366666666666667; loss: 2.030846118927002\n",
      "Training epoch 2959 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 2959 ; accuracy: 0.7366666666666667; loss: 2.0309107303619385\n",
      "Training epoch 2960 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2960 ; accuracy: 0.74; loss: 2.0309720039367676\n",
      "Training epoch 2961 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2961 ; accuracy: 0.74; loss: 2.031034231185913\n",
      "Training epoch 2962 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2962 ; accuracy: 0.74; loss: 2.0310957431793213\n",
      "Training epoch 2963 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2963 ; accuracy: 0.74; loss: 2.031160593032837\n",
      "Training epoch 2964 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2964 ; accuracy: 0.74; loss: 2.031238555908203\n",
      "Training epoch 2965 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2965 ; accuracy: 0.74; loss: 2.0313191413879395\n",
      "Training epoch 2966 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2966 ; accuracy: 0.74; loss: 2.031402587890625\n",
      "Training epoch 2967 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2967 ; accuracy: 0.74; loss: 2.0314834117889404\n",
      "Training epoch 2968 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2968 ; accuracy: 0.74; loss: 2.0315651893615723\n",
      "Training epoch 2969 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2969 ; accuracy: 0.74; loss: 2.031649112701416\n",
      "Training epoch 2970 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2970 ; accuracy: 0.74; loss: 2.0317280292510986\n",
      "Training epoch 2971 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2971 ; accuracy: 0.74; loss: 2.0318071842193604\n",
      "Training epoch 2972 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2972 ; accuracy: 0.74; loss: 2.0318961143493652\n",
      "Training epoch 2973 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2973 ; accuracy: 0.74; loss: 2.031979560852051\n",
      "Training epoch 2974 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 2974 ; accuracy: 0.74; loss: 2.0320754051208496\n",
      "Training epoch 2975 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2975 ; accuracy: 0.74; loss: 2.0321695804595947\n",
      "Training epoch 2976 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 2976 ; accuracy: 0.74; loss: 2.032254219055176\n",
      "Training epoch 2977 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2977 ; accuracy: 0.74; loss: 2.032346248626709\n",
      "Training epoch 2978 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2978 ; accuracy: 0.74; loss: 2.032440423965454\n",
      "Training epoch 2979 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2979 ; accuracy: 0.74; loss: 2.0325310230255127\n",
      "Training epoch 2980 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2980 ; accuracy: 0.74; loss: 2.0326225757598877\n",
      "Training epoch 2981 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2981 ; accuracy: 0.74; loss: 2.032708168029785\n",
      "Training epoch 2982 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2982 ; accuracy: 0.74; loss: 2.0327816009521484\n",
      "Training epoch 2983 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 2983 ; accuracy: 0.74; loss: 2.032888174057007\n",
      "Training epoch 2984 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2984 ; accuracy: 0.74; loss: 2.0329864025115967\n",
      "Training epoch 2985 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2985 ; accuracy: 0.74; loss: 2.033080577850342\n",
      "Training epoch 2986 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2986 ; accuracy: 0.74; loss: 2.0331740379333496\n",
      "Training epoch 2987 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2987 ; accuracy: 0.74; loss: 2.033266544342041\n",
      "Training epoch 2988 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 2988 ; accuracy: 0.74; loss: 2.0333962440490723\n",
      "Training epoch 2989 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2989 ; accuracy: 0.74; loss: 2.033522605895996\n",
      "Training epoch 2990 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2990 ; accuracy: 0.74; loss: 2.033644676208496\n",
      "Training epoch 2991 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2991 ; accuracy: 0.74; loss: 2.033759117126465\n",
      "Training epoch 2992 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2992 ; accuracy: 0.74; loss: 2.0338704586029053\n",
      "Training epoch 2993 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2993 ; accuracy: 0.74; loss: 2.0339760780334473\n",
      "Training epoch 2994 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2994 ; accuracy: 0.74; loss: 2.0340917110443115\n",
      "Training epoch 2995 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 2995 ; accuracy: 0.74; loss: 2.034213066101074\n",
      "Training epoch 2996 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 2996 ; accuracy: 0.74; loss: 2.0343308448791504\n",
      "Training epoch 2997 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 2997 ; accuracy: 0.74; loss: 2.034442901611328\n",
      "Training epoch 2998 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 2998 ; accuracy: 0.74; loss: 2.034555196762085\n",
      "Training epoch 2999 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 2999 ; accuracy: 0.74; loss: 2.0346813201904297\n",
      "Training epoch 3000 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3000 ; accuracy: 0.74; loss: 2.0348005294799805\n",
      "Training epoch 3001 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3001 ; accuracy: 0.74; loss: 2.0349175930023193\n",
      "Training epoch 3002 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3002 ; accuracy: 0.74; loss: 2.035036087036133\n",
      "Training epoch 3003 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3003 ; accuracy: 0.74; loss: 2.035149335861206\n",
      "Training epoch 3004 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3004 ; accuracy: 0.7366666666666667; loss: 2.0352683067321777\n",
      "Training epoch 3005 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3005 ; accuracy: 0.7366666666666667; loss: 2.0353844165802\n",
      "Training epoch 3006 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3006 ; accuracy: 0.7366666666666667; loss: 2.035489082336426\n",
      "Training epoch 3007 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3007 ; accuracy: 0.7366666666666667; loss: 2.0355887413024902\n",
      "Training epoch 3008 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3008 ; accuracy: 0.74; loss: 2.035686492919922\n",
      "Training epoch 3009 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3009 ; accuracy: 0.74; loss: 2.035784959793091\n",
      "Training epoch 3010 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3010 ; accuracy: 0.74; loss: 2.035874128341675\n",
      "Training epoch 3011 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3011 ; accuracy: 0.74; loss: 2.035957098007202\n",
      "Training epoch 3012 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3012 ; accuracy: 0.74; loss: 2.036039113998413\n",
      "Training epoch 3013 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3013 ; accuracy: 0.74; loss: 2.0361132621765137\n",
      "Training epoch 3014 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3014 ; accuracy: 0.74; loss: 2.0361876487731934\n",
      "Training epoch 3015 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3015 ; accuracy: 0.74; loss: 2.0362606048583984\n",
      "Training epoch 3016 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3016 ; accuracy: 0.74; loss: 2.036337375640869\n",
      "Training epoch 3017 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3017 ; accuracy: 0.74; loss: 2.036417007446289\n",
      "Training epoch 3018 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3018 ; accuracy: 0.74; loss: 2.036494255065918\n",
      "Training epoch 3019 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3019 ; accuracy: 0.74; loss: 2.0365755558013916\n",
      "Training epoch 3020 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3020 ; accuracy: 0.74; loss: 2.0366523265838623\n",
      "Training epoch 3021 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3021 ; accuracy: 0.74; loss: 2.036733865737915\n",
      "Training epoch 3022 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3022 ; accuracy: 0.74; loss: 2.0368235111236572\n",
      "Training epoch 3023 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3023 ; accuracy: 0.74; loss: 2.0369205474853516\n",
      "Training epoch 3024 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3024 ; accuracy: 0.74; loss: 2.0370163917541504\n",
      "Training epoch 3025 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3025 ; accuracy: 0.74; loss: 2.037120819091797\n",
      "Training epoch 3026 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3026 ; accuracy: 0.74; loss: 2.0372281074523926\n",
      "Training epoch 3027 ; accuracy: 0.9; loss: 0.19459125399589539\n",
      "Validation epoch 3027 ; accuracy: 0.74; loss: 2.0373542308807373\n",
      "Training epoch 3028 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3028 ; accuracy: 0.74; loss: 2.0374844074249268\n",
      "Training epoch 3029 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3029 ; accuracy: 0.74; loss: 2.0376062393188477\n",
      "Training epoch 3030 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3030 ; accuracy: 0.74; loss: 2.037741184234619\n",
      "Training epoch 3031 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3031 ; accuracy: 0.74; loss: 2.037869453430176\n",
      "Training epoch 3032 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3032 ; accuracy: 0.7366666666666667; loss: 2.037989616394043\n",
      "Training epoch 3033 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3033 ; accuracy: 0.7366666666666667; loss: 2.0381104946136475\n",
      "Training epoch 3034 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3034 ; accuracy: 0.7366666666666667; loss: 2.038224458694458\n",
      "Training epoch 3035 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3035 ; accuracy: 0.7366666666666667; loss: 2.0383358001708984\n",
      "Training epoch 3036 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3036 ; accuracy: 0.7366666666666667; loss: 2.0384390354156494\n",
      "Training epoch 3037 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3037 ; accuracy: 0.7366666666666667; loss: 2.038540840148926\n",
      "Training epoch 3038 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3038 ; accuracy: 0.7366666666666667; loss: 2.0386390686035156\n",
      "Training epoch 3039 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3039 ; accuracy: 0.7366666666666667; loss: 2.038731098175049\n",
      "Training epoch 3040 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3040 ; accuracy: 0.7366666666666667; loss: 2.038823127746582\n",
      "Training epoch 3041 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3041 ; accuracy: 0.7366666666666667; loss: 2.0389087200164795\n",
      "Training epoch 3042 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3042 ; accuracy: 0.7366666666666667; loss: 2.0389902591705322\n",
      "Training epoch 3043 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3043 ; accuracy: 0.7366666666666667; loss: 2.0390894412994385\n",
      "Training epoch 3044 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 3044 ; accuracy: 0.7366666666666667; loss: 2.0391945838928223\n",
      "Training epoch 3045 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3045 ; accuracy: 0.7366666666666667; loss: 2.0392916202545166\n",
      "Training epoch 3046 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3046 ; accuracy: 0.7366666666666667; loss: 2.039395570755005\n",
      "Training epoch 3047 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 3047 ; accuracy: 0.7366666666666667; loss: 2.03951096534729\n",
      "Training epoch 3048 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3048 ; accuracy: 0.7366666666666667; loss: 2.0396149158477783\n",
      "Training epoch 3049 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3049 ; accuracy: 0.7366666666666667; loss: 2.0397207736968994\n",
      "Training epoch 3050 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3050 ; accuracy: 0.7366666666666667; loss: 2.039815664291382\n",
      "Training epoch 3051 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3051 ; accuracy: 0.7366666666666667; loss: 2.0398991107940674\n",
      "Training epoch 3052 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3052 ; accuracy: 0.7366666666666667; loss: 2.0399627685546875\n",
      "Training epoch 3053 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3053 ; accuracy: 0.7366666666666667; loss: 2.0400233268737793\n",
      "Training epoch 3054 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3054 ; accuracy: 0.7366666666666667; loss: 2.0400900840759277\n",
      "Training epoch 3055 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3055 ; accuracy: 0.7366666666666667; loss: 2.0401575565338135\n",
      "Training epoch 3056 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3056 ; accuracy: 0.7366666666666667; loss: 2.0402302742004395\n",
      "Training epoch 3057 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3057 ; accuracy: 0.7366666666666667; loss: 2.040302038192749\n",
      "Training epoch 3058 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3058 ; accuracy: 0.7366666666666667; loss: 2.040374517440796\n",
      "Training epoch 3059 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3059 ; accuracy: 0.7366666666666667; loss: 2.0404558181762695\n",
      "Training epoch 3060 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3060 ; accuracy: 0.7366666666666667; loss: 2.0405290126800537\n",
      "Training epoch 3061 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3061 ; accuracy: 0.74; loss: 2.04060435295105\n",
      "Training epoch 3062 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3062 ; accuracy: 0.74; loss: 2.040668249130249\n",
      "Training epoch 3063 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3063 ; accuracy: 0.74; loss: 2.0407307147979736\n",
      "Training epoch 3064 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3064 ; accuracy: 0.74; loss: 2.040792226791382\n",
      "Training epoch 3065 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3065 ; accuracy: 0.74; loss: 2.0408482551574707\n",
      "Training epoch 3066 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3066 ; accuracy: 0.74; loss: 2.0408992767333984\n",
      "Training epoch 3067 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3067 ; accuracy: 0.74; loss: 2.040952205657959\n",
      "Training epoch 3068 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3068 ; accuracy: 0.74; loss: 2.0410077571868896\n",
      "Training epoch 3069 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3069 ; accuracy: 0.74; loss: 2.0410637855529785\n",
      "Training epoch 3070 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3070 ; accuracy: 0.74; loss: 2.0411198139190674\n",
      "Training epoch 3071 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3071 ; accuracy: 0.74; loss: 2.041170835494995\n",
      "Training epoch 3072 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3072 ; accuracy: 0.74; loss: 2.041233539581299\n",
      "Training epoch 3073 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3073 ; accuracy: 0.74; loss: 2.041301965713501\n",
      "Training epoch 3074 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3074 ; accuracy: 0.74; loss: 2.0413687229156494\n",
      "Training epoch 3075 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3075 ; accuracy: 0.74; loss: 2.041433334350586\n",
      "Training epoch 3076 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3076 ; accuracy: 0.74; loss: 2.041494846343994\n",
      "Training epoch 3077 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3077 ; accuracy: 0.74; loss: 2.0415682792663574\n",
      "Training epoch 3078 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3078 ; accuracy: 0.74; loss: 2.041637420654297\n",
      "Training epoch 3079 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3079 ; accuracy: 0.74; loss: 2.0417087078094482\n",
      "Training epoch 3080 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3080 ; accuracy: 0.74; loss: 2.0417873859405518\n",
      "Training epoch 3081 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3081 ; accuracy: 0.74; loss: 2.041867256164551\n",
      "Training epoch 3082 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3082 ; accuracy: 0.74; loss: 2.0419466495513916\n",
      "Training epoch 3083 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3083 ; accuracy: 0.74; loss: 2.0420281887054443\n",
      "Training epoch 3084 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3084 ; accuracy: 0.74; loss: 2.0421078205108643\n",
      "Training epoch 3085 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3085 ; accuracy: 0.74; loss: 2.042151689529419\n",
      "Training epoch 3086 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3086 ; accuracy: 0.74; loss: 2.0421998500823975\n",
      "Training epoch 3087 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3087 ; accuracy: 0.74; loss: 2.042255163192749\n",
      "Training epoch 3088 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3088 ; accuracy: 0.74; loss: 2.0423104763031006\n",
      "Training epoch 3089 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3089 ; accuracy: 0.74; loss: 2.042372226715088\n",
      "Training epoch 3090 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3090 ; accuracy: 0.74; loss: 2.042433500289917\n",
      "Training epoch 3091 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3091 ; accuracy: 0.74; loss: 2.042496919631958\n",
      "Training epoch 3092 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3092 ; accuracy: 0.74; loss: 2.0425591468811035\n",
      "Training epoch 3093 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3093 ; accuracy: 0.74; loss: 2.0426180362701416\n",
      "Training epoch 3094 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3094 ; accuracy: 0.74; loss: 2.0426747798919678\n",
      "Training epoch 3095 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3095 ; accuracy: 0.74; loss: 2.0427355766296387\n",
      "Training epoch 3096 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3096 ; accuracy: 0.74; loss: 2.042797565460205\n",
      "Training epoch 3097 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3097 ; accuracy: 0.74; loss: 2.042863607406616\n",
      "Training epoch 3098 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3098 ; accuracy: 0.74; loss: 2.0429294109344482\n",
      "Training epoch 3099 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3099 ; accuracy: 0.74; loss: 2.042990207672119\n",
      "Training epoch 3100 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3100 ; accuracy: 0.74; loss: 2.0430526733398438\n",
      "Training epoch 3101 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3101 ; accuracy: 0.74; loss: 2.043114185333252\n",
      "Training epoch 3102 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3102 ; accuracy: 0.74; loss: 2.043177604675293\n",
      "Training epoch 3103 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3103 ; accuracy: 0.74; loss: 2.0432345867156982\n",
      "Training epoch 3104 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3104 ; accuracy: 0.74; loss: 2.043297529220581\n",
      "Training epoch 3105 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3105 ; accuracy: 0.74; loss: 2.043360710144043\n",
      "Training epoch 3106 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3106 ; accuracy: 0.74; loss: 2.043421506881714\n",
      "Training epoch 3107 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3107 ; accuracy: 0.74; loss: 2.0434837341308594\n",
      "Training epoch 3108 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3108 ; accuracy: 0.74; loss: 2.043541669845581\n",
      "Training epoch 3109 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3109 ; accuracy: 0.74; loss: 2.0436010360717773\n",
      "Training epoch 3110 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3110 ; accuracy: 0.74; loss: 2.043668746948242\n",
      "Training epoch 3111 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3111 ; accuracy: 0.74; loss: 2.043727397918701\n",
      "Training epoch 3112 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3112 ; accuracy: 0.74; loss: 2.043790340423584\n",
      "Training epoch 3113 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3113 ; accuracy: 0.74; loss: 2.0438551902770996\n",
      "Training epoch 3114 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 3114 ; accuracy: 0.74; loss: 2.043972969055176\n",
      "Training epoch 3115 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3115 ; accuracy: 0.74; loss: 2.0440938472747803\n",
      "Training epoch 3116 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3116 ; accuracy: 0.74; loss: 2.0442051887512207\n",
      "Training epoch 3117 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 3117 ; accuracy: 0.74; loss: 2.0442776679992676\n",
      "Training epoch 3118 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3118 ; accuracy: 0.74; loss: 2.0443522930145264\n",
      "Training epoch 3119 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3119 ; accuracy: 0.74; loss: 2.04443621635437\n",
      "Training epoch 3120 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3120 ; accuracy: 0.74; loss: 2.0445165634155273\n",
      "Training epoch 3121 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3121 ; accuracy: 0.74; loss: 2.0445966720581055\n",
      "Training epoch 3122 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3122 ; accuracy: 0.74; loss: 2.0446741580963135\n",
      "Training epoch 3123 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3123 ; accuracy: 0.74; loss: 2.0447492599487305\n",
      "Training epoch 3124 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3124 ; accuracy: 0.74; loss: 2.044821262359619\n",
      "Training epoch 3125 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3125 ; accuracy: 0.74; loss: 2.044895648956299\n",
      "Training epoch 3126 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3126 ; accuracy: 0.74; loss: 2.044975519180298\n",
      "Training epoch 3127 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3127 ; accuracy: 0.74; loss: 2.0450594425201416\n",
      "Training epoch 3128 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3128 ; accuracy: 0.74; loss: 2.045144557952881\n",
      "Training epoch 3129 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3129 ; accuracy: 0.74; loss: 2.0452258586883545\n",
      "Training epoch 3130 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3130 ; accuracy: 0.74; loss: 2.0453081130981445\n",
      "Training epoch 3131 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3131 ; accuracy: 0.74; loss: 2.0453858375549316\n",
      "Training epoch 3132 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3132 ; accuracy: 0.74; loss: 2.045474052429199\n",
      "Training epoch 3133 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3133 ; accuracy: 0.74; loss: 2.0455691814422607\n",
      "Training epoch 3134 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3134 ; accuracy: 0.74; loss: 2.045665979385376\n",
      "Training epoch 3135 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3135 ; accuracy: 0.74; loss: 2.0457725524902344\n",
      "Training epoch 3136 ; accuracy: 0.9; loss: 0.1945912092924118\n",
      "Validation epoch 3136 ; accuracy: 0.74; loss: 2.045912981033325\n",
      "Training epoch 3137 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3137 ; accuracy: 0.74; loss: 2.0460495948791504\n",
      "Training epoch 3138 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3138 ; accuracy: 0.74; loss: 2.0461833477020264\n",
      "Training epoch 3139 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3139 ; accuracy: 0.74; loss: 2.046307325363159\n",
      "Training epoch 3140 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3140 ; accuracy: 0.74; loss: 2.0464298725128174\n",
      "Training epoch 3141 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3141 ; accuracy: 0.74; loss: 2.04654860496521\n",
      "Training epoch 3142 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3142 ; accuracy: 0.74; loss: 2.046660900115967\n",
      "Training epoch 3143 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3143 ; accuracy: 0.74; loss: 2.0467705726623535\n",
      "Training epoch 3144 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 3144 ; accuracy: 0.74; loss: 2.046959161758423\n",
      "Training epoch 3145 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3145 ; accuracy: 0.74; loss: 2.047142744064331\n",
      "Training epoch 3146 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3146 ; accuracy: 0.74; loss: 2.0473413467407227\n",
      "Training epoch 3147 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3147 ; accuracy: 0.74; loss: 2.0475220680236816\n",
      "Training epoch 3148 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3148 ; accuracy: 0.74; loss: 2.0476863384246826\n",
      "Training epoch 3149 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3149 ; accuracy: 0.74; loss: 2.047839641571045\n",
      "Training epoch 3150 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3150 ; accuracy: 0.74; loss: 2.0479812622070312\n",
      "Training epoch 3151 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3151 ; accuracy: 0.74; loss: 2.048110008239746\n",
      "Training epoch 3152 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3152 ; accuracy: 0.74; loss: 2.048229455947876\n",
      "Training epoch 3153 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3153 ; accuracy: 0.74; loss: 2.048342227935791\n",
      "Training epoch 3154 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3154 ; accuracy: 0.74; loss: 2.048457145690918\n",
      "Training epoch 3155 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3155 ; accuracy: 0.74; loss: 2.0485668182373047\n",
      "Training epoch 3156 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3156 ; accuracy: 0.74; loss: 2.0486674308776855\n",
      "Training epoch 3157 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3157 ; accuracy: 0.74; loss: 2.048767328262329\n",
      "Training epoch 3158 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3158 ; accuracy: 0.7366666666666667; loss: 2.0488662719726562\n",
      "Training epoch 3159 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3159 ; accuracy: 0.7366666666666667; loss: 2.0489633083343506\n",
      "Training epoch 3160 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3160 ; accuracy: 0.7366666666666667; loss: 2.0490493774414062\n",
      "Training epoch 3161 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3161 ; accuracy: 0.7366666666666667; loss: 2.049138069152832\n",
      "Training epoch 3162 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3162 ; accuracy: 0.7366666666666667; loss: 2.049224376678467\n",
      "Training epoch 3163 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3163 ; accuracy: 0.7366666666666667; loss: 2.0493083000183105\n",
      "Training epoch 3164 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3164 ; accuracy: 0.7366666666666667; loss: 2.049395799636841\n",
      "Training epoch 3165 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3165 ; accuracy: 0.7366666666666667; loss: 2.0494801998138428\n",
      "Training epoch 3166 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3166 ; accuracy: 0.7366666666666667; loss: 2.0495667457580566\n",
      "Training epoch 3167 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3167 ; accuracy: 0.7366666666666667; loss: 2.0496504306793213\n",
      "Training epoch 3168 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3168 ; accuracy: 0.7366666666666667; loss: 2.049734592437744\n",
      "Training epoch 3169 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3169 ; accuracy: 0.7366666666666667; loss: 2.0498080253601074\n",
      "Training epoch 3170 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3170 ; accuracy: 0.7366666666666667; loss: 2.049912214279175\n",
      "Training epoch 3171 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3171 ; accuracy: 0.7366666666666667; loss: 2.0500104427337646\n",
      "Training epoch 3172 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3172 ; accuracy: 0.7366666666666667; loss: 2.0501060485839844\n",
      "Training epoch 3173 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3173 ; accuracy: 0.7366666666666667; loss: 2.0502026081085205\n",
      "Training epoch 3174 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3174 ; accuracy: 0.7366666666666667; loss: 2.0502943992614746\n",
      "Training epoch 3175 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3175 ; accuracy: 0.7366666666666667; loss: 2.0503907203674316\n",
      "Training epoch 3176 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3176 ; accuracy: 0.7366666666666667; loss: 2.0504891872406006\n",
      "Training epoch 3177 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3177 ; accuracy: 0.7366666666666667; loss: 2.0505917072296143\n",
      "Training epoch 3178 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3178 ; accuracy: 0.7366666666666667; loss: 2.050680637359619\n",
      "Training epoch 3179 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3179 ; accuracy: 0.7366666666666667; loss: 2.0507593154907227\n",
      "Training epoch 3180 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3180 ; accuracy: 0.7366666666666667; loss: 2.050837755203247\n",
      "Training epoch 3181 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3181 ; accuracy: 0.7366666666666667; loss: 2.050912857055664\n",
      "Training epoch 3182 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3182 ; accuracy: 0.7366666666666667; loss: 2.050983190536499\n",
      "Training epoch 3183 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3183 ; accuracy: 0.7366666666666667; loss: 2.0510497093200684\n",
      "Training epoch 3184 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3184 ; accuracy: 0.7366666666666667; loss: 2.0511114597320557\n",
      "Training epoch 3185 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3185 ; accuracy: 0.7366666666666667; loss: 2.0511789321899414\n",
      "Training epoch 3186 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3186 ; accuracy: 0.7366666666666667; loss: 2.051246404647827\n",
      "Training epoch 3187 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3187 ; accuracy: 0.7366666666666667; loss: 2.0513105392456055\n",
      "Training epoch 3188 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3188 ; accuracy: 0.7366666666666667; loss: 2.0513854026794434\n",
      "Training epoch 3189 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3189 ; accuracy: 0.74; loss: 2.051460027694702\n",
      "Training epoch 3190 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3190 ; accuracy: 0.74; loss: 2.0515353679656982\n",
      "Training epoch 3191 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3191 ; accuracy: 0.74; loss: 2.0516083240509033\n",
      "Training epoch 3192 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3192 ; accuracy: 0.74; loss: 2.0516793727874756\n",
      "Training epoch 3193 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3193 ; accuracy: 0.74; loss: 2.051751136779785\n",
      "Training epoch 3194 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3194 ; accuracy: 0.74; loss: 2.051828622817993\n",
      "Training epoch 3195 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3195 ; accuracy: 0.74; loss: 2.051905632019043\n",
      "Training epoch 3196 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3196 ; accuracy: 0.74; loss: 2.051969289779663\n",
      "Training epoch 3197 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3197 ; accuracy: 0.74; loss: 2.052034616470337\n",
      "Training epoch 3198 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3198 ; accuracy: 0.74; loss: 2.0520997047424316\n",
      "Training epoch 3199 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3199 ; accuracy: 0.74; loss: 2.052166223526001\n",
      "Training epoch 3200 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3200 ; accuracy: 0.74; loss: 2.0522358417510986\n",
      "Training epoch 3201 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3201 ; accuracy: 0.74; loss: 2.05230975151062\n",
      "Training epoch 3202 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3202 ; accuracy: 0.74; loss: 2.0523808002471924\n",
      "Training epoch 3203 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3203 ; accuracy: 0.74; loss: 2.052449941635132\n",
      "Training epoch 3204 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3204 ; accuracy: 0.74; loss: 2.0525197982788086\n",
      "Training epoch 3205 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3205 ; accuracy: 0.74; loss: 2.0525879859924316\n",
      "Training epoch 3206 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3206 ; accuracy: 0.74; loss: 2.0526626110076904\n",
      "Training epoch 3207 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3207 ; accuracy: 0.74; loss: 2.052734851837158\n",
      "Training epoch 3208 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3208 ; accuracy: 0.74; loss: 2.052807569503784\n",
      "Training epoch 3209 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3209 ; accuracy: 0.74; loss: 2.052886724472046\n",
      "Training epoch 3210 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3210 ; accuracy: 0.74; loss: 2.0529704093933105\n",
      "Training epoch 3211 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3211 ; accuracy: 0.74; loss: 2.0530476570129395\n",
      "Training epoch 3212 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3212 ; accuracy: 0.74; loss: 2.0531234741210938\n",
      "Training epoch 3213 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3213 ; accuracy: 0.74; loss: 2.053201198577881\n",
      "Training epoch 3214 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3214 ; accuracy: 0.74; loss: 2.0532732009887695\n",
      "Training epoch 3215 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3215 ; accuracy: 0.74; loss: 2.053351879119873\n",
      "Training epoch 3216 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3216 ; accuracy: 0.74; loss: 2.0534234046936035\n",
      "Training epoch 3217 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3217 ; accuracy: 0.74; loss: 2.0534932613372803\n",
      "Training epoch 3218 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3218 ; accuracy: 0.74; loss: 2.0535404682159424\n",
      "Training epoch 3219 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3219 ; accuracy: 0.74; loss: 2.0535953044891357\n",
      "Training epoch 3220 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3220 ; accuracy: 0.74; loss: 2.053647756576538\n",
      "Training epoch 3221 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3221 ; accuracy: 0.74; loss: 2.053699493408203\n",
      "Training epoch 3222 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3222 ; accuracy: 0.74; loss: 2.0537683963775635\n",
      "Training epoch 3223 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3223 ; accuracy: 0.74; loss: 2.0538341999053955\n",
      "Training epoch 3224 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3224 ; accuracy: 0.74; loss: 2.053919792175293\n",
      "Training epoch 3225 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3225 ; accuracy: 0.74; loss: 2.054007053375244\n",
      "Training epoch 3226 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3226 ; accuracy: 0.74; loss: 2.0540878772735596\n",
      "Training epoch 3227 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3227 ; accuracy: 0.74; loss: 2.054163694381714\n",
      "Training epoch 3228 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3228 ; accuracy: 0.74; loss: 2.054230213165283\n",
      "Training epoch 3229 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3229 ; accuracy: 0.74; loss: 2.0542969703674316\n",
      "Training epoch 3230 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3230 ; accuracy: 0.74; loss: 2.0543649196624756\n",
      "Training epoch 3231 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3231 ; accuracy: 0.74; loss: 2.0544309616088867\n",
      "Training epoch 3232 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3232 ; accuracy: 0.74; loss: 2.0544981956481934\n",
      "Training epoch 3233 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3233 ; accuracy: 0.74; loss: 2.0545647144317627\n",
      "Training epoch 3234 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3234 ; accuracy: 0.74; loss: 2.054628610610962\n",
      "Training epoch 3235 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3235 ; accuracy: 0.74; loss: 2.054682970046997\n",
      "Training epoch 3236 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3236 ; accuracy: 0.74; loss: 2.0547401905059814\n",
      "Training epoch 3237 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3237 ; accuracy: 0.74; loss: 2.0547780990600586\n",
      "Training epoch 3238 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3238 ; accuracy: 0.74; loss: 2.054830551147461\n",
      "Training epoch 3239 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3239 ; accuracy: 0.74; loss: 2.054885149002075\n",
      "Training epoch 3240 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3240 ; accuracy: 0.74; loss: 2.0549392700195312\n",
      "Training epoch 3241 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3241 ; accuracy: 0.74; loss: 2.054997444152832\n",
      "Training epoch 3242 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3242 ; accuracy: 0.74; loss: 2.055058479309082\n",
      "Training epoch 3243 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3243 ; accuracy: 0.74; loss: 2.055117607116699\n",
      "Training epoch 3244 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3244 ; accuracy: 0.74; loss: 2.055168867111206\n",
      "Training epoch 3245 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3245 ; accuracy: 0.74; loss: 2.055225133895874\n",
      "Training epoch 3246 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3246 ; accuracy: 0.74; loss: 2.0552902221679688\n",
      "Training epoch 3247 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3247 ; accuracy: 0.74; loss: 2.0553464889526367\n",
      "Training epoch 3248 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 3248 ; accuracy: 0.74; loss: 2.0554492473602295\n",
      "Training epoch 3249 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3249 ; accuracy: 0.74; loss: 2.055551290512085\n",
      "Training epoch 3250 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3250 ; accuracy: 0.74; loss: 2.0556459426879883\n",
      "Training epoch 3251 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3251 ; accuracy: 0.74; loss: 2.055736780166626\n",
      "Training epoch 3252 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3252 ; accuracy: 0.74; loss: 2.0558128356933594\n",
      "Training epoch 3253 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3253 ; accuracy: 0.74; loss: 2.0558860301971436\n",
      "Training epoch 3254 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3254 ; accuracy: 0.74; loss: 2.0559561252593994\n",
      "Training epoch 3255 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3255 ; accuracy: 0.74; loss: 2.0560238361358643\n",
      "Training epoch 3256 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3256 ; accuracy: 0.74; loss: 2.0560905933380127\n",
      "Training epoch 3257 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3257 ; accuracy: 0.74; loss: 2.0561604499816895\n",
      "Training epoch 3258 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3258 ; accuracy: 0.74; loss: 2.05623459815979\n",
      "Training epoch 3259 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3259 ; accuracy: 0.74; loss: 2.0563085079193115\n",
      "Training epoch 3260 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3260 ; accuracy: 0.74; loss: 2.0563764572143555\n",
      "Training epoch 3261 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3261 ; accuracy: 0.74; loss: 2.0564420223236084\n",
      "Training epoch 3262 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3262 ; accuracy: 0.74; loss: 2.056504249572754\n",
      "Training epoch 3263 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3263 ; accuracy: 0.74; loss: 2.056572198867798\n",
      "Training epoch 3264 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3264 ; accuracy: 0.74; loss: 2.056658983230591\n",
      "Training epoch 3265 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3265 ; accuracy: 0.74; loss: 2.056746006011963\n",
      "Training epoch 3266 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3266 ; accuracy: 0.74; loss: 2.0568249225616455\n",
      "Training epoch 3267 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3267 ; accuracy: 0.74; loss: 2.056910991668701\n",
      "Training epoch 3268 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3268 ; accuracy: 0.74; loss: 2.056983709335327\n",
      "Training epoch 3269 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3269 ; accuracy: 0.74; loss: 2.0570573806762695\n",
      "Training epoch 3270 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3270 ; accuracy: 0.74; loss: 2.0571300983428955\n",
      "Training epoch 3271 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3271 ; accuracy: 0.74; loss: 2.057201623916626\n",
      "Training epoch 3272 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3272 ; accuracy: 0.74; loss: 2.057274580001831\n",
      "Training epoch 3273 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3273 ; accuracy: 0.74; loss: 2.0573477745056152\n",
      "Training epoch 3274 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3274 ; accuracy: 0.74; loss: 2.0574193000793457\n",
      "Training epoch 3275 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3275 ; accuracy: 0.74; loss: 2.0574917793273926\n",
      "Training epoch 3276 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3276 ; accuracy: 0.74; loss: 2.0575664043426514\n",
      "Training epoch 3277 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3277 ; accuracy: 0.74; loss: 2.0576419830322266\n",
      "Training epoch 3278 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3278 ; accuracy: 0.74; loss: 2.057713270187378\n",
      "Training epoch 3279 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3279 ; accuracy: 0.74; loss: 2.0577831268310547\n",
      "Training epoch 3280 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3280 ; accuracy: 0.74; loss: 2.0578556060791016\n",
      "Training epoch 3281 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3281 ; accuracy: 0.74; loss: 2.057922124862671\n",
      "Training epoch 3282 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3282 ; accuracy: 0.74; loss: 2.0579965114593506\n",
      "Training epoch 3283 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3283 ; accuracy: 0.74; loss: 2.0580708980560303\n",
      "Training epoch 3284 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3284 ; accuracy: 0.74; loss: 2.0581436157226562\n",
      "Training epoch 3285 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3285 ; accuracy: 0.74; loss: 2.058213233947754\n",
      "Training epoch 3286 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3286 ; accuracy: 0.74; loss: 2.058281660079956\n",
      "Training epoch 3287 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3287 ; accuracy: 0.74; loss: 2.058345317840576\n",
      "Training epoch 3288 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3288 ; accuracy: 0.74; loss: 2.058407783508301\n",
      "Training epoch 3289 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3289 ; accuracy: 0.74; loss: 2.0584628582000732\n",
      "Training epoch 3290 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3290 ; accuracy: 0.74; loss: 2.0585169792175293\n",
      "Training epoch 3291 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3291 ; accuracy: 0.74; loss: 2.0585777759552\n",
      "Training epoch 3292 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3292 ; accuracy: 0.74; loss: 2.0586295127868652\n",
      "Training epoch 3293 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3293 ; accuracy: 0.74; loss: 2.0586767196655273\n",
      "Training epoch 3294 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3294 ; accuracy: 0.74; loss: 2.0587258338928223\n",
      "Training epoch 3295 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3295 ; accuracy: 0.74; loss: 2.0587735176086426\n",
      "Training epoch 3296 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3296 ; accuracy: 0.74; loss: 2.058818817138672\n",
      "Training epoch 3297 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3297 ; accuracy: 0.74; loss: 2.0588676929473877\n",
      "Training epoch 3298 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3298 ; accuracy: 0.74; loss: 2.058920383453369\n",
      "Training epoch 3299 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3299 ; accuracy: 0.74; loss: 2.0589845180511475\n",
      "Training epoch 3300 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3300 ; accuracy: 0.74; loss: 2.059048891067505\n",
      "Training epoch 3301 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3301 ; accuracy: 0.74; loss: 2.0591025352478027\n",
      "Training epoch 3302 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3302 ; accuracy: 0.74; loss: 2.059159278869629\n",
      "Training epoch 3303 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3303 ; accuracy: 0.74; loss: 2.0592236518859863\n",
      "Training epoch 3304 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3304 ; accuracy: 0.74; loss: 2.059290885925293\n",
      "Training epoch 3305 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3305 ; accuracy: 0.74; loss: 2.0593574047088623\n",
      "Training epoch 3306 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3306 ; accuracy: 0.74; loss: 2.0594286918640137\n",
      "Training epoch 3307 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3307 ; accuracy: 0.74; loss: 2.0594987869262695\n",
      "Training epoch 3308 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3308 ; accuracy: 0.74; loss: 2.0595691204071045\n",
      "Training epoch 3309 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3309 ; accuracy: 0.74; loss: 2.059644937515259\n",
      "Training epoch 3310 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3310 ; accuracy: 0.74; loss: 2.0597152709960938\n",
      "Training epoch 3311 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3311 ; accuracy: 0.74; loss: 2.059788227081299\n",
      "Training epoch 3312 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3312 ; accuracy: 0.74; loss: 2.0598697662353516\n",
      "Training epoch 3313 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3313 ; accuracy: 0.74; loss: 2.059957265853882\n",
      "Training epoch 3314 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3314 ; accuracy: 0.74; loss: 2.0600411891937256\n",
      "Training epoch 3315 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3315 ; accuracy: 0.74; loss: 2.060121774673462\n",
      "Training epoch 3316 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3316 ; accuracy: 0.74; loss: 2.0602023601531982\n",
      "Training epoch 3317 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3317 ; accuracy: 0.74; loss: 2.060276746749878\n",
      "Training epoch 3318 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3318 ; accuracy: 0.74; loss: 2.0603528022766113\n",
      "Training epoch 3319 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3319 ; accuracy: 0.74; loss: 2.060431480407715\n",
      "Training epoch 3320 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3320 ; accuracy: 0.74; loss: 2.0605063438415527\n",
      "Training epoch 3321 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3321 ; accuracy: 0.74; loss: 2.0605814456939697\n",
      "Training epoch 3322 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3322 ; accuracy: 0.74; loss: 2.060657262802124\n",
      "Training epoch 3323 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3323 ; accuracy: 0.74; loss: 2.06072998046875\n",
      "Training epoch 3324 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3324 ; accuracy: 0.74; loss: 2.0607969760894775\n",
      "Training epoch 3325 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3325 ; accuracy: 0.74; loss: 2.06086802482605\n",
      "Training epoch 3326 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3326 ; accuracy: 0.74; loss: 2.0609381198883057\n",
      "Training epoch 3327 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3327 ; accuracy: 0.74; loss: 2.060988664627075\n",
      "Training epoch 3328 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3328 ; accuracy: 0.74; loss: 2.0610415935516357\n",
      "Training epoch 3329 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3329 ; accuracy: 0.74; loss: 2.06109356880188\n",
      "Training epoch 3330 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3330 ; accuracy: 0.74; loss: 2.061154842376709\n",
      "Training epoch 3331 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3331 ; accuracy: 0.74; loss: 2.061220407485962\n",
      "Training epoch 3332 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3332 ; accuracy: 0.74; loss: 2.0612897872924805\n",
      "Training epoch 3333 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3333 ; accuracy: 0.74; loss: 2.061358690261841\n",
      "Training epoch 3334 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3334 ; accuracy: 0.74; loss: 2.0614266395568848\n",
      "Training epoch 3335 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3335 ; accuracy: 0.74; loss: 2.061499834060669\n",
      "Training epoch 3336 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3336 ; accuracy: 0.74; loss: 2.0615696907043457\n",
      "Training epoch 3337 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3337 ; accuracy: 0.74; loss: 2.061643600463867\n",
      "Training epoch 3338 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3338 ; accuracy: 0.74; loss: 2.0617146492004395\n",
      "Training epoch 3339 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3339 ; accuracy: 0.74; loss: 2.0617830753326416\n",
      "Training epoch 3340 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3340 ; accuracy: 0.74; loss: 2.0618484020233154\n",
      "Training epoch 3341 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3341 ; accuracy: 0.74; loss: 2.0619091987609863\n",
      "Training epoch 3342 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3342 ; accuracy: 0.74; loss: 2.0619804859161377\n",
      "Training epoch 3343 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3343 ; accuracy: 0.74; loss: 2.0620484352111816\n",
      "Training epoch 3344 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3344 ; accuracy: 0.74; loss: 2.0621163845062256\n",
      "Training epoch 3345 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3345 ; accuracy: 0.74; loss: 2.0621869564056396\n",
      "Training epoch 3346 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3346 ; accuracy: 0.74; loss: 2.0622572898864746\n",
      "Training epoch 3347 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3347 ; accuracy: 0.74; loss: 2.0623323917388916\n",
      "Training epoch 3348 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3348 ; accuracy: 0.74; loss: 2.062411308288574\n",
      "Training epoch 3349 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3349 ; accuracy: 0.74; loss: 2.062487840652466\n",
      "Training epoch 3350 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3350 ; accuracy: 0.74; loss: 2.0625579357147217\n",
      "Training epoch 3351 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3351 ; accuracy: 0.74; loss: 2.0626296997070312\n",
      "Training epoch 3352 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3352 ; accuracy: 0.74; loss: 2.0627057552337646\n",
      "Training epoch 3353 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3353 ; accuracy: 0.74; loss: 2.062779426574707\n",
      "Training epoch 3354 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3354 ; accuracy: 0.74; loss: 2.0628502368927\n",
      "Training epoch 3355 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3355 ; accuracy: 0.74; loss: 2.0629541873931885\n",
      "Training epoch 3356 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3356 ; accuracy: 0.74; loss: 2.063053846359253\n",
      "Training epoch 3357 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3357 ; accuracy: 0.74; loss: 2.063152313232422\n",
      "Training epoch 3358 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3358 ; accuracy: 0.74; loss: 2.063244104385376\n",
      "Training epoch 3359 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3359 ; accuracy: 0.74; loss: 2.063331365585327\n",
      "Training epoch 3360 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3360 ; accuracy: 0.74; loss: 2.06341814994812\n",
      "Training epoch 3361 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3361 ; accuracy: 0.74; loss: 2.0635063648223877\n",
      "Training epoch 3362 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3362 ; accuracy: 0.74; loss: 2.063594102859497\n",
      "Training epoch 3363 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3363 ; accuracy: 0.74; loss: 2.0636773109436035\n",
      "Training epoch 3364 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3364 ; accuracy: 0.74; loss: 2.063760280609131\n",
      "Training epoch 3365 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3365 ; accuracy: 0.74; loss: 2.063844680786133\n",
      "Training epoch 3366 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3366 ; accuracy: 0.74; loss: 2.0639266967773438\n",
      "Training epoch 3367 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3367 ; accuracy: 0.74; loss: 2.0640087127685547\n",
      "Training epoch 3368 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3368 ; accuracy: 0.74; loss: 2.0640859603881836\n",
      "Training epoch 3369 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3369 ; accuracy: 0.74; loss: 2.0641772747039795\n",
      "Training epoch 3370 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3370 ; accuracy: 0.74; loss: 2.064267158508301\n",
      "Training epoch 3371 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3371 ; accuracy: 0.74; loss: 2.064356803894043\n",
      "Training epoch 3372 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3372 ; accuracy: 0.74; loss: 2.0644378662109375\n",
      "Training epoch 3373 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3373 ; accuracy: 0.74; loss: 2.0645148754119873\n",
      "Training epoch 3374 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3374 ; accuracy: 0.74; loss: 2.064582586288452\n",
      "Training epoch 3375 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3375 ; accuracy: 0.74; loss: 2.06465220451355\n",
      "Training epoch 3376 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3376 ; accuracy: 0.74; loss: 2.0647242069244385\n",
      "Training epoch 3377 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3377 ; accuracy: 0.74; loss: 2.0647943019866943\n",
      "Training epoch 3378 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3378 ; accuracy: 0.74; loss: 2.06485652923584\n",
      "Training epoch 3379 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3379 ; accuracy: 0.74; loss: 2.0649220943450928\n",
      "Training epoch 3380 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3380 ; accuracy: 0.74; loss: 2.0649983882904053\n",
      "Training epoch 3381 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3381 ; accuracy: 0.74; loss: 2.065117597579956\n",
      "Training epoch 3382 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3382 ; accuracy: 0.74; loss: 2.065239906311035\n",
      "Training epoch 3383 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3383 ; accuracy: 0.74; loss: 2.065349817276001\n",
      "Training epoch 3384 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3384 ; accuracy: 0.74; loss: 2.0654752254486084\n",
      "Training epoch 3385 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3385 ; accuracy: 0.74; loss: 2.065598487854004\n",
      "Training epoch 3386 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3386 ; accuracy: 0.74; loss: 2.065711259841919\n",
      "Training epoch 3387 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3387 ; accuracy: 0.74; loss: 2.0658187866210938\n",
      "Training epoch 3388 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3388 ; accuracy: 0.74; loss: 2.0659263134002686\n",
      "Training epoch 3389 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3389 ; accuracy: 0.74; loss: 2.066026449203491\n",
      "Training epoch 3390 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3390 ; accuracy: 0.74; loss: 2.0661239624023438\n",
      "Training epoch 3391 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3391 ; accuracy: 0.74; loss: 2.0662224292755127\n",
      "Training epoch 3392 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3392 ; accuracy: 0.74; loss: 2.066318988800049\n",
      "Training epoch 3393 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3393 ; accuracy: 0.74; loss: 2.0664103031158447\n",
      "Training epoch 3394 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3394 ; accuracy: 0.74; loss: 2.066495656967163\n",
      "Training epoch 3395 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3395 ; accuracy: 0.74; loss: 2.066591739654541\n",
      "Training epoch 3396 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3396 ; accuracy: 0.74; loss: 2.06668758392334\n",
      "Training epoch 3397 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3397 ; accuracy: 0.74; loss: 2.0667712688446045\n",
      "Training epoch 3398 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3398 ; accuracy: 0.74; loss: 2.0668561458587646\n",
      "Training epoch 3399 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3399 ; accuracy: 0.74; loss: 2.066937208175659\n",
      "Training epoch 3400 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3400 ; accuracy: 0.74; loss: 2.0670149326324463\n",
      "Training epoch 3401 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3401 ; accuracy: 0.74; loss: 2.0670907497406006\n",
      "Training epoch 3402 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3402 ; accuracy: 0.74; loss: 2.0671768188476562\n",
      "Training epoch 3403 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3403 ; accuracy: 0.74; loss: 2.067258596420288\n",
      "Training epoch 3404 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3404 ; accuracy: 0.74; loss: 2.06734037399292\n",
      "Training epoch 3405 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3405 ; accuracy: 0.74; loss: 2.0674219131469727\n",
      "Training epoch 3406 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3406 ; accuracy: 0.74; loss: 2.0674984455108643\n",
      "Training epoch 3407 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3407 ; accuracy: 0.74; loss: 2.0675721168518066\n",
      "Training epoch 3408 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3408 ; accuracy: 0.74; loss: 2.067641258239746\n",
      "Training epoch 3409 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3409 ; accuracy: 0.74; loss: 2.067706823348999\n",
      "Training epoch 3410 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3410 ; accuracy: 0.74; loss: 2.0677742958068848\n",
      "Training epoch 3411 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3411 ; accuracy: 0.74; loss: 2.0678319931030273\n",
      "Training epoch 3412 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3412 ; accuracy: 0.74; loss: 2.0678865909576416\n",
      "Training epoch 3413 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3413 ; accuracy: 0.74; loss: 2.067927360534668\n",
      "Training epoch 3414 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3414 ; accuracy: 0.74; loss: 2.0679688453674316\n",
      "Training epoch 3415 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3415 ; accuracy: 0.74; loss: 2.0680108070373535\n",
      "Training epoch 3416 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3416 ; accuracy: 0.74; loss: 2.0680549144744873\n",
      "Training epoch 3417 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3417 ; accuracy: 0.74; loss: 2.06809401512146\n",
      "Training epoch 3418 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3418 ; accuracy: 0.74; loss: 2.068138599395752\n",
      "Training epoch 3419 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3419 ; accuracy: 0.74; loss: 2.0681915283203125\n",
      "Training epoch 3420 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3420 ; accuracy: 0.74; loss: 2.0682263374328613\n",
      "Training epoch 3421 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3421 ; accuracy: 0.74; loss: 2.0682685375213623\n",
      "Training epoch 3422 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3422 ; accuracy: 0.74; loss: 2.068314790725708\n",
      "Training epoch 3423 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3423 ; accuracy: 0.74; loss: 2.0683629512786865\n",
      "Training epoch 3424 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3424 ; accuracy: 0.74; loss: 2.0684094429016113\n",
      "Training epoch 3425 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3425 ; accuracy: 0.74; loss: 2.068455457687378\n",
      "Training epoch 3426 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3426 ; accuracy: 0.74; loss: 2.0685014724731445\n",
      "Training epoch 3427 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3427 ; accuracy: 0.74; loss: 2.0685486793518066\n",
      "Training epoch 3428 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3428 ; accuracy: 0.74; loss: 2.0685973167419434\n",
      "Training epoch 3429 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3429 ; accuracy: 0.74; loss: 2.06864333152771\n",
      "Training epoch 3430 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3430 ; accuracy: 0.74; loss: 2.068692922592163\n",
      "Training epoch 3431 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3431 ; accuracy: 0.74; loss: 2.0687811374664307\n",
      "Training epoch 3432 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3432 ; accuracy: 0.74; loss: 2.0688538551330566\n",
      "Training epoch 3433 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3433 ; accuracy: 0.74; loss: 2.0689218044281006\n",
      "Training epoch 3434 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3434 ; accuracy: 0.74; loss: 2.068985939025879\n",
      "Training epoch 3435 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3435 ; accuracy: 0.74; loss: 2.0690560340881348\n",
      "Training epoch 3436 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3436 ; accuracy: 0.74; loss: 2.0691354274749756\n",
      "Training epoch 3437 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3437 ; accuracy: 0.74; loss: 2.0692107677459717\n",
      "Training epoch 3438 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3438 ; accuracy: 0.74; loss: 2.069291353225708\n",
      "Training epoch 3439 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3439 ; accuracy: 0.74; loss: 2.069371223449707\n",
      "Training epoch 3440 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3440 ; accuracy: 0.74; loss: 2.0694713592529297\n",
      "Training epoch 3441 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3441 ; accuracy: 0.74; loss: 2.0695674419403076\n",
      "Training epoch 3442 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3442 ; accuracy: 0.74; loss: 2.0696585178375244\n",
      "Training epoch 3443 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3443 ; accuracy: 0.74; loss: 2.0697500705718994\n",
      "Training epoch 3444 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3444 ; accuracy: 0.74; loss: 2.0698351860046387\n",
      "Training epoch 3445 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3445 ; accuracy: 0.74; loss: 2.0699164867401123\n",
      "Training epoch 3446 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3446 ; accuracy: 0.74; loss: 2.069993495941162\n",
      "Training epoch 3447 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3447 ; accuracy: 0.74; loss: 2.0700697898864746\n",
      "Training epoch 3448 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3448 ; accuracy: 0.74; loss: 2.0701465606689453\n",
      "Training epoch 3449 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3449 ; accuracy: 0.74; loss: 2.070220470428467\n",
      "Training epoch 3450 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3450 ; accuracy: 0.74; loss: 2.0702931880950928\n",
      "Training epoch 3451 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3451 ; accuracy: 0.74; loss: 2.070366382598877\n",
      "Training epoch 3452 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3452 ; accuracy: 0.74; loss: 2.070439338684082\n",
      "Training epoch 3453 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3453 ; accuracy: 0.74; loss: 2.070509433746338\n",
      "Training epoch 3454 ; accuracy: 0.9; loss: 0.194591224193573\n",
      "Validation epoch 3454 ; accuracy: 0.74; loss: 2.0706300735473633\n",
      "Training epoch 3455 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3455 ; accuracy: 0.74; loss: 2.070740222930908\n",
      "Training epoch 3456 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3456 ; accuracy: 0.74; loss: 2.070858955383301\n",
      "Training epoch 3457 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3457 ; accuracy: 0.74; loss: 2.0709781646728516\n",
      "Training epoch 3458 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3458 ; accuracy: 0.74; loss: 2.071120023727417\n",
      "Training epoch 3459 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3459 ; accuracy: 0.74; loss: 2.0712506771087646\n",
      "Training epoch 3460 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3460 ; accuracy: 0.74; loss: 2.071377754211426\n",
      "Training epoch 3461 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3461 ; accuracy: 0.74; loss: 2.0714969635009766\n",
      "Training epoch 3462 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3462 ; accuracy: 0.74; loss: 2.0716052055358887\n",
      "Training epoch 3463 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3463 ; accuracy: 0.74; loss: 2.071712017059326\n",
      "Training epoch 3464 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3464 ; accuracy: 0.74; loss: 2.071812152862549\n",
      "Training epoch 3465 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3465 ; accuracy: 0.74; loss: 2.0718986988067627\n",
      "Training epoch 3466 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3466 ; accuracy: 0.74; loss: 2.0719757080078125\n",
      "Training epoch 3467 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3467 ; accuracy: 0.74; loss: 2.0720479488372803\n",
      "Training epoch 3468 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3468 ; accuracy: 0.74; loss: 2.0721189975738525\n",
      "Training epoch 3469 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3469 ; accuracy: 0.74; loss: 2.072188377380371\n",
      "Training epoch 3470 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3470 ; accuracy: 0.74; loss: 2.072254180908203\n",
      "Training epoch 3471 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3471 ; accuracy: 0.74; loss: 2.0723164081573486\n",
      "Training epoch 3472 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3472 ; accuracy: 0.74; loss: 2.0723845958709717\n",
      "Training epoch 3473 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3473 ; accuracy: 0.74; loss: 2.072458267211914\n",
      "Training epoch 3474 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3474 ; accuracy: 0.74; loss: 2.0725395679473877\n",
      "Training epoch 3475 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3475 ; accuracy: 0.74; loss: 2.072622060775757\n",
      "Training epoch 3476 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3476 ; accuracy: 0.74; loss: 2.0727148056030273\n",
      "Training epoch 3477 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3477 ; accuracy: 0.74; loss: 2.0728042125701904\n",
      "Training epoch 3478 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3478 ; accuracy: 0.74; loss: 2.072890520095825\n",
      "Training epoch 3479 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3479 ; accuracy: 0.74; loss: 2.072970390319824\n",
      "Training epoch 3480 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3480 ; accuracy: 0.74; loss: 2.0730464458465576\n",
      "Training epoch 3481 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3481 ; accuracy: 0.74; loss: 2.0731215476989746\n",
      "Training epoch 3482 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3482 ; accuracy: 0.74; loss: 2.073194742202759\n",
      "Training epoch 3483 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3483 ; accuracy: 0.74; loss: 2.0732715129852295\n",
      "Training epoch 3484 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3484 ; accuracy: 0.74; loss: 2.073350667953491\n",
      "Training epoch 3485 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3485 ; accuracy: 0.74; loss: 2.073427200317383\n",
      "Training epoch 3486 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3486 ; accuracy: 0.74; loss: 2.073499917984009\n",
      "Training epoch 3487 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3487 ; accuracy: 0.74; loss: 2.0735697746276855\n",
      "Training epoch 3488 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3488 ; accuracy: 0.74; loss: 2.0736451148986816\n",
      "Training epoch 3489 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3489 ; accuracy: 0.74; loss: 2.0737204551696777\n",
      "Training epoch 3490 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3490 ; accuracy: 0.74; loss: 2.07379412651062\n",
      "Training epoch 3491 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3491 ; accuracy: 0.74; loss: 2.0738704204559326\n",
      "Training epoch 3492 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3492 ; accuracy: 0.74; loss: 2.0739574432373047\n",
      "Training epoch 3493 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3493 ; accuracy: 0.74; loss: 2.074049234390259\n",
      "Training epoch 3494 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3494 ; accuracy: 0.74; loss: 2.074134111404419\n",
      "Training epoch 3495 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3495 ; accuracy: 0.74; loss: 2.0742173194885254\n",
      "Training epoch 3496 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3496 ; accuracy: 0.74; loss: 2.074296712875366\n",
      "Training epoch 3497 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3497 ; accuracy: 0.74; loss: 2.074371814727783\n",
      "Training epoch 3498 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3498 ; accuracy: 0.74; loss: 2.0744435787200928\n",
      "Training epoch 3499 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3499 ; accuracy: 0.74; loss: 2.0745134353637695\n",
      "Training epoch 3500 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3500 ; accuracy: 0.74; loss: 2.074586868286133\n",
      "Training epoch 3501 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3501 ; accuracy: 0.74; loss: 2.0746536254882812\n",
      "Training epoch 3502 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3502 ; accuracy: 0.74; loss: 2.074716329574585\n",
      "Training epoch 3503 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3503 ; accuracy: 0.74; loss: 2.074782609939575\n",
      "Training epoch 3504 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3504 ; accuracy: 0.74; loss: 2.0748445987701416\n",
      "Training epoch 3505 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3505 ; accuracy: 0.74; loss: 2.074904203414917\n",
      "Training epoch 3506 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3506 ; accuracy: 0.74; loss: 2.074970245361328\n",
      "Training epoch 3507 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3507 ; accuracy: 0.74; loss: 2.0750327110290527\n",
      "Training epoch 3508 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3508 ; accuracy: 0.74; loss: 2.0750908851623535\n",
      "Training epoch 3509 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3509 ; accuracy: 0.74; loss: 2.075143337249756\n",
      "Training epoch 3510 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3510 ; accuracy: 0.74; loss: 2.07521390914917\n",
      "Training epoch 3511 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3511 ; accuracy: 0.74; loss: 2.0752861499786377\n",
      "Training epoch 3512 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3512 ; accuracy: 0.74; loss: 2.0753519535064697\n",
      "Training epoch 3513 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 3513 ; accuracy: 0.74; loss: 2.0754499435424805\n",
      "Training epoch 3514 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3514 ; accuracy: 0.74; loss: 2.075547218322754\n",
      "Training epoch 3515 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3515 ; accuracy: 0.74; loss: 2.0756406784057617\n",
      "Training epoch 3516 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3516 ; accuracy: 0.74; loss: 2.075742721557617\n",
      "Training epoch 3517 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3517 ; accuracy: 0.74; loss: 2.0758514404296875\n",
      "Training epoch 3518 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3518 ; accuracy: 0.74; loss: 2.0759506225585938\n",
      "Training epoch 3519 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3519 ; accuracy: 0.74; loss: 2.076047658920288\n",
      "Training epoch 3520 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3520 ; accuracy: 0.74; loss: 2.0761334896087646\n",
      "Training epoch 3521 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3521 ; accuracy: 0.74; loss: 2.0762147903442383\n",
      "Training epoch 3522 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3522 ; accuracy: 0.74; loss: 2.076291799545288\n",
      "Training epoch 3523 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3523 ; accuracy: 0.74; loss: 2.0763752460479736\n",
      "Training epoch 3524 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3524 ; accuracy: 0.74; loss: 2.0764567852020264\n",
      "Training epoch 3525 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3525 ; accuracy: 0.74; loss: 2.076535224914551\n",
      "Training epoch 3526 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3526 ; accuracy: 0.74; loss: 2.076608657836914\n",
      "Training epoch 3527 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3527 ; accuracy: 0.74; loss: 2.076664686203003\n",
      "Training epoch 3528 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3528 ; accuracy: 0.74; loss: 2.0767176151275635\n",
      "Training epoch 3529 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3529 ; accuracy: 0.74; loss: 2.0767734050750732\n",
      "Training epoch 3530 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3530 ; accuracy: 0.74; loss: 2.076826572418213\n",
      "Training epoch 3531 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3531 ; accuracy: 0.74; loss: 2.076880931854248\n",
      "Training epoch 3532 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3532 ; accuracy: 0.74; loss: 2.0769286155700684\n",
      "Training epoch 3533 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3533 ; accuracy: 0.74; loss: 2.0769686698913574\n",
      "Training epoch 3534 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3534 ; accuracy: 0.74; loss: 2.0770041942596436\n",
      "Training epoch 3535 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3535 ; accuracy: 0.74; loss: 2.0770368576049805\n",
      "Training epoch 3536 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3536 ; accuracy: 0.74; loss: 2.0770699977874756\n",
      "Training epoch 3537 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3537 ; accuracy: 0.74; loss: 2.0771048069000244\n",
      "Training epoch 3538 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3538 ; accuracy: 0.74; loss: 2.077145576477051\n",
      "Training epoch 3539 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3539 ; accuracy: 0.74; loss: 2.077186346054077\n",
      "Training epoch 3540 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3540 ; accuracy: 0.74; loss: 2.0772247314453125\n",
      "Training epoch 3541 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3541 ; accuracy: 0.74; loss: 2.077268123626709\n",
      "Training epoch 3542 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3542 ; accuracy: 0.74; loss: 2.077308416366577\n",
      "Training epoch 3543 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3543 ; accuracy: 0.74; loss: 2.0773532390594482\n",
      "Training epoch 3544 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3544 ; accuracy: 0.74; loss: 2.077404022216797\n",
      "Training epoch 3545 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3545 ; accuracy: 0.74; loss: 2.0774524211883545\n",
      "Training epoch 3546 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3546 ; accuracy: 0.74; loss: 2.0774986743927\n",
      "Training epoch 3547 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3547 ; accuracy: 0.74; loss: 2.0775671005249023\n",
      "Training epoch 3548 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3548 ; accuracy: 0.74; loss: 2.077636957168579\n",
      "Training epoch 3549 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3549 ; accuracy: 0.74; loss: 2.0777177810668945\n",
      "Training epoch 3550 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3550 ; accuracy: 0.74; loss: 2.0777928829193115\n",
      "Training epoch 3551 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3551 ; accuracy: 0.74; loss: 2.077862024307251\n",
      "Training epoch 3552 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3552 ; accuracy: 0.74; loss: 2.077928066253662\n",
      "Training epoch 3553 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3553 ; accuracy: 0.74; loss: 2.0779800415039062\n",
      "Training epoch 3554 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3554 ; accuracy: 0.74; loss: 2.07804274559021\n",
      "Training epoch 3555 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3555 ; accuracy: 0.74; loss: 2.0781049728393555\n",
      "Training epoch 3556 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3556 ; accuracy: 0.74; loss: 2.0781683921813965\n",
      "Training epoch 3557 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3557 ; accuracy: 0.74; loss: 2.0782268047332764\n",
      "Training epoch 3558 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3558 ; accuracy: 0.74; loss: 2.0782840251922607\n",
      "Training epoch 3559 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3559 ; accuracy: 0.74; loss: 2.0783379077911377\n",
      "Training epoch 3560 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3560 ; accuracy: 0.74; loss: 2.0784106254577637\n",
      "Training epoch 3561 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3561 ; accuracy: 0.74; loss: 2.078469753265381\n",
      "Training epoch 3562 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3562 ; accuracy: 0.74; loss: 2.0785257816314697\n",
      "Training epoch 3563 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3563 ; accuracy: 0.74; loss: 2.078580379486084\n",
      "Training epoch 3564 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3564 ; accuracy: 0.74; loss: 2.078636884689331\n",
      "Training epoch 3565 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3565 ; accuracy: 0.74; loss: 2.0786967277526855\n",
      "Training epoch 3566 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3566 ; accuracy: 0.74; loss: 2.07875657081604\n",
      "Training epoch 3567 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3567 ; accuracy: 0.74; loss: 2.078817844390869\n",
      "Training epoch 3568 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3568 ; accuracy: 0.74; loss: 2.0788817405700684\n",
      "Training epoch 3569 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3569 ; accuracy: 0.74; loss: 2.078951120376587\n",
      "Training epoch 3570 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3570 ; accuracy: 0.74; loss: 2.0790176391601562\n",
      "Training epoch 3571 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3571 ; accuracy: 0.74; loss: 2.0790822505950928\n",
      "Training epoch 3572 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3572 ; accuracy: 0.74; loss: 2.0791420936584473\n",
      "Training epoch 3573 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3573 ; accuracy: 0.74; loss: 2.0792016983032227\n",
      "Training epoch 3574 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3574 ; accuracy: 0.74; loss: 2.0792508125305176\n",
      "Training epoch 3575 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3575 ; accuracy: 0.74; loss: 2.0792999267578125\n",
      "Training epoch 3576 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3576 ; accuracy: 0.74; loss: 2.0793423652648926\n",
      "Training epoch 3577 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3577 ; accuracy: 0.74; loss: 2.079385280609131\n",
      "Training epoch 3578 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3578 ; accuracy: 0.74; loss: 2.079421043395996\n",
      "Training epoch 3579 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3579 ; accuracy: 0.74; loss: 2.079455852508545\n",
      "Training epoch 3580 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3580 ; accuracy: 0.74; loss: 2.07949161529541\n",
      "Training epoch 3581 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3581 ; accuracy: 0.74; loss: 2.0795516967773438\n",
      "Training epoch 3582 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3582 ; accuracy: 0.74; loss: 2.0796070098876953\n",
      "Training epoch 3583 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3583 ; accuracy: 0.74; loss: 2.0796656608581543\n",
      "Training epoch 3584 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3584 ; accuracy: 0.74; loss: 2.0797204971313477\n",
      "Training epoch 3585 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3585 ; accuracy: 0.74; loss: 2.0797677040100098\n",
      "Training epoch 3586 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3586 ; accuracy: 0.74; loss: 2.079822301864624\n",
      "Training epoch 3587 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3587 ; accuracy: 0.74; loss: 2.079880475997925\n",
      "Training epoch 3588 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3588 ; accuracy: 0.74; loss: 2.0799484252929688\n",
      "Training epoch 3589 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3589 ; accuracy: 0.74; loss: 2.080014228820801\n",
      "Training epoch 3590 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3590 ; accuracy: 0.74; loss: 2.080073118209839\n",
      "Training epoch 3591 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3591 ; accuracy: 0.74; loss: 2.080129384994507\n",
      "Training epoch 3592 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3592 ; accuracy: 0.74; loss: 2.080186367034912\n",
      "Training epoch 3593 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3593 ; accuracy: 0.74; loss: 2.080242395401001\n",
      "Training epoch 3594 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3594 ; accuracy: 0.74; loss: 2.08030104637146\n",
      "Training epoch 3595 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3595 ; accuracy: 0.74; loss: 2.08035945892334\n",
      "Training epoch 3596 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3596 ; accuracy: 0.74; loss: 2.08042049407959\n",
      "Training epoch 3597 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3597 ; accuracy: 0.74; loss: 2.0804691314697266\n",
      "Training epoch 3598 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3598 ; accuracy: 0.74; loss: 2.080521583557129\n",
      "Training epoch 3599 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3599 ; accuracy: 0.74; loss: 2.080571413040161\n",
      "Training epoch 3600 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3600 ; accuracy: 0.74; loss: 2.080625295639038\n",
      "Training epoch 3601 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3601 ; accuracy: 0.74; loss: 2.0806872844696045\n",
      "Training epoch 3602 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3602 ; accuracy: 0.74; loss: 2.080744981765747\n",
      "Training epoch 3603 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3603 ; accuracy: 0.74; loss: 2.0807948112487793\n",
      "Training epoch 3604 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3604 ; accuracy: 0.74; loss: 2.080842971801758\n",
      "Training epoch 3605 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3605 ; accuracy: 0.74; loss: 2.0808911323547363\n",
      "Training epoch 3606 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3606 ; accuracy: 0.74; loss: 2.080944061279297\n",
      "Training epoch 3607 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3607 ; accuracy: 0.74; loss: 2.0809988975524902\n",
      "Training epoch 3608 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3608 ; accuracy: 0.74; loss: 2.0810649394989014\n",
      "Training epoch 3609 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3609 ; accuracy: 0.74; loss: 2.0811209678649902\n",
      "Training epoch 3610 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3610 ; accuracy: 0.74; loss: 2.0811731815338135\n",
      "Training epoch 3611 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3611 ; accuracy: 0.74; loss: 2.0812220573425293\n",
      "Training epoch 3612 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3612 ; accuracy: 0.74; loss: 2.0812692642211914\n",
      "Training epoch 3613 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3613 ; accuracy: 0.74; loss: 2.081317186355591\n",
      "Training epoch 3614 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3614 ; accuracy: 0.74; loss: 2.081371545791626\n",
      "Training epoch 3615 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3615 ; accuracy: 0.74; loss: 2.0814273357391357\n",
      "Training epoch 3616 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3616 ; accuracy: 0.74; loss: 2.0814788341522217\n",
      "Training epoch 3617 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3617 ; accuracy: 0.74; loss: 2.081529140472412\n",
      "Training epoch 3618 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3618 ; accuracy: 0.74; loss: 2.0815823078155518\n",
      "Training epoch 3619 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3619 ; accuracy: 0.74; loss: 2.0816314220428467\n",
      "Training epoch 3620 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3620 ; accuracy: 0.74; loss: 2.081669807434082\n",
      "Training epoch 3621 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3621 ; accuracy: 0.74; loss: 2.081716775894165\n",
      "Training epoch 3622 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3622 ; accuracy: 0.74; loss: 2.08176589012146\n",
      "Training epoch 3623 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3623 ; accuracy: 0.74; loss: 2.081817388534546\n",
      "Training epoch 3624 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3624 ; accuracy: 0.74; loss: 2.0818700790405273\n",
      "Training epoch 3625 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3625 ; accuracy: 0.74; loss: 2.081920862197876\n",
      "Training epoch 3626 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3626 ; accuracy: 0.74; loss: 2.0819780826568604\n",
      "Training epoch 3627 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3627 ; accuracy: 0.74; loss: 2.082038402557373\n",
      "Training epoch 3628 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3628 ; accuracy: 0.74; loss: 2.0820930004119873\n",
      "Training epoch 3629 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3629 ; accuracy: 0.74; loss: 2.0821428298950195\n",
      "Training epoch 3630 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3630 ; accuracy: 0.74; loss: 2.082193613052368\n",
      "Training epoch 3631 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3631 ; accuracy: 0.74; loss: 2.082242727279663\n",
      "Training epoch 3632 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3632 ; accuracy: 0.74; loss: 2.0822949409484863\n",
      "Training epoch 3633 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3633 ; accuracy: 0.74; loss: 2.0823466777801514\n",
      "Training epoch 3634 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3634 ; accuracy: 0.74; loss: 2.0823910236358643\n",
      "Training epoch 3635 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3635 ; accuracy: 0.74; loss: 2.0824475288391113\n",
      "Training epoch 3636 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3636 ; accuracy: 0.74; loss: 2.0825016498565674\n",
      "Training epoch 3637 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3637 ; accuracy: 0.74; loss: 2.08255934715271\n",
      "Training epoch 3638 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3638 ; accuracy: 0.74; loss: 2.0826196670532227\n",
      "Training epoch 3639 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3639 ; accuracy: 0.74; loss: 2.082681179046631\n",
      "Training epoch 3640 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3640 ; accuracy: 0.74; loss: 2.0827395915985107\n",
      "Training epoch 3641 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 3641 ; accuracy: 0.74; loss: 2.0828213691711426\n",
      "Training epoch 3642 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3642 ; accuracy: 0.74; loss: 2.0829010009765625\n",
      "Training epoch 3643 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3643 ; accuracy: 0.74; loss: 2.0829808712005615\n",
      "Training epoch 3644 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3644 ; accuracy: 0.74; loss: 2.0830576419830322\n",
      "Training epoch 3645 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3645 ; accuracy: 0.74; loss: 2.083131790161133\n",
      "Training epoch 3646 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3646 ; accuracy: 0.74; loss: 2.083199977874756\n",
      "Training epoch 3647 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3647 ; accuracy: 0.74; loss: 2.083277463912964\n",
      "Training epoch 3648 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3648 ; accuracy: 0.74; loss: 2.083343744277954\n",
      "Training epoch 3649 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3649 ; accuracy: 0.74; loss: 2.0834274291992188\n",
      "Training epoch 3650 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3650 ; accuracy: 0.74; loss: 2.083505153656006\n",
      "Training epoch 3651 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3651 ; accuracy: 0.74; loss: 2.0835752487182617\n",
      "Training epoch 3652 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3652 ; accuracy: 0.74; loss: 2.083638906478882\n",
      "Training epoch 3653 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3653 ; accuracy: 0.74; loss: 2.0837056636810303\n",
      "Training epoch 3654 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3654 ; accuracy: 0.74; loss: 2.083770990371704\n",
      "Training epoch 3655 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3655 ; accuracy: 0.74; loss: 2.0838429927825928\n",
      "Training epoch 3656 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3656 ; accuracy: 0.74; loss: 2.0839169025421143\n",
      "Training epoch 3657 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3657 ; accuracy: 0.74; loss: 2.083981990814209\n",
      "Training epoch 3658 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3658 ; accuracy: 0.74; loss: 2.0840489864349365\n",
      "Training epoch 3659 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3659 ; accuracy: 0.74; loss: 2.0841195583343506\n",
      "Training epoch 3660 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3660 ; accuracy: 0.74; loss: 2.08418345451355\n",
      "Training epoch 3661 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3661 ; accuracy: 0.74; loss: 2.084240436553955\n",
      "Training epoch 3662 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 3662 ; accuracy: 0.74; loss: 2.084301710128784\n",
      "Training epoch 3663 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3663 ; accuracy: 0.74; loss: 2.0843613147735596\n",
      "Training epoch 3664 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3664 ; accuracy: 0.74; loss: 2.084423303604126\n",
      "Training epoch 3665 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3665 ; accuracy: 0.74; loss: 2.0844924449920654\n",
      "Training epoch 3666 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3666 ; accuracy: 0.74; loss: 2.084564208984375\n",
      "Training epoch 3667 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3667 ; accuracy: 0.74; loss: 2.084636926651001\n",
      "Training epoch 3668 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3668 ; accuracy: 0.74; loss: 2.0847055912017822\n",
      "Training epoch 3669 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3669 ; accuracy: 0.74; loss: 2.084775924682617\n",
      "Training epoch 3670 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3670 ; accuracy: 0.74; loss: 2.0848612785339355\n",
      "Training epoch 3671 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3671 ; accuracy: 0.74; loss: 2.084942102432251\n",
      "Training epoch 3672 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3672 ; accuracy: 0.74; loss: 2.0850164890289307\n",
      "Training epoch 3673 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3673 ; accuracy: 0.74; loss: 2.0850887298583984\n",
      "Training epoch 3674 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3674 ; accuracy: 0.74; loss: 2.085164785385132\n",
      "Training epoch 3675 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3675 ; accuracy: 0.74; loss: 2.085240125656128\n",
      "Training epoch 3676 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3676 ; accuracy: 0.74; loss: 2.085312604904175\n",
      "Training epoch 3677 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3677 ; accuracy: 0.74; loss: 2.0853874683380127\n",
      "Training epoch 3678 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3678 ; accuracy: 0.74; loss: 2.0854575634002686\n",
      "Training epoch 3679 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3679 ; accuracy: 0.74; loss: 2.085521697998047\n",
      "Training epoch 3680 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3680 ; accuracy: 0.74; loss: 2.0855798721313477\n",
      "Training epoch 3681 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3681 ; accuracy: 0.74; loss: 2.0856359004974365\n",
      "Training epoch 3682 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3682 ; accuracy: 0.74; loss: 2.085688829421997\n",
      "Training epoch 3683 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3683 ; accuracy: 0.74; loss: 2.0857441425323486\n",
      "Training epoch 3684 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3684 ; accuracy: 0.74; loss: 2.085791826248169\n",
      "Training epoch 3685 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3685 ; accuracy: 0.74; loss: 2.085831642150879\n",
      "Training epoch 3686 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3686 ; accuracy: 0.74; loss: 2.08587384223938\n",
      "Training epoch 3687 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3687 ; accuracy: 0.74; loss: 2.0859150886535645\n",
      "Training epoch 3688 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3688 ; accuracy: 0.74; loss: 2.0859570503234863\n",
      "Training epoch 3689 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3689 ; accuracy: 0.74; loss: 2.085993766784668\n",
      "Training epoch 3690 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3690 ; accuracy: 0.74; loss: 2.086031675338745\n",
      "Training epoch 3691 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3691 ; accuracy: 0.74; loss: 2.0860791206359863\n",
      "Training epoch 3692 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3692 ; accuracy: 0.74; loss: 2.0861284732818604\n",
      "Training epoch 3693 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3693 ; accuracy: 0.74; loss: 2.0861740112304688\n",
      "Training epoch 3694 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3694 ; accuracy: 0.74; loss: 2.086216449737549\n",
      "Training epoch 3695 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3695 ; accuracy: 0.74; loss: 2.086273670196533\n",
      "Training epoch 3696 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3696 ; accuracy: 0.74; loss: 2.08632755279541\n",
      "Training epoch 3697 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3697 ; accuracy: 0.74; loss: 2.086378812789917\n",
      "Training epoch 3698 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3698 ; accuracy: 0.74; loss: 2.086427688598633\n",
      "Training epoch 3699 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3699 ; accuracy: 0.74; loss: 2.0864832401275635\n",
      "Training epoch 3700 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3700 ; accuracy: 0.74; loss: 2.086540699005127\n",
      "Training epoch 3701 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3701 ; accuracy: 0.74; loss: 2.0866003036499023\n",
      "Training epoch 3702 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3702 ; accuracy: 0.74; loss: 2.0866527557373047\n",
      "Training epoch 3703 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3703 ; accuracy: 0.74; loss: 2.086704969406128\n",
      "Training epoch 3704 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3704 ; accuracy: 0.74; loss: 2.0867457389831543\n",
      "Training epoch 3705 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3705 ; accuracy: 0.74; loss: 2.0867860317230225\n",
      "Training epoch 3706 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3706 ; accuracy: 0.74; loss: 2.0868256092071533\n",
      "Training epoch 3707 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3707 ; accuracy: 0.74; loss: 2.086867332458496\n",
      "Training epoch 3708 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3708 ; accuracy: 0.74; loss: 2.0869088172912598\n",
      "Training epoch 3709 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3709 ; accuracy: 0.74; loss: 2.0869526863098145\n",
      "Training epoch 3710 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3710 ; accuracy: 0.74; loss: 2.0869994163513184\n",
      "Training epoch 3711 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3711 ; accuracy: 0.74; loss: 2.0870466232299805\n",
      "Training epoch 3712 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3712 ; accuracy: 0.74; loss: 2.0870847702026367\n",
      "Training epoch 3713 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3713 ; accuracy: 0.74; loss: 2.0871293544769287\n",
      "Training epoch 3714 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3714 ; accuracy: 0.74; loss: 2.0871713161468506\n",
      "Training epoch 3715 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3715 ; accuracy: 0.74; loss: 2.0872228145599365\n",
      "Training epoch 3716 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3716 ; accuracy: 0.74; loss: 2.0872745513916016\n",
      "Training epoch 3717 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3717 ; accuracy: 0.74; loss: 2.08732533454895\n",
      "Training epoch 3718 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3718 ; accuracy: 0.74; loss: 2.0873799324035645\n",
      "Training epoch 3719 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3719 ; accuracy: 0.74; loss: 2.0874361991882324\n",
      "Training epoch 3720 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3720 ; accuracy: 0.74; loss: 2.0874931812286377\n",
      "Training epoch 3721 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3721 ; accuracy: 0.74; loss: 2.0875511169433594\n",
      "Training epoch 3722 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3722 ; accuracy: 0.74; loss: 2.0876102447509766\n",
      "Training epoch 3723 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3723 ; accuracy: 0.74; loss: 2.0876667499542236\n",
      "Training epoch 3724 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3724 ; accuracy: 0.74; loss: 2.0877292156219482\n",
      "Training epoch 3725 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3725 ; accuracy: 0.74; loss: 2.0877881050109863\n",
      "Training epoch 3726 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3726 ; accuracy: 0.74; loss: 2.0878520011901855\n",
      "Training epoch 3727 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3727 ; accuracy: 0.74; loss: 2.087921380996704\n",
      "Training epoch 3728 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3728 ; accuracy: 0.74; loss: 2.08798885345459\n",
      "Training epoch 3729 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3729 ; accuracy: 0.74; loss: 2.0880539417266846\n",
      "Training epoch 3730 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3730 ; accuracy: 0.74; loss: 2.088116407394409\n",
      "Training epoch 3731 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3731 ; accuracy: 0.74; loss: 2.0881752967834473\n",
      "Training epoch 3732 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3732 ; accuracy: 0.74; loss: 2.0882434844970703\n",
      "Training epoch 3733 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3733 ; accuracy: 0.74; loss: 2.088312864303589\n",
      "Training epoch 3734 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3734 ; accuracy: 0.74; loss: 2.0883755683898926\n",
      "Training epoch 3735 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3735 ; accuracy: 0.74; loss: 2.088435649871826\n",
      "Training epoch 3736 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3736 ; accuracy: 0.74; loss: 2.0884933471679688\n",
      "Training epoch 3737 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3737 ; accuracy: 0.74; loss: 2.088547468185425\n",
      "Training epoch 3738 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3738 ; accuracy: 0.74; loss: 2.088594436645508\n",
      "Training epoch 3739 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3739 ; accuracy: 0.74; loss: 2.088642120361328\n",
      "Training epoch 3740 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3740 ; accuracy: 0.74; loss: 2.0886898040771484\n",
      "Training epoch 3741 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3741 ; accuracy: 0.74; loss: 2.088740348815918\n",
      "Training epoch 3742 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3742 ; accuracy: 0.74; loss: 2.0887904167175293\n",
      "Training epoch 3743 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3743 ; accuracy: 0.74; loss: 2.088841199874878\n",
      "Training epoch 3744 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3744 ; accuracy: 0.74; loss: 2.0888850688934326\n",
      "Training epoch 3745 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3745 ; accuracy: 0.74; loss: 2.0889229774475098\n",
      "Training epoch 3746 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3746 ; accuracy: 0.74; loss: 2.0889506340026855\n",
      "Training epoch 3747 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3747 ; accuracy: 0.74; loss: 2.0889880657196045\n",
      "Training epoch 3748 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3748 ; accuracy: 0.74; loss: 2.0890302658081055\n",
      "Training epoch 3749 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3749 ; accuracy: 0.74; loss: 2.089080810546875\n",
      "Training epoch 3750 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3750 ; accuracy: 0.74; loss: 2.089132070541382\n",
      "Training epoch 3751 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3751 ; accuracy: 0.74; loss: 2.089179039001465\n",
      "Training epoch 3752 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3752 ; accuracy: 0.74; loss: 2.0892269611358643\n",
      "Training epoch 3753 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3753 ; accuracy: 0.74; loss: 2.089279890060425\n",
      "Training epoch 3754 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3754 ; accuracy: 0.74; loss: 2.089320659637451\n",
      "Training epoch 3755 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3755 ; accuracy: 0.74; loss: 2.089367151260376\n",
      "Training epoch 3756 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3756 ; accuracy: 0.74; loss: 2.0894205570220947\n",
      "Training epoch 3757 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3757 ; accuracy: 0.74; loss: 2.0894715785980225\n",
      "Training epoch 3758 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3758 ; accuracy: 0.74; loss: 2.0895349979400635\n",
      "Training epoch 3759 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3759 ; accuracy: 0.74; loss: 2.0895943641662598\n",
      "Training epoch 3760 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3760 ; accuracy: 0.74; loss: 2.0896615982055664\n",
      "Training epoch 3761 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3761 ; accuracy: 0.74; loss: 2.0897305011749268\n",
      "Training epoch 3762 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3762 ; accuracy: 0.74; loss: 2.089801073074341\n",
      "Training epoch 3763 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3763 ; accuracy: 0.74; loss: 2.0898735523223877\n",
      "Training epoch 3764 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3764 ; accuracy: 0.74; loss: 2.089942693710327\n",
      "Training epoch 3765 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3765 ; accuracy: 0.74; loss: 2.0900068283081055\n",
      "Training epoch 3766 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3766 ; accuracy: 0.74; loss: 2.0900721549987793\n",
      "Training epoch 3767 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3767 ; accuracy: 0.74; loss: 2.09015154838562\n",
      "Training epoch 3768 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3768 ; accuracy: 0.74; loss: 2.0902302265167236\n",
      "Training epoch 3769 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3769 ; accuracy: 0.74; loss: 2.090301990509033\n",
      "Training epoch 3770 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3770 ; accuracy: 0.74; loss: 2.0903868675231934\n",
      "Training epoch 3771 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3771 ; accuracy: 0.74; loss: 2.090466260910034\n",
      "Training epoch 3772 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3772 ; accuracy: 0.74; loss: 2.090542793273926\n",
      "Training epoch 3773 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3773 ; accuracy: 0.74; loss: 2.090627670288086\n",
      "Training epoch 3774 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3774 ; accuracy: 0.74; loss: 2.0907108783721924\n",
      "Training epoch 3775 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3775 ; accuracy: 0.74; loss: 2.090791940689087\n",
      "Training epoch 3776 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3776 ; accuracy: 0.74; loss: 2.0908639430999756\n",
      "Training epoch 3777 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3777 ; accuracy: 0.74; loss: 2.090930938720703\n",
      "Training epoch 3778 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3778 ; accuracy: 0.74; loss: 2.0909910202026367\n",
      "Training epoch 3779 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3779 ; accuracy: 0.74; loss: 2.091054916381836\n",
      "Training epoch 3780 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3780 ; accuracy: 0.74; loss: 2.091132164001465\n",
      "Training epoch 3781 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3781 ; accuracy: 0.74; loss: 2.091207981109619\n",
      "Training epoch 3782 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3782 ; accuracy: 0.74; loss: 2.091275453567505\n",
      "Training epoch 3783 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3783 ; accuracy: 0.74; loss: 2.091339349746704\n",
      "Training epoch 3784 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3784 ; accuracy: 0.74; loss: 2.0914008617401123\n",
      "Training epoch 3785 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3785 ; accuracy: 0.74; loss: 2.0914642810821533\n",
      "Training epoch 3786 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3786 ; accuracy: 0.74; loss: 2.091522216796875\n",
      "Training epoch 3787 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3787 ; accuracy: 0.74; loss: 2.09157657623291\n",
      "Training epoch 3788 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3788 ; accuracy: 0.74; loss: 2.091627359390259\n",
      "Training epoch 3789 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3789 ; accuracy: 0.74; loss: 2.091670036315918\n",
      "Training epoch 3790 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3790 ; accuracy: 0.74; loss: 2.0917158126831055\n",
      "Training epoch 3791 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3791 ; accuracy: 0.74; loss: 2.091764450073242\n",
      "Training epoch 3792 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3792 ; accuracy: 0.74; loss: 2.0918185710906982\n",
      "Training epoch 3793 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3793 ; accuracy: 0.74; loss: 2.091874599456787\n",
      "Training epoch 3794 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3794 ; accuracy: 0.74; loss: 2.0919363498687744\n",
      "Training epoch 3795 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3795 ; accuracy: 0.74; loss: 2.0919878482818604\n",
      "Training epoch 3796 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3796 ; accuracy: 0.74; loss: 2.092045545578003\n",
      "Training epoch 3797 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3797 ; accuracy: 0.74; loss: 2.092101812362671\n",
      "Training epoch 3798 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3798 ; accuracy: 0.74; loss: 2.092167377471924\n",
      "Training epoch 3799 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3799 ; accuracy: 0.74; loss: 2.092233180999756\n",
      "Training epoch 3800 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3800 ; accuracy: 0.74; loss: 2.0922939777374268\n",
      "Training epoch 3801 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3801 ; accuracy: 0.74; loss: 2.092350959777832\n",
      "Training epoch 3802 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3802 ; accuracy: 0.74; loss: 2.0924088954925537\n",
      "Training epoch 3803 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3803 ; accuracy: 0.74; loss: 2.092463731765747\n",
      "Training epoch 3804 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3804 ; accuracy: 0.74; loss: 2.092520236968994\n",
      "Training epoch 3805 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3805 ; accuracy: 0.74; loss: 2.0925779342651367\n",
      "Training epoch 3806 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3806 ; accuracy: 0.74; loss: 2.0926401615142822\n",
      "Training epoch 3807 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3807 ; accuracy: 0.74; loss: 2.092703342437744\n",
      "Training epoch 3808 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3808 ; accuracy: 0.74; loss: 2.092761278152466\n",
      "Training epoch 3809 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3809 ; accuracy: 0.74; loss: 2.0928189754486084\n",
      "Training epoch 3810 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3810 ; accuracy: 0.74; loss: 2.0928773880004883\n",
      "Training epoch 3811 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3811 ; accuracy: 0.74; loss: 2.0929415225982666\n",
      "Training epoch 3812 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3812 ; accuracy: 0.74; loss: 2.0930094718933105\n",
      "Training epoch 3813 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3813 ; accuracy: 0.74; loss: 2.0930819511413574\n",
      "Training epoch 3814 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3814 ; accuracy: 0.74; loss: 2.093147039413452\n",
      "Training epoch 3815 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3815 ; accuracy: 0.74; loss: 2.0932159423828125\n",
      "Training epoch 3816 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3816 ; accuracy: 0.74; loss: 2.0932822227478027\n",
      "Training epoch 3817 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3817 ; accuracy: 0.74; loss: 2.093352794647217\n",
      "Training epoch 3818 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3818 ; accuracy: 0.74; loss: 2.0934107303619385\n",
      "Training epoch 3819 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3819 ; accuracy: 0.74; loss: 2.093477725982666\n",
      "Training epoch 3820 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3820 ; accuracy: 0.74; loss: 2.0935275554656982\n",
      "Training epoch 3821 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3821 ; accuracy: 0.74; loss: 2.093571901321411\n",
      "Training epoch 3822 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3822 ; accuracy: 0.74; loss: 2.0936129093170166\n",
      "Training epoch 3823 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 3823 ; accuracy: 0.74; loss: 2.0936882495880127\n",
      "Training epoch 3824 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3824 ; accuracy: 0.74; loss: 2.093759059906006\n",
      "Training epoch 3825 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3825 ; accuracy: 0.74; loss: 2.0938241481781006\n",
      "Training epoch 3826 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3826 ; accuracy: 0.74; loss: 2.09389328956604\n",
      "Training epoch 3827 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3827 ; accuracy: 0.74; loss: 2.0939536094665527\n",
      "Training epoch 3828 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3828 ; accuracy: 0.74; loss: 2.0940098762512207\n",
      "Training epoch 3829 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3829 ; accuracy: 0.74; loss: 2.094057321548462\n",
      "Training epoch 3830 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3830 ; accuracy: 0.74; loss: 2.0941030979156494\n",
      "Training epoch 3831 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3831 ; accuracy: 0.74; loss: 2.094151020050049\n",
      "Training epoch 3832 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3832 ; accuracy: 0.74; loss: 2.094200372695923\n",
      "Training epoch 3833 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3833 ; accuracy: 0.74; loss: 2.0942506790161133\n",
      "Training epoch 3834 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3834 ; accuracy: 0.74; loss: 2.0943055152893066\n",
      "Training epoch 3835 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3835 ; accuracy: 0.74; loss: 2.094367504119873\n",
      "Training epoch 3836 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3836 ; accuracy: 0.74; loss: 2.094423770904541\n",
      "Training epoch 3837 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3837 ; accuracy: 0.74; loss: 2.0944743156433105\n",
      "Training epoch 3838 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3838 ; accuracy: 0.74; loss: 2.0945279598236084\n",
      "Training epoch 3839 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3839 ; accuracy: 0.74; loss: 2.09458589553833\n",
      "Training epoch 3840 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3840 ; accuracy: 0.74; loss: 2.0946474075317383\n",
      "Training epoch 3841 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3841 ; accuracy: 0.74; loss: 2.0947093963623047\n",
      "Training epoch 3842 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3842 ; accuracy: 0.74; loss: 2.094766139984131\n",
      "Training epoch 3843 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3843 ; accuracy: 0.74; loss: 2.094822645187378\n",
      "Training epoch 3844 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3844 ; accuracy: 0.74; loss: 2.0948867797851562\n",
      "Training epoch 3845 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3845 ; accuracy: 0.74; loss: 2.0949547290802\n",
      "Training epoch 3846 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3846 ; accuracy: 0.74; loss: 2.0950169563293457\n",
      "Training epoch 3847 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3847 ; accuracy: 0.74; loss: 2.0950732231140137\n",
      "Training epoch 3848 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3848 ; accuracy: 0.74; loss: 2.0951309204101562\n",
      "Training epoch 3849 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3849 ; accuracy: 0.74; loss: 2.0951857566833496\n",
      "Training epoch 3850 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3850 ; accuracy: 0.74; loss: 2.0952517986297607\n",
      "Training epoch 3851 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3851 ; accuracy: 0.74; loss: 2.095308542251587\n",
      "Training epoch 3852 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3852 ; accuracy: 0.74; loss: 2.0953664779663086\n",
      "Training epoch 3853 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3853 ; accuracy: 0.74; loss: 2.0954198837280273\n",
      "Training epoch 3854 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3854 ; accuracy: 0.74; loss: 2.0954713821411133\n",
      "Training epoch 3855 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3855 ; accuracy: 0.74; loss: 2.0955286026000977\n",
      "Training epoch 3856 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3856 ; accuracy: 0.74; loss: 2.0955874919891357\n",
      "Training epoch 3857 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3857 ; accuracy: 0.74; loss: 2.0956437587738037\n",
      "Training epoch 3858 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3858 ; accuracy: 0.74; loss: 2.0956990718841553\n",
      "Training epoch 3859 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3859 ; accuracy: 0.74; loss: 2.095752239227295\n",
      "Training epoch 3860 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3860 ; accuracy: 0.74; loss: 2.095803737640381\n",
      "Training epoch 3861 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3861 ; accuracy: 0.74; loss: 2.095850944519043\n",
      "Training epoch 3862 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3862 ; accuracy: 0.74; loss: 2.0958898067474365\n",
      "Training epoch 3863 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3863 ; accuracy: 0.74; loss: 2.095935583114624\n",
      "Training epoch 3864 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3864 ; accuracy: 0.74; loss: 2.0959830284118652\n",
      "Training epoch 3865 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3865 ; accuracy: 0.74; loss: 2.096035957336426\n",
      "Training epoch 3866 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3866 ; accuracy: 0.74; loss: 2.0960896015167236\n",
      "Training epoch 3867 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3867 ; accuracy: 0.74; loss: 2.096142530441284\n",
      "Training epoch 3868 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3868 ; accuracy: 0.74; loss: 2.096191644668579\n",
      "Training epoch 3869 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3869 ; accuracy: 0.74; loss: 2.096250295639038\n",
      "Training epoch 3870 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3870 ; accuracy: 0.74; loss: 2.0963196754455566\n",
      "Training epoch 3871 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3871 ; accuracy: 0.74; loss: 2.096388578414917\n",
      "Training epoch 3872 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3872 ; accuracy: 0.74; loss: 2.09645414352417\n",
      "Training epoch 3873 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3873 ; accuracy: 0.74; loss: 2.0965187549591064\n",
      "Training epoch 3874 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3874 ; accuracy: 0.74; loss: 2.0965774059295654\n",
      "Training epoch 3875 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3875 ; accuracy: 0.74; loss: 2.0966362953186035\n",
      "Training epoch 3876 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3876 ; accuracy: 0.74; loss: 2.096693515777588\n",
      "Training epoch 3877 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3877 ; accuracy: 0.74; loss: 2.096750259399414\n",
      "Training epoch 3878 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3878 ; accuracy: 0.74; loss: 2.0968117713928223\n",
      "Training epoch 3879 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3879 ; accuracy: 0.74; loss: 2.0968685150146484\n",
      "Training epoch 3880 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3880 ; accuracy: 0.74; loss: 2.0969202518463135\n",
      "Training epoch 3881 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3881 ; accuracy: 0.74; loss: 2.0969722270965576\n",
      "Training epoch 3882 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3882 ; accuracy: 0.74; loss: 2.0970284938812256\n",
      "Training epoch 3883 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3883 ; accuracy: 0.74; loss: 2.097092390060425\n",
      "Training epoch 3884 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3884 ; accuracy: 0.74; loss: 2.097158908843994\n",
      "Training epoch 3885 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3885 ; accuracy: 0.74; loss: 2.0972282886505127\n",
      "Training epoch 3886 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3886 ; accuracy: 0.74; loss: 2.0972936153411865\n",
      "Training epoch 3887 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3887 ; accuracy: 0.74; loss: 2.0973739624023438\n",
      "Training epoch 3888 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3888 ; accuracy: 0.74; loss: 2.097439765930176\n",
      "Training epoch 3889 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3889 ; accuracy: 0.74; loss: 2.0975022315979004\n",
      "Training epoch 3890 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3890 ; accuracy: 0.74; loss: 2.0975558757781982\n",
      "Training epoch 3891 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 3891 ; accuracy: 0.74; loss: 2.097562551498413\n",
      "Training epoch 3892 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3892 ; accuracy: 0.74; loss: 2.097571611404419\n",
      "Training epoch 3893 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3893 ; accuracy: 0.74; loss: 2.097583532333374\n",
      "Training epoch 3894 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3894 ; accuracy: 0.74; loss: 2.097602367401123\n",
      "Training epoch 3895 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3895 ; accuracy: 0.74; loss: 2.097623348236084\n",
      "Training epoch 3896 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3896 ; accuracy: 0.74; loss: 2.0976433753967285\n",
      "Training epoch 3897 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 3897 ; accuracy: 0.74; loss: 2.0976665019989014\n",
      "Training epoch 3898 ; accuracy: 0.9; loss: 0.19459117949008942\n",
      "Validation epoch 3898 ; accuracy: 0.74; loss: 2.0977749824523926\n",
      "Training epoch 3899 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3899 ; accuracy: 0.74; loss: 2.0978760719299316\n",
      "Training epoch 3900 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3900 ; accuracy: 0.74; loss: 2.097970724105835\n",
      "Training epoch 3901 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3901 ; accuracy: 0.74; loss: 2.098071575164795\n",
      "Training epoch 3902 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3902 ; accuracy: 0.74; loss: 2.098163366317749\n",
      "Training epoch 3903 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3903 ; accuracy: 0.74; loss: 2.098270893096924\n",
      "Training epoch 3904 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3904 ; accuracy: 0.74; loss: 2.098376989364624\n",
      "Training epoch 3905 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3905 ; accuracy: 0.74; loss: 2.09847092628479\n",
      "Training epoch 3906 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3906 ; accuracy: 0.74; loss: 2.0985612869262695\n",
      "Training epoch 3907 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3907 ; accuracy: 0.74; loss: 2.098646879196167\n",
      "Training epoch 3908 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3908 ; accuracy: 0.74; loss: 2.098724603652954\n",
      "Training epoch 3909 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3909 ; accuracy: 0.74; loss: 2.098799705505371\n",
      "Training epoch 3910 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3910 ; accuracy: 0.74; loss: 2.098874092102051\n",
      "Training epoch 3911 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3911 ; accuracy: 0.74; loss: 2.0989463329315186\n",
      "Training epoch 3912 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3912 ; accuracy: 0.74; loss: 2.099025011062622\n",
      "Training epoch 3913 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3913 ; accuracy: 0.74; loss: 2.0990965366363525\n",
      "Training epoch 3914 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3914 ; accuracy: 0.74; loss: 2.0991714000701904\n",
      "Training epoch 3915 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3915 ; accuracy: 0.74; loss: 2.0992467403411865\n",
      "Training epoch 3916 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3916 ; accuracy: 0.74; loss: 2.0993199348449707\n",
      "Training epoch 3917 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3917 ; accuracy: 0.74; loss: 2.099391460418701\n",
      "Training epoch 3918 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3918 ; accuracy: 0.74; loss: 2.099461317062378\n",
      "Training epoch 3919 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3919 ; accuracy: 0.74; loss: 2.099536180496216\n",
      "Training epoch 3920 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3920 ; accuracy: 0.74; loss: 2.0996108055114746\n",
      "Training epoch 3921 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3921 ; accuracy: 0.74; loss: 2.0996830463409424\n",
      "Training epoch 3922 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3922 ; accuracy: 0.74; loss: 2.0997471809387207\n",
      "Training epoch 3923 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3923 ; accuracy: 0.74; loss: 2.0998120307922363\n",
      "Training epoch 3924 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3924 ; accuracy: 0.74; loss: 2.0998847484588623\n",
      "Training epoch 3925 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3925 ; accuracy: 0.74; loss: 2.0999605655670166\n",
      "Training epoch 3926 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3926 ; accuracy: 0.74; loss: 2.100038766860962\n",
      "Training epoch 3927 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3927 ; accuracy: 0.74; loss: 2.1001131534576416\n",
      "Training epoch 3928 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3928 ; accuracy: 0.74; loss: 2.100219488143921\n",
      "Training epoch 3929 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3929 ; accuracy: 0.74; loss: 2.100320816040039\n",
      "Training epoch 3930 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3930 ; accuracy: 0.74; loss: 2.1004230976104736\n",
      "Training epoch 3931 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3931 ; accuracy: 0.74; loss: 2.100520372390747\n",
      "Training epoch 3932 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3932 ; accuracy: 0.74; loss: 2.100612163543701\n",
      "Training epoch 3933 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3933 ; accuracy: 0.74; loss: 2.100710153579712\n",
      "Training epoch 3934 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3934 ; accuracy: 0.74; loss: 2.1008102893829346\n",
      "Training epoch 3935 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3935 ; accuracy: 0.74; loss: 2.1009020805358887\n",
      "Training epoch 3936 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3936 ; accuracy: 0.74; loss: 2.1009864807128906\n",
      "Training epoch 3937 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3937 ; accuracy: 0.74; loss: 2.1010706424713135\n",
      "Training epoch 3938 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3938 ; accuracy: 0.74; loss: 2.1011500358581543\n",
      "Training epoch 3939 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3939 ; accuracy: 0.74; loss: 2.1012320518493652\n",
      "Training epoch 3940 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3940 ; accuracy: 0.74; loss: 2.1013126373291016\n",
      "Training epoch 3941 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3941 ; accuracy: 0.74; loss: 2.101386070251465\n",
      "Training epoch 3942 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3942 ; accuracy: 0.74; loss: 2.1014559268951416\n",
      "Training epoch 3943 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3943 ; accuracy: 0.74; loss: 2.1015186309814453\n",
      "Training epoch 3944 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3944 ; accuracy: 0.74; loss: 2.1015756130218506\n",
      "Training epoch 3945 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3945 ; accuracy: 0.74; loss: 2.101635217666626\n",
      "Training epoch 3946 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3946 ; accuracy: 0.74; loss: 2.101696014404297\n",
      "Training epoch 3947 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3947 ; accuracy: 0.74; loss: 2.1017558574676514\n",
      "Training epoch 3948 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3948 ; accuracy: 0.74; loss: 2.1018097400665283\n",
      "Training epoch 3949 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3949 ; accuracy: 0.74; loss: 2.1018669605255127\n",
      "Training epoch 3950 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3950 ; accuracy: 0.74; loss: 2.1019272804260254\n",
      "Training epoch 3951 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3951 ; accuracy: 0.74; loss: 2.10198712348938\n",
      "Training epoch 3952 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3952 ; accuracy: 0.74; loss: 2.1020476818084717\n",
      "Training epoch 3953 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3953 ; accuracy: 0.74; loss: 2.102109432220459\n",
      "Training epoch 3954 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3954 ; accuracy: 0.74; loss: 2.1021790504455566\n",
      "Training epoch 3955 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3955 ; accuracy: 0.74; loss: 2.1022469997406006\n",
      "Training epoch 3956 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3956 ; accuracy: 0.74; loss: 2.102311372756958\n",
      "Training epoch 3957 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3957 ; accuracy: 0.74; loss: 2.102370023727417\n",
      "Training epoch 3958 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3958 ; accuracy: 0.74; loss: 2.1024279594421387\n",
      "Training epoch 3959 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3959 ; accuracy: 0.74; loss: 2.1024770736694336\n",
      "Training epoch 3960 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3960 ; accuracy: 0.74; loss: 2.1025280952453613\n",
      "Training epoch 3961 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3961 ; accuracy: 0.74; loss: 2.102581262588501\n",
      "Training epoch 3962 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3962 ; accuracy: 0.74; loss: 2.1026337146759033\n",
      "Training epoch 3963 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3963 ; accuracy: 0.74; loss: 2.1026864051818848\n",
      "Training epoch 3964 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3964 ; accuracy: 0.74; loss: 2.102738857269287\n",
      "Training epoch 3965 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3965 ; accuracy: 0.74; loss: 2.1027915477752686\n",
      "Training epoch 3966 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3966 ; accuracy: 0.74; loss: 2.1028430461883545\n",
      "Training epoch 3967 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3967 ; accuracy: 0.74; loss: 2.1028878688812256\n",
      "Training epoch 3968 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3968 ; accuracy: 0.74; loss: 2.1029415130615234\n",
      "Training epoch 3969 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3969 ; accuracy: 0.74; loss: 2.1029927730560303\n",
      "Training epoch 3970 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3970 ; accuracy: 0.74; loss: 2.1030349731445312\n",
      "Training epoch 3971 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3971 ; accuracy: 0.74; loss: 2.103086233139038\n",
      "Training epoch 3972 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3972 ; accuracy: 0.74; loss: 2.1031312942504883\n",
      "Training epoch 3973 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 3973 ; accuracy: 0.74; loss: 2.1031768321990967\n",
      "Training epoch 3974 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3974 ; accuracy: 0.74; loss: 2.1032278537750244\n",
      "Training epoch 3975 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3975 ; accuracy: 0.74; loss: 2.103278398513794\n",
      "Training epoch 3976 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3976 ; accuracy: 0.74; loss: 2.1033151149749756\n",
      "Training epoch 3977 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 3977 ; accuracy: 0.74; loss: 2.103367805480957\n",
      "Training epoch 3978 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3978 ; accuracy: 0.74; loss: 2.1034178733825684\n",
      "Training epoch 3979 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3979 ; accuracy: 0.74; loss: 2.1034622192382812\n",
      "Training epoch 3980 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3980 ; accuracy: 0.74; loss: 2.1035051345825195\n",
      "Training epoch 3981 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3981 ; accuracy: 0.74; loss: 2.10355281829834\n",
      "Training epoch 3982 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3982 ; accuracy: 0.74; loss: 2.10359263420105\n",
      "Training epoch 3983 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3983 ; accuracy: 0.74; loss: 2.1036312580108643\n",
      "Training epoch 3984 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3984 ; accuracy: 0.74; loss: 2.103667736053467\n",
      "Training epoch 3985 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 3985 ; accuracy: 0.74; loss: 2.1037049293518066\n",
      "Training epoch 3986 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3986 ; accuracy: 0.74; loss: 2.103745222091675\n",
      "Training epoch 3987 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3987 ; accuracy: 0.74; loss: 2.1037909984588623\n",
      "Training epoch 3988 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3988 ; accuracy: 0.74; loss: 2.103832483291626\n",
      "Training epoch 3989 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3989 ; accuracy: 0.74; loss: 2.1038782596588135\n",
      "Training epoch 3990 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3990 ; accuracy: 0.74; loss: 2.1039366722106934\n",
      "Training epoch 3991 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3991 ; accuracy: 0.74; loss: 2.103989839553833\n",
      "Training epoch 3992 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3992 ; accuracy: 0.74; loss: 2.1040420532226562\n",
      "Training epoch 3993 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3993 ; accuracy: 0.74; loss: 2.1040918827056885\n",
      "Training epoch 3994 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 3994 ; accuracy: 0.74; loss: 2.1041512489318848\n",
      "Training epoch 3995 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3995 ; accuracy: 0.74; loss: 2.104203939437866\n",
      "Training epoch 3996 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 3996 ; accuracy: 0.74; loss: 2.1042654514312744\n",
      "Training epoch 3997 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3997 ; accuracy: 0.74; loss: 2.1043195724487305\n",
      "Training epoch 3998 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 3998 ; accuracy: 0.74; loss: 2.1043713092803955\n",
      "Training epoch 3999 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 3999 ; accuracy: 0.74; loss: 2.1044206619262695\n",
      "Training epoch 4000 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4000 ; accuracy: 0.74; loss: 2.1044609546661377\n",
      "Training epoch 4001 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4001 ; accuracy: 0.74; loss: 2.104503870010376\n",
      "Training epoch 4002 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4002 ; accuracy: 0.74; loss: 2.1045374870300293\n",
      "Training epoch 4003 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4003 ; accuracy: 0.74; loss: 2.1045544147491455\n",
      "Training epoch 4004 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4004 ; accuracy: 0.74; loss: 2.104565143585205\n",
      "Training epoch 4005 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4005 ; accuracy: 0.74; loss: 2.104581117630005\n",
      "Training epoch 4006 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4006 ; accuracy: 0.74; loss: 2.1045968532562256\n",
      "Training epoch 4007 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4007 ; accuracy: 0.74; loss: 2.104616165161133\n",
      "Training epoch 4008 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4008 ; accuracy: 0.74; loss: 2.104640245437622\n",
      "Training epoch 4009 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 4009 ; accuracy: 0.74; loss: 2.1047203540802\n",
      "Training epoch 4010 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4010 ; accuracy: 0.74; loss: 2.1048052310943604\n",
      "Training epoch 4011 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4011 ; accuracy: 0.74; loss: 2.1048812866210938\n",
      "Training epoch 4012 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4012 ; accuracy: 0.74; loss: 2.104965925216675\n",
      "Training epoch 4013 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4013 ; accuracy: 0.74; loss: 2.1050477027893066\n",
      "Training epoch 4014 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4014 ; accuracy: 0.74; loss: 2.105125904083252\n",
      "Training epoch 4015 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4015 ; accuracy: 0.74; loss: 2.105201244354248\n",
      "Training epoch 4016 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4016 ; accuracy: 0.74; loss: 2.1052799224853516\n",
      "Training epoch 4017 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4017 ; accuracy: 0.74; loss: 2.1053550243377686\n",
      "Training epoch 4018 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4018 ; accuracy: 0.74; loss: 2.1054210662841797\n",
      "Training epoch 4019 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4019 ; accuracy: 0.74; loss: 2.1054818630218506\n",
      "Training epoch 4020 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4020 ; accuracy: 0.74; loss: 2.1055402755737305\n",
      "Training epoch 4021 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 4021 ; accuracy: 0.74; loss: 2.1056628227233887\n",
      "Training epoch 4022 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4022 ; accuracy: 0.74; loss: 2.1057794094085693\n",
      "Training epoch 4023 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4023 ; accuracy: 0.74; loss: 2.1058924198150635\n",
      "Training epoch 4024 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4024 ; accuracy: 0.74; loss: 2.105990409851074\n",
      "Training epoch 4025 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4025 ; accuracy: 0.74; loss: 2.106088161468506\n",
      "Training epoch 4026 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4026 ; accuracy: 0.74; loss: 2.1061747074127197\n",
      "Training epoch 4027 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4027 ; accuracy: 0.74; loss: 2.106257915496826\n",
      "Training epoch 4028 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4028 ; accuracy: 0.74; loss: 2.1063411235809326\n",
      "Training epoch 4029 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4029 ; accuracy: 0.74; loss: 2.1064224243164062\n",
      "Training epoch 4030 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4030 ; accuracy: 0.74; loss: 2.106502056121826\n",
      "Training epoch 4031 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4031 ; accuracy: 0.74; loss: 2.106572151184082\n",
      "Training epoch 4032 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4032 ; accuracy: 0.74; loss: 2.10663104057312\n",
      "Training epoch 4033 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4033 ; accuracy: 0.74; loss: 2.1066906452178955\n",
      "Training epoch 4034 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4034 ; accuracy: 0.74; loss: 2.1067490577697754\n",
      "Training epoch 4035 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4035 ; accuracy: 0.74; loss: 2.106809377670288\n",
      "Training epoch 4036 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4036 ; accuracy: 0.74; loss: 2.106875419616699\n",
      "Training epoch 4037 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4037 ; accuracy: 0.74; loss: 2.10693097114563\n",
      "Training epoch 4038 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4038 ; accuracy: 0.74; loss: 2.1069860458374023\n",
      "Training epoch 4039 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4039 ; accuracy: 0.74; loss: 2.107038974761963\n",
      "Training epoch 4040 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4040 ; accuracy: 0.74; loss: 2.1070950031280518\n",
      "Training epoch 4041 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4041 ; accuracy: 0.74; loss: 2.1071383953094482\n",
      "Training epoch 4042 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4042 ; accuracy: 0.74; loss: 2.1071853637695312\n",
      "Training epoch 4043 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4043 ; accuracy: 0.74; loss: 2.107229709625244\n",
      "Training epoch 4044 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4044 ; accuracy: 0.74; loss: 2.107271194458008\n",
      "Training epoch 4045 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4045 ; accuracy: 0.74; loss: 2.107316017150879\n",
      "Training epoch 4046 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4046 ; accuracy: 0.74; loss: 2.107356548309326\n",
      "Training epoch 4047 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4047 ; accuracy: 0.74; loss: 2.1074059009552\n",
      "Training epoch 4048 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4048 ; accuracy: 0.74; loss: 2.1074459552764893\n",
      "Training epoch 4049 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4049 ; accuracy: 0.74; loss: 2.1074914932250977\n",
      "Training epoch 4050 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4050 ; accuracy: 0.74; loss: 2.107532501220703\n",
      "Training epoch 4051 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4051 ; accuracy: 0.74; loss: 2.107576847076416\n",
      "Training epoch 4052 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4052 ; accuracy: 0.74; loss: 2.10762619972229\n",
      "Training epoch 4053 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4053 ; accuracy: 0.74; loss: 2.1076691150665283\n",
      "Training epoch 4054 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4054 ; accuracy: 0.74; loss: 2.10771107673645\n",
      "Training epoch 4055 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4055 ; accuracy: 0.74; loss: 2.1077473163604736\n",
      "Training epoch 4056 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4056 ; accuracy: 0.74; loss: 2.107787609100342\n",
      "Training epoch 4057 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4057 ; accuracy: 0.74; loss: 2.10783314704895\n",
      "Training epoch 4058 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4058 ; accuracy: 0.74; loss: 2.107895851135254\n",
      "Training epoch 4059 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4059 ; accuracy: 0.74; loss: 2.1079940795898438\n",
      "Training epoch 4060 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4060 ; accuracy: 0.74; loss: 2.108084201812744\n",
      "Training epoch 4061 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4061 ; accuracy: 0.74; loss: 2.108166456222534\n",
      "Training epoch 4062 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4062 ; accuracy: 0.74; loss: 2.1082451343536377\n",
      "Training epoch 4063 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4063 ; accuracy: 0.74; loss: 2.1083202362060547\n",
      "Training epoch 4064 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4064 ; accuracy: 0.74; loss: 2.1083879470825195\n",
      "Training epoch 4065 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4065 ; accuracy: 0.74; loss: 2.1084423065185547\n",
      "Training epoch 4066 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4066 ; accuracy: 0.74; loss: 2.108492136001587\n",
      "Training epoch 4067 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4067 ; accuracy: 0.74; loss: 2.1085424423217773\n",
      "Training epoch 4068 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4068 ; accuracy: 0.74; loss: 2.1085896492004395\n",
      "Training epoch 4069 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4069 ; accuracy: 0.74; loss: 2.108638048171997\n",
      "Training epoch 4070 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4070 ; accuracy: 0.74; loss: 2.108680486679077\n",
      "Training epoch 4071 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4071 ; accuracy: 0.74; loss: 2.1087257862091064\n",
      "Training epoch 4072 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4072 ; accuracy: 0.74; loss: 2.108769416809082\n",
      "Training epoch 4073 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4073 ; accuracy: 0.74; loss: 2.1088194847106934\n",
      "Training epoch 4074 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4074 ; accuracy: 0.74; loss: 2.108867645263672\n",
      "Training epoch 4075 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4075 ; accuracy: 0.74; loss: 2.1089212894439697\n",
      "Training epoch 4076 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4076 ; accuracy: 0.74; loss: 2.1089730262756348\n",
      "Training epoch 4077 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4077 ; accuracy: 0.74; loss: 2.109027624130249\n",
      "Training epoch 4078 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4078 ; accuracy: 0.74; loss: 2.1090829372406006\n",
      "Training epoch 4079 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4079 ; accuracy: 0.74; loss: 2.1091339588165283\n",
      "Training epoch 4080 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4080 ; accuracy: 0.74; loss: 2.1091866493225098\n",
      "Training epoch 4081 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4081 ; accuracy: 0.74; loss: 2.10923433303833\n",
      "Training epoch 4082 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4082 ; accuracy: 0.74; loss: 2.109281301498413\n",
      "Training epoch 4083 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4083 ; accuracy: 0.74; loss: 2.1093273162841797\n",
      "Training epoch 4084 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4084 ; accuracy: 0.74; loss: 2.1093857288360596\n",
      "Training epoch 4085 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4085 ; accuracy: 0.74; loss: 2.1094446182250977\n",
      "Training epoch 4086 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4086 ; accuracy: 0.74; loss: 2.109499216079712\n",
      "Training epoch 4087 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4087 ; accuracy: 0.74; loss: 2.109558343887329\n",
      "Training epoch 4088 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4088 ; accuracy: 0.74; loss: 2.109618902206421\n",
      "Training epoch 4089 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4089 ; accuracy: 0.74; loss: 2.1096818447113037\n",
      "Training epoch 4090 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4090 ; accuracy: 0.74; loss: 2.1097378730773926\n",
      "Training epoch 4091 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4091 ; accuracy: 0.74; loss: 2.1097967624664307\n",
      "Training epoch 4092 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4092 ; accuracy: 0.74; loss: 2.1098527908325195\n",
      "Training epoch 4093 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4093 ; accuracy: 0.74; loss: 2.109910726547241\n",
      "Training epoch 4094 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4094 ; accuracy: 0.74; loss: 2.109966993331909\n",
      "Training epoch 4095 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4095 ; accuracy: 0.74; loss: 2.1100282669067383\n",
      "Training epoch 4096 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4096 ; accuracy: 0.74; loss: 2.110086441040039\n",
      "Training epoch 4097 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4097 ; accuracy: 0.74; loss: 2.110135793685913\n",
      "Training epoch 4098 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4098 ; accuracy: 0.74; loss: 2.1101794242858887\n",
      "Training epoch 4099 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4099 ; accuracy: 0.74; loss: 2.1102211475372314\n",
      "Training epoch 4100 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4100 ; accuracy: 0.74; loss: 2.110259771347046\n",
      "Training epoch 4101 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4101 ; accuracy: 0.74; loss: 2.1103081703186035\n",
      "Training epoch 4102 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4102 ; accuracy: 0.74; loss: 2.110363006591797\n",
      "Training epoch 4103 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4103 ; accuracy: 0.74; loss: 2.1104111671447754\n",
      "Training epoch 4104 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4104 ; accuracy: 0.74; loss: 2.11045503616333\n",
      "Training epoch 4105 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4105 ; accuracy: 0.74; loss: 2.1105058193206787\n",
      "Training epoch 4106 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4106 ; accuracy: 0.74; loss: 2.11055326461792\n",
      "Training epoch 4107 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4107 ; accuracy: 0.74; loss: 2.110600471496582\n",
      "Training epoch 4108 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4108 ; accuracy: 0.74; loss: 2.1106488704681396\n",
      "Training epoch 4109 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4109 ; accuracy: 0.74; loss: 2.110691547393799\n",
      "Training epoch 4110 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4110 ; accuracy: 0.74; loss: 2.1107349395751953\n",
      "Training epoch 4111 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4111 ; accuracy: 0.74; loss: 2.1107821464538574\n",
      "Training epoch 4112 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4112 ; accuracy: 0.74; loss: 2.110830783843994\n",
      "Training epoch 4113 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4113 ; accuracy: 0.74; loss: 2.1108808517456055\n",
      "Training epoch 4114 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4114 ; accuracy: 0.74; loss: 2.11092209815979\n",
      "Training epoch 4115 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4115 ; accuracy: 0.74; loss: 2.110970973968506\n",
      "Training epoch 4116 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4116 ; accuracy: 0.74; loss: 2.1110236644744873\n",
      "Training epoch 4117 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4117 ; accuracy: 0.74; loss: 2.111071825027466\n",
      "Training epoch 4118 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4118 ; accuracy: 0.74; loss: 2.1111137866973877\n",
      "Training epoch 4119 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4119 ; accuracy: 0.74; loss: 2.11116361618042\n",
      "Training epoch 4120 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4120 ; accuracy: 0.74; loss: 2.111212730407715\n",
      "Training epoch 4121 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4121 ; accuracy: 0.74; loss: 2.111264944076538\n",
      "Training epoch 4122 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4122 ; accuracy: 0.74; loss: 2.1113195419311523\n",
      "Training epoch 4123 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4123 ; accuracy: 0.74; loss: 2.1113762855529785\n",
      "Training epoch 4124 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4124 ; accuracy: 0.74; loss: 2.1114323139190674\n",
      "Training epoch 4125 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4125 ; accuracy: 0.74; loss: 2.1114914417266846\n",
      "Training epoch 4126 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4126 ; accuracy: 0.74; loss: 2.11155104637146\n",
      "Training epoch 4127 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4127 ; accuracy: 0.74; loss: 2.1116089820861816\n",
      "Training epoch 4128 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4128 ; accuracy: 0.74; loss: 2.111665964126587\n",
      "Training epoch 4129 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4129 ; accuracy: 0.74; loss: 2.1117210388183594\n",
      "Training epoch 4130 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4130 ; accuracy: 0.74; loss: 2.1117870807647705\n",
      "Training epoch 4131 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4131 ; accuracy: 0.74; loss: 2.1118462085723877\n",
      "Training epoch 4132 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4132 ; accuracy: 0.74; loss: 2.111898422241211\n",
      "Training epoch 4133 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4133 ; accuracy: 0.74; loss: 2.111945867538452\n",
      "Training epoch 4134 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4134 ; accuracy: 0.74; loss: 2.1119954586029053\n",
      "Training epoch 4135 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4135 ; accuracy: 0.74; loss: 2.1120431423187256\n",
      "Training epoch 4136 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4136 ; accuracy: 0.74; loss: 2.1120903491973877\n",
      "Training epoch 4137 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4137 ; accuracy: 0.74; loss: 2.112139940261841\n",
      "Training epoch 4138 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4138 ; accuracy: 0.74; loss: 2.1121983528137207\n",
      "Training epoch 4139 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4139 ; accuracy: 0.74; loss: 2.1122543811798096\n",
      "Training epoch 4140 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4140 ; accuracy: 0.74; loss: 2.112309217453003\n",
      "Training epoch 4141 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4141 ; accuracy: 0.74; loss: 2.112370252609253\n",
      "Training epoch 4142 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4142 ; accuracy: 0.74; loss: 2.112431049346924\n",
      "Training epoch 4143 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4143 ; accuracy: 0.74; loss: 2.11248517036438\n",
      "Training epoch 4144 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4144 ; accuracy: 0.74; loss: 2.1125404834747314\n",
      "Training epoch 4145 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4145 ; accuracy: 0.74; loss: 2.1125965118408203\n",
      "Training epoch 4146 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4146 ; accuracy: 0.74; loss: 2.112651824951172\n",
      "Training epoch 4147 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4147 ; accuracy: 0.74; loss: 2.1127073764801025\n",
      "Training epoch 4148 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4148 ; accuracy: 0.74; loss: 2.112764596939087\n",
      "Training epoch 4149 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4149 ; accuracy: 0.74; loss: 2.1128170490264893\n",
      "Training epoch 4150 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4150 ; accuracy: 0.74; loss: 2.112865686416626\n",
      "Training epoch 4151 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4151 ; accuracy: 0.74; loss: 2.1129114627838135\n",
      "Training epoch 4152 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4152 ; accuracy: 0.74; loss: 2.112952709197998\n",
      "Training epoch 4153 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4153 ; accuracy: 0.74; loss: 2.1129956245422363\n",
      "Training epoch 4154 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4154 ; accuracy: 0.74; loss: 2.113039970397949\n",
      "Training epoch 4155 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4155 ; accuracy: 0.74; loss: 2.113083600997925\n",
      "Training epoch 4156 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4156 ; accuracy: 0.74; loss: 2.1131370067596436\n",
      "Training epoch 4157 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4157 ; accuracy: 0.74; loss: 2.11319637298584\n",
      "Training epoch 4158 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4158 ; accuracy: 0.74; loss: 2.1132469177246094\n",
      "Training epoch 4159 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4159 ; accuracy: 0.74; loss: 2.113300323486328\n",
      "Training epoch 4160 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4160 ; accuracy: 0.74; loss: 2.1133453845977783\n",
      "Training epoch 4161 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4161 ; accuracy: 0.74; loss: 2.113379716873169\n",
      "Training epoch 4162 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4162 ; accuracy: 0.74; loss: 2.1133992671966553\n",
      "Training epoch 4163 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4163 ; accuracy: 0.74; loss: 2.113424777984619\n",
      "Training epoch 4164 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4164 ; accuracy: 0.74; loss: 2.113450050354004\n",
      "Training epoch 4165 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4165 ; accuracy: 0.74; loss: 2.1134731769561768\n",
      "Training epoch 4166 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4166 ; accuracy: 0.74; loss: 2.1135001182556152\n",
      "Training epoch 4167 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4167 ; accuracy: 0.74; loss: 2.1135289669036865\n",
      "Training epoch 4168 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4168 ; accuracy: 0.74; loss: 2.113560199737549\n",
      "Training epoch 4169 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4169 ; accuracy: 0.74; loss: 2.1135940551757812\n",
      "Training epoch 4170 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4170 ; accuracy: 0.74; loss: 2.1136274337768555\n",
      "Training epoch 4171 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4171 ; accuracy: 0.74; loss: 2.113668441772461\n",
      "Training epoch 4172 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4172 ; accuracy: 0.74; loss: 2.1137075424194336\n",
      "Training epoch 4173 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4173 ; accuracy: 0.74; loss: 2.1137428283691406\n",
      "Training epoch 4174 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4174 ; accuracy: 0.74; loss: 2.113779306411743\n",
      "Training epoch 4175 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4175 ; accuracy: 0.74; loss: 2.113811731338501\n",
      "Training epoch 4176 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4176 ; accuracy: 0.74; loss: 2.1138439178466797\n",
      "Training epoch 4177 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4177 ; accuracy: 0.74; loss: 2.1138815879821777\n",
      "Training epoch 4178 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4178 ; accuracy: 0.74; loss: 2.1139159202575684\n",
      "Training epoch 4179 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4179 ; accuracy: 0.74; loss: 2.1139509677886963\n",
      "Training epoch 4180 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4180 ; accuracy: 0.74; loss: 2.1139862537384033\n",
      "Training epoch 4181 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4181 ; accuracy: 0.74; loss: 2.1140172481536865\n",
      "Training epoch 4182 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4182 ; accuracy: 0.74; loss: 2.1140551567077637\n",
      "Training epoch 4183 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4183 ; accuracy: 0.74; loss: 2.1140878200531006\n",
      "Training epoch 4184 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4184 ; accuracy: 0.74; loss: 2.114125967025757\n",
      "Training epoch 4185 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4185 ; accuracy: 0.74; loss: 2.114171266555786\n",
      "Training epoch 4186 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4186 ; accuracy: 0.74; loss: 2.11421799659729\n",
      "Training epoch 4187 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4187 ; accuracy: 0.74; loss: 2.114264488220215\n",
      "Training epoch 4188 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4188 ; accuracy: 0.74; loss: 2.114306688308716\n",
      "Training epoch 4189 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4189 ; accuracy: 0.74; loss: 2.1143476963043213\n",
      "Training epoch 4190 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4190 ; accuracy: 0.74; loss: 2.114396810531616\n",
      "Training epoch 4191 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4191 ; accuracy: 0.74; loss: 2.1144447326660156\n",
      "Training epoch 4192 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4192 ; accuracy: 0.74; loss: 2.1144981384277344\n",
      "Training epoch 4193 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4193 ; accuracy: 0.74; loss: 2.1145479679107666\n",
      "Training epoch 4194 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4194 ; accuracy: 0.74; loss: 2.1145968437194824\n",
      "Training epoch 4195 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4195 ; accuracy: 0.74; loss: 2.114642381668091\n",
      "Training epoch 4196 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4196 ; accuracy: 0.74; loss: 2.114689588546753\n",
      "Training epoch 4197 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4197 ; accuracy: 0.74; loss: 2.114739179611206\n",
      "Training epoch 4198 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4198 ; accuracy: 0.74; loss: 2.1147847175598145\n",
      "Training epoch 4199 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4199 ; accuracy: 0.74; loss: 2.1148314476013184\n",
      "Training epoch 4200 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4200 ; accuracy: 0.74; loss: 2.1148815155029297\n",
      "Training epoch 4201 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4201 ; accuracy: 0.74; loss: 2.114931344985962\n",
      "Training epoch 4202 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4202 ; accuracy: 0.74; loss: 2.114976167678833\n",
      "Training epoch 4203 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4203 ; accuracy: 0.74; loss: 2.115016222000122\n",
      "Training epoch 4204 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4204 ; accuracy: 0.74; loss: 2.1150548458099365\n",
      "Training epoch 4205 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4205 ; accuracy: 0.74; loss: 2.1150941848754883\n",
      "Training epoch 4206 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4206 ; accuracy: 0.74; loss: 2.115135908126831\n",
      "Training epoch 4207 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4207 ; accuracy: 0.74; loss: 2.1151795387268066\n",
      "Training epoch 4208 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4208 ; accuracy: 0.74; loss: 2.1152184009552\n",
      "Training epoch 4209 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4209 ; accuracy: 0.74; loss: 2.1152572631835938\n",
      "Training epoch 4210 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4210 ; accuracy: 0.74; loss: 2.115302324295044\n",
      "Training epoch 4211 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4211 ; accuracy: 0.74; loss: 2.115349054336548\n",
      "Training epoch 4212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4212 ; accuracy: 0.74; loss: 2.115396022796631\n",
      "Training epoch 4213 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4213 ; accuracy: 0.74; loss: 2.1154448986053467\n",
      "Training epoch 4214 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4214 ; accuracy: 0.74; loss: 2.1154944896698\n",
      "Training epoch 4215 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4215 ; accuracy: 0.74; loss: 2.1155407428741455\n",
      "Training epoch 4216 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4216 ; accuracy: 0.74; loss: 2.1155872344970703\n",
      "Training epoch 4217 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4217 ; accuracy: 0.74; loss: 2.115631103515625\n",
      "Training epoch 4218 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4218 ; accuracy: 0.74; loss: 2.115675449371338\n",
      "Training epoch 4219 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4219 ; accuracy: 0.74; loss: 2.115723133087158\n",
      "Training epoch 4220 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4220 ; accuracy: 0.74; loss: 2.115776777267456\n",
      "Training epoch 4221 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4221 ; accuracy: 0.74; loss: 2.115830421447754\n",
      "Training epoch 4222 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4222 ; accuracy: 0.74; loss: 2.115907907485962\n",
      "Training epoch 4223 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4223 ; accuracy: 0.74; loss: 2.1159849166870117\n",
      "Training epoch 4224 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4224 ; accuracy: 0.74; loss: 2.116055488586426\n",
      "Training epoch 4225 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4225 ; accuracy: 0.74; loss: 2.1161229610443115\n",
      "Training epoch 4226 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4226 ; accuracy: 0.74; loss: 2.1161935329437256\n",
      "Training epoch 4227 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4227 ; accuracy: 0.74; loss: 2.116253137588501\n",
      "Training epoch 4228 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4228 ; accuracy: 0.74; loss: 2.116304636001587\n",
      "Training epoch 4229 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4229 ; accuracy: 0.74; loss: 2.1163651943206787\n",
      "Training epoch 4230 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4230 ; accuracy: 0.74; loss: 2.1164205074310303\n",
      "Training epoch 4231 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 4231 ; accuracy: 0.74; loss: 2.116515636444092\n",
      "Training epoch 4232 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4232 ; accuracy: 0.74; loss: 2.116607666015625\n",
      "Training epoch 4233 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4233 ; accuracy: 0.74; loss: 2.1166982650756836\n",
      "Training epoch 4234 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4234 ; accuracy: 0.74; loss: 2.116785764694214\n",
      "Training epoch 4235 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4235 ; accuracy: 0.74; loss: 2.1168675422668457\n",
      "Training epoch 4236 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4236 ; accuracy: 0.74; loss: 2.1169657707214355\n",
      "Training epoch 4237 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4237 ; accuracy: 0.74; loss: 2.117056131362915\n",
      "Training epoch 4238 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4238 ; accuracy: 0.74; loss: 2.1171395778656006\n",
      "Training epoch 4239 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4239 ; accuracy: 0.74; loss: 2.117215156555176\n",
      "Training epoch 4240 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4240 ; accuracy: 0.74; loss: 2.117290735244751\n",
      "Training epoch 4241 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4241 ; accuracy: 0.74; loss: 2.117356061935425\n",
      "Training epoch 4242 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4242 ; accuracy: 0.74; loss: 2.1174116134643555\n",
      "Training epoch 4243 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4243 ; accuracy: 0.74; loss: 2.117462158203125\n",
      "Training epoch 4244 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4244 ; accuracy: 0.74; loss: 2.1175084114074707\n",
      "Training epoch 4245 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4245 ; accuracy: 0.74; loss: 2.1175544261932373\n",
      "Training epoch 4246 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4246 ; accuracy: 0.74; loss: 2.1176016330718994\n",
      "Training epoch 4247 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4247 ; accuracy: 0.74; loss: 2.117645263671875\n",
      "Training epoch 4248 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4248 ; accuracy: 0.74; loss: 2.117684841156006\n",
      "Training epoch 4249 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4249 ; accuracy: 0.74; loss: 2.1177303791046143\n",
      "Training epoch 4250 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4250 ; accuracy: 0.74; loss: 2.1177823543548584\n",
      "Training epoch 4251 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4251 ; accuracy: 0.74; loss: 2.1178224086761475\n",
      "Training epoch 4252 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4252 ; accuracy: 0.74; loss: 2.117865800857544\n",
      "Training epoch 4253 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4253 ; accuracy: 0.74; loss: 2.1179087162017822\n",
      "Training epoch 4254 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4254 ; accuracy: 0.74; loss: 2.1179580688476562\n",
      "Training epoch 4255 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4255 ; accuracy: 0.74; loss: 2.118009567260742\n",
      "Training epoch 4256 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4256 ; accuracy: 0.74; loss: 2.118055582046509\n",
      "Training epoch 4257 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4257 ; accuracy: 0.74; loss: 2.118103504180908\n",
      "Training epoch 4258 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4258 ; accuracy: 0.74; loss: 2.1181540489196777\n",
      "Training epoch 4259 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4259 ; accuracy: 0.74; loss: 2.1182022094726562\n",
      "Training epoch 4260 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4260 ; accuracy: 0.74; loss: 2.118227481842041\n",
      "Training epoch 4261 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4261 ; accuracy: 0.74; loss: 2.1182596683502197\n",
      "Training epoch 4262 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4262 ; accuracy: 0.74; loss: 2.1182947158813477\n",
      "Training epoch 4263 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4263 ; accuracy: 0.74; loss: 2.118321418762207\n",
      "Training epoch 4264 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4264 ; accuracy: 0.74; loss: 2.1183531284332275\n",
      "Training epoch 4265 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4265 ; accuracy: 0.74; loss: 2.118384838104248\n",
      "Training epoch 4266 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4266 ; accuracy: 0.74; loss: 2.118412971496582\n",
      "Training epoch 4267 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4267 ; accuracy: 0.74; loss: 2.118440866470337\n",
      "Training epoch 4268 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4268 ; accuracy: 0.74; loss: 2.118468999862671\n",
      "Training epoch 4269 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4269 ; accuracy: 0.74; loss: 2.118499279022217\n",
      "Training epoch 4270 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4270 ; accuracy: 0.74; loss: 2.118532180786133\n",
      "Training epoch 4271 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4271 ; accuracy: 0.74; loss: 2.118570566177368\n",
      "Training epoch 4272 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4272 ; accuracy: 0.74; loss: 2.1186115741729736\n",
      "Training epoch 4273 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4273 ; accuracy: 0.74; loss: 2.118650197982788\n",
      "Training epoch 4274 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4274 ; accuracy: 0.74; loss: 2.1187021732330322\n",
      "Training epoch 4275 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4275 ; accuracy: 0.74; loss: 2.1187551021575928\n",
      "Training epoch 4276 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4276 ; accuracy: 0.74; loss: 2.1188066005706787\n",
      "Training epoch 4277 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4277 ; accuracy: 0.74; loss: 2.1188597679138184\n",
      "Training epoch 4278 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4278 ; accuracy: 0.74; loss: 2.118913412094116\n",
      "Training epoch 4279 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4279 ; accuracy: 0.74; loss: 2.118967294692993\n",
      "Training epoch 4280 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4280 ; accuracy: 0.74; loss: 2.119025945663452\n",
      "Training epoch 4281 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4281 ; accuracy: 0.74; loss: 2.1190757751464844\n",
      "Training epoch 4282 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4282 ; accuracy: 0.74; loss: 2.1191258430480957\n",
      "Training epoch 4283 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4283 ; accuracy: 0.74; loss: 2.1191751956939697\n",
      "Training epoch 4284 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4284 ; accuracy: 0.74; loss: 2.119218349456787\n",
      "Training epoch 4285 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4285 ; accuracy: 0.74; loss: 2.119270086288452\n",
      "Training epoch 4286 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4286 ; accuracy: 0.74; loss: 2.11932110786438\n",
      "Training epoch 4287 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4287 ; accuracy: 0.74; loss: 2.119382619857788\n",
      "Training epoch 4288 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4288 ; accuracy: 0.74; loss: 2.119443416595459\n",
      "Training epoch 4289 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4289 ; accuracy: 0.74; loss: 2.1195068359375\n",
      "Training epoch 4290 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4290 ; accuracy: 0.74; loss: 2.119556427001953\n",
      "Training epoch 4291 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4291 ; accuracy: 0.74; loss: 2.1196045875549316\n",
      "Training epoch 4292 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4292 ; accuracy: 0.74; loss: 2.119652271270752\n",
      "Training epoch 4293 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4293 ; accuracy: 0.74; loss: 2.1196987628936768\n",
      "Training epoch 4294 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4294 ; accuracy: 0.74; loss: 2.119739532470703\n",
      "Training epoch 4295 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4295 ; accuracy: 0.74; loss: 2.1197779178619385\n",
      "Training epoch 4296 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4296 ; accuracy: 0.74; loss: 2.1198182106018066\n",
      "Training epoch 4297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4297 ; accuracy: 0.74; loss: 2.1198506355285645\n",
      "Training epoch 4298 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4298 ; accuracy: 0.74; loss: 2.119884490966797\n",
      "Training epoch 4299 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4299 ; accuracy: 0.74; loss: 2.119920492172241\n",
      "Training epoch 4300 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4300 ; accuracy: 0.74; loss: 2.1199545860290527\n",
      "Training epoch 4301 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4301 ; accuracy: 0.74; loss: 2.1199862957000732\n",
      "Training epoch 4302 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4302 ; accuracy: 0.74; loss: 2.1200175285339355\n",
      "Training epoch 4303 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4303 ; accuracy: 0.74; loss: 2.120048999786377\n",
      "Training epoch 4304 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4304 ; accuracy: 0.74; loss: 2.1200828552246094\n",
      "Training epoch 4305 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4305 ; accuracy: 0.74; loss: 2.1201283931732178\n",
      "Training epoch 4306 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4306 ; accuracy: 0.74; loss: 2.12015700340271\n",
      "Training epoch 4307 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4307 ; accuracy: 0.74; loss: 2.120173692703247\n",
      "Training epoch 4308 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4308 ; accuracy: 0.74; loss: 2.1201910972595215\n",
      "Training epoch 4309 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4309 ; accuracy: 0.74; loss: 2.120208501815796\n",
      "Training epoch 4310 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4310 ; accuracy: 0.74; loss: 2.1202259063720703\n",
      "Training epoch 4311 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4311 ; accuracy: 0.74; loss: 2.120250701904297\n",
      "Training epoch 4312 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4312 ; accuracy: 0.74; loss: 2.120276689529419\n",
      "Training epoch 4313 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4313 ; accuracy: 0.74; loss: 2.1203038692474365\n",
      "Training epoch 4314 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4314 ; accuracy: 0.74; loss: 2.1203324794769287\n",
      "Training epoch 4315 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4315 ; accuracy: 0.74; loss: 2.120358467102051\n",
      "Training epoch 4316 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4316 ; accuracy: 0.74; loss: 2.1203837394714355\n",
      "Training epoch 4317 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4317 ; accuracy: 0.74; loss: 2.1204159259796143\n",
      "Training epoch 4318 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4318 ; accuracy: 0.74; loss: 2.120448350906372\n",
      "Training epoch 4319 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4319 ; accuracy: 0.74; loss: 2.1204891204833984\n",
      "Training epoch 4320 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4320 ; accuracy: 0.74; loss: 2.120530366897583\n",
      "Training epoch 4321 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4321 ; accuracy: 0.74; loss: 2.1205742359161377\n",
      "Training epoch 4322 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4322 ; accuracy: 0.74; loss: 2.120612144470215\n",
      "Training epoch 4323 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4323 ; accuracy: 0.74; loss: 2.1206533908843994\n",
      "Training epoch 4324 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4324 ; accuracy: 0.74; loss: 2.120692014694214\n",
      "Training epoch 4325 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4325 ; accuracy: 0.74; loss: 2.120727777481079\n",
      "Training epoch 4326 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4326 ; accuracy: 0.74; loss: 2.1207799911499023\n",
      "Training epoch 4327 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4327 ; accuracy: 0.74; loss: 2.120830774307251\n",
      "Training epoch 4328 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4328 ; accuracy: 0.74; loss: 2.1208837032318115\n",
      "Training epoch 4329 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4329 ; accuracy: 0.74; loss: 2.1209404468536377\n",
      "Training epoch 4330 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4330 ; accuracy: 0.74; loss: 2.1209986209869385\n",
      "Training epoch 4331 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4331 ; accuracy: 0.74; loss: 2.1210575103759766\n",
      "Training epoch 4332 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4332 ; accuracy: 0.74; loss: 2.121112823486328\n",
      "Training epoch 4333 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4333 ; accuracy: 0.74; loss: 2.121168851852417\n",
      "Training epoch 4334 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4334 ; accuracy: 0.74; loss: 2.121220588684082\n",
      "Training epoch 4335 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4335 ; accuracy: 0.74; loss: 2.121265411376953\n",
      "Training epoch 4336 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4336 ; accuracy: 0.74; loss: 2.121307373046875\n",
      "Training epoch 4337 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4337 ; accuracy: 0.74; loss: 2.121352434158325\n",
      "Training epoch 4338 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4338 ; accuracy: 0.74; loss: 2.121392011642456\n",
      "Training epoch 4339 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4339 ; accuracy: 0.74; loss: 2.12143874168396\n",
      "Training epoch 4340 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4340 ; accuracy: 0.74; loss: 2.1214828491210938\n",
      "Training epoch 4341 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4341 ; accuracy: 0.74; loss: 2.1215293407440186\n",
      "Training epoch 4342 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4342 ; accuracy: 0.74; loss: 2.121572256088257\n",
      "Training epoch 4343 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4343 ; accuracy: 0.74; loss: 2.121614694595337\n",
      "Training epoch 4344 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4344 ; accuracy: 0.74; loss: 2.1216607093811035\n",
      "Training epoch 4345 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4345 ; accuracy: 0.74; loss: 2.121716260910034\n",
      "Training epoch 4346 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4346 ; accuracy: 0.74; loss: 2.121774673461914\n",
      "Training epoch 4347 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4347 ; accuracy: 0.74; loss: 2.1218254566192627\n",
      "Training epoch 4348 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4348 ; accuracy: 0.74; loss: 2.1218791007995605\n",
      "Training epoch 4349 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4349 ; accuracy: 0.74; loss: 2.1219377517700195\n",
      "Training epoch 4350 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4350 ; accuracy: 0.74; loss: 2.121995449066162\n",
      "Training epoch 4351 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4351 ; accuracy: 0.74; loss: 2.1220510005950928\n",
      "Training epoch 4352 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4352 ; accuracy: 0.74; loss: 2.1220991611480713\n",
      "Training epoch 4353 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4353 ; accuracy: 0.74; loss: 2.122145891189575\n",
      "Training epoch 4354 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4354 ; accuracy: 0.74; loss: 2.122194290161133\n",
      "Training epoch 4355 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4355 ; accuracy: 0.74; loss: 2.1222400665283203\n",
      "Training epoch 4356 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 4356 ; accuracy: 0.74; loss: 2.1222856044769287\n",
      "Training epoch 4357 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4357 ; accuracy: 0.74; loss: 2.1223368644714355\n",
      "Training epoch 4358 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4358 ; accuracy: 0.74; loss: 2.1223883628845215\n",
      "Training epoch 4359 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4359 ; accuracy: 0.74; loss: 2.1224424839019775\n",
      "Training epoch 4360 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4360 ; accuracy: 0.74; loss: 2.1225011348724365\n",
      "Training epoch 4361 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4361 ; accuracy: 0.74; loss: 2.1225478649139404\n",
      "Training epoch 4362 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4362 ; accuracy: 0.74; loss: 2.1225950717926025\n",
      "Training epoch 4363 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4363 ; accuracy: 0.74; loss: 2.122619390487671\n",
      "Training epoch 4364 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4364 ; accuracy: 0.74; loss: 2.122636556625366\n",
      "Training epoch 4365 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4365 ; accuracy: 0.74; loss: 2.1226584911346436\n",
      "Training epoch 4366 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4366 ; accuracy: 0.74; loss: 2.1226820945739746\n",
      "Training epoch 4367 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4367 ; accuracy: 0.74; loss: 2.1227126121520996\n",
      "Training epoch 4368 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4368 ; accuracy: 0.74; loss: 2.122746229171753\n",
      "Training epoch 4369 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4369 ; accuracy: 0.74; loss: 2.122779130935669\n",
      "Training epoch 4370 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4370 ; accuracy: 0.74; loss: 2.122805595397949\n",
      "Training epoch 4371 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4371 ; accuracy: 0.74; loss: 2.1228201389312744\n",
      "Training epoch 4372 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 4372 ; accuracy: 0.74; loss: 2.1228785514831543\n",
      "Training epoch 4373 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4373 ; accuracy: 0.74; loss: 2.122936487197876\n",
      "Training epoch 4374 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4374 ; accuracy: 0.74; loss: 2.1229941844940186\n",
      "Training epoch 4375 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4375 ; accuracy: 0.74; loss: 2.1230523586273193\n",
      "Training epoch 4376 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4376 ; accuracy: 0.74; loss: 2.1231048107147217\n",
      "Training epoch 4377 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4377 ; accuracy: 0.74; loss: 2.1231746673583984\n",
      "Training epoch 4378 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4378 ; accuracy: 0.74; loss: 2.123258352279663\n",
      "Training epoch 4379 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4379 ; accuracy: 0.74; loss: 2.1233415603637695\n",
      "Training epoch 4380 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4380 ; accuracy: 0.74; loss: 2.1234195232391357\n",
      "Training epoch 4381 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4381 ; accuracy: 0.74; loss: 2.123483657836914\n",
      "Training epoch 4382 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4382 ; accuracy: 0.74; loss: 2.123549222946167\n",
      "Training epoch 4383 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4383 ; accuracy: 0.74; loss: 2.123615264892578\n",
      "Training epoch 4384 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4384 ; accuracy: 0.74; loss: 2.1236844062805176\n",
      "Training epoch 4385 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4385 ; accuracy: 0.74; loss: 2.1237494945526123\n",
      "Training epoch 4386 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4386 ; accuracy: 0.74; loss: 2.123812198638916\n",
      "Training epoch 4387 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 4387 ; accuracy: 0.74; loss: 2.1238062381744385\n",
      "Training epoch 4388 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4388 ; accuracy: 0.74; loss: 2.1238033771514893\n",
      "Training epoch 4389 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4389 ; accuracy: 0.74; loss: 2.1238040924072266\n",
      "Training epoch 4390 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4390 ; accuracy: 0.7433333333333333; loss: 2.1238174438476562\n",
      "Training epoch 4391 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4391 ; accuracy: 0.7433333333333333; loss: 2.123849391937256\n",
      "Training epoch 4392 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4392 ; accuracy: 0.7433333333333333; loss: 2.123885154724121\n",
      "Training epoch 4393 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4393 ; accuracy: 0.7433333333333333; loss: 2.123924732208252\n",
      "Training epoch 4394 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4394 ; accuracy: 0.7433333333333333; loss: 2.1239733695983887\n",
      "Training epoch 4395 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4395 ; accuracy: 0.7433333333333333; loss: 2.1240153312683105\n",
      "Training epoch 4396 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4396 ; accuracy: 0.7433333333333333; loss: 2.1240713596343994\n",
      "Training epoch 4397 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4397 ; accuracy: 0.7433333333333333; loss: 2.1241180896759033\n",
      "Training epoch 4398 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4398 ; accuracy: 0.7433333333333333; loss: 2.124164342880249\n",
      "Training epoch 4399 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4399 ; accuracy: 0.7433333333333333; loss: 2.124211072921753\n",
      "Training epoch 4400 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4400 ; accuracy: 0.7433333333333333; loss: 2.1242575645446777\n",
      "Training epoch 4401 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4401 ; accuracy: 0.7433333333333333; loss: 2.124305486679077\n",
      "Training epoch 4402 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4402 ; accuracy: 0.7433333333333333; loss: 2.1243529319763184\n",
      "Training epoch 4403 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4403 ; accuracy: 0.7433333333333333; loss: 2.124382972717285\n",
      "Training epoch 4404 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4404 ; accuracy: 0.7433333333333333; loss: 2.1244163513183594\n",
      "Training epoch 4405 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4405 ; accuracy: 0.7433333333333333; loss: 2.1244564056396484\n",
      "Training epoch 4406 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4406 ; accuracy: 0.7433333333333333; loss: 2.1244966983795166\n",
      "Training epoch 4407 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4407 ; accuracy: 0.7433333333333333; loss: 2.1245415210723877\n",
      "Training epoch 4408 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4408 ; accuracy: 0.7433333333333333; loss: 2.124582052230835\n",
      "Training epoch 4409 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4409 ; accuracy: 0.7433333333333333; loss: 2.1246280670166016\n",
      "Training epoch 4410 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4410 ; accuracy: 0.7433333333333333; loss: 2.1246776580810547\n",
      "Training epoch 4411 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4411 ; accuracy: 0.7433333333333333; loss: 2.1247236728668213\n",
      "Training epoch 4412 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4412 ; accuracy: 0.7433333333333333; loss: 2.124770402908325\n",
      "Training epoch 4413 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4413 ; accuracy: 0.7433333333333333; loss: 2.124824047088623\n",
      "Training epoch 4414 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4414 ; accuracy: 0.7433333333333333; loss: 2.124875783920288\n",
      "Training epoch 4415 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4415 ; accuracy: 0.7433333333333333; loss: 2.124922037124634\n",
      "Training epoch 4416 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4416 ; accuracy: 0.7433333333333333; loss: 2.124971628189087\n",
      "Training epoch 4417 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4417 ; accuracy: 0.7433333333333333; loss: 2.125018835067749\n",
      "Training epoch 4418 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4418 ; accuracy: 0.7466666666666667; loss: 2.125067949295044\n",
      "Training epoch 4419 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4419 ; accuracy: 0.7466666666666667; loss: 2.125110149383545\n",
      "Training epoch 4420 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4420 ; accuracy: 0.7466666666666667; loss: 2.1251533031463623\n",
      "Training epoch 4421 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4421 ; accuracy: 0.7466666666666667; loss: 2.125213861465454\n",
      "Training epoch 4422 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4422 ; accuracy: 0.7466666666666667; loss: 2.1252682209014893\n",
      "Training epoch 4423 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4423 ; accuracy: 0.7466666666666667; loss: 2.1253185272216797\n",
      "Training epoch 4424 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4424 ; accuracy: 0.7466666666666667; loss: 2.1253669261932373\n",
      "Training epoch 4425 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4425 ; accuracy: 0.7466666666666667; loss: 2.125417470932007\n",
      "Training epoch 4426 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4426 ; accuracy: 0.7466666666666667; loss: 2.125467300415039\n",
      "Training epoch 4427 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4427 ; accuracy: 0.7466666666666667; loss: 2.125516653060913\n",
      "Training epoch 4428 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4428 ; accuracy: 0.7466666666666667; loss: 2.125563859939575\n",
      "Training epoch 4429 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4429 ; accuracy: 0.7466666666666667; loss: 2.1256086826324463\n",
      "Training epoch 4430 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4430 ; accuracy: 0.7466666666666667; loss: 2.1256513595581055\n",
      "Training epoch 4431 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4431 ; accuracy: 0.7466666666666667; loss: 2.125697374343872\n",
      "Training epoch 4432 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4432 ; accuracy: 0.7466666666666667; loss: 2.125744104385376\n",
      "Training epoch 4433 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4433 ; accuracy: 0.7466666666666667; loss: 2.1257903575897217\n",
      "Training epoch 4434 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4434 ; accuracy: 0.7466666666666667; loss: 2.1258442401885986\n",
      "Training epoch 4435 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4435 ; accuracy: 0.7466666666666667; loss: 2.125899314880371\n",
      "Training epoch 4436 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4436 ; accuracy: 0.7466666666666667; loss: 2.1259541511535645\n",
      "Training epoch 4437 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4437 ; accuracy: 0.7466666666666667; loss: 2.1259982585906982\n",
      "Training epoch 4438 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4438 ; accuracy: 0.7466666666666667; loss: 2.126047372817993\n",
      "Training epoch 4439 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4439 ; accuracy: 0.7466666666666667; loss: 2.126095771789551\n",
      "Training epoch 4440 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4440 ; accuracy: 0.7466666666666667; loss: 2.1261491775512695\n",
      "Training epoch 4441 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4441 ; accuracy: 0.7466666666666667; loss: 2.126204013824463\n",
      "Training epoch 4442 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4442 ; accuracy: 0.7466666666666667; loss: 2.1262588500976562\n",
      "Training epoch 4443 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4443 ; accuracy: 0.7466666666666667; loss: 2.126307964324951\n",
      "Training epoch 4444 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4444 ; accuracy: 0.7466666666666667; loss: 2.1263535022735596\n",
      "Training epoch 4445 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4445 ; accuracy: 0.7466666666666667; loss: 2.126399517059326\n",
      "Training epoch 4446 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4446 ; accuracy: 0.7466666666666667; loss: 2.1264419555664062\n",
      "Training epoch 4447 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4447 ; accuracy: 0.7466666666666667; loss: 2.1264851093292236\n",
      "Training epoch 4448 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4448 ; accuracy: 0.7466666666666667; loss: 2.1265382766723633\n",
      "Training epoch 4449 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4449 ; accuracy: 0.7466666666666667; loss: 2.126587390899658\n",
      "Training epoch 4450 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4450 ; accuracy: 0.7466666666666667; loss: 2.126636505126953\n",
      "Training epoch 4451 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4451 ; accuracy: 0.7466666666666667; loss: 2.1266820430755615\n",
      "Training epoch 4452 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4452 ; accuracy: 0.7466666666666667; loss: 2.1267249584198\n",
      "Training epoch 4453 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4453 ; accuracy: 0.7466666666666667; loss: 2.1267647743225098\n",
      "Training epoch 4454 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4454 ; accuracy: 0.7466666666666667; loss: 2.1268086433410645\n",
      "Training epoch 4455 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4455 ; accuracy: 0.7466666666666667; loss: 2.1268553733825684\n",
      "Training epoch 4456 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4456 ; accuracy: 0.7466666666666667; loss: 2.1269023418426514\n",
      "Training epoch 4457 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4457 ; accuracy: 0.7466666666666667; loss: 2.126950740814209\n",
      "Training epoch 4458 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4458 ; accuracy: 0.7466666666666667; loss: 2.1269986629486084\n",
      "Training epoch 4459 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4459 ; accuracy: 0.7466666666666667; loss: 2.1270463466644287\n",
      "Training epoch 4460 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4460 ; accuracy: 0.7466666666666667; loss: 2.127091646194458\n",
      "Training epoch 4461 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4461 ; accuracy: 0.7466666666666667; loss: 2.127138376235962\n",
      "Training epoch 4462 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4462 ; accuracy: 0.7466666666666667; loss: 2.127187967300415\n",
      "Training epoch 4463 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4463 ; accuracy: 0.7466666666666667; loss: 2.1272408962249756\n",
      "Training epoch 4464 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4464 ; accuracy: 0.7466666666666667; loss: 2.127284526824951\n",
      "Training epoch 4465 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4465 ; accuracy: 0.7466666666666667; loss: 2.1273324489593506\n",
      "Training epoch 4466 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4466 ; accuracy: 0.7466666666666667; loss: 2.1273868083953857\n",
      "Training epoch 4467 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4467 ; accuracy: 0.7466666666666667; loss: 2.1274383068084717\n",
      "Training epoch 4468 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4468 ; accuracy: 0.7466666666666667; loss: 2.127493381500244\n",
      "Training epoch 4469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4469 ; accuracy: 0.7466666666666667; loss: 2.1275439262390137\n",
      "Training epoch 4470 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4470 ; accuracy: 0.7466666666666667; loss: 2.1275906562805176\n",
      "Training epoch 4471 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4471 ; accuracy: 0.7466666666666667; loss: 2.127639055252075\n",
      "Training epoch 4472 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4472 ; accuracy: 0.7466666666666667; loss: 2.127685785293579\n",
      "Training epoch 4473 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4473 ; accuracy: 0.7466666666666667; loss: 2.1277365684509277\n",
      "Training epoch 4474 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4474 ; accuracy: 0.7466666666666667; loss: 2.1277897357940674\n",
      "Training epoch 4475 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4475 ; accuracy: 0.7466666666666667; loss: 2.1278433799743652\n",
      "Training epoch 4476 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4476 ; accuracy: 0.7466666666666667; loss: 2.127899646759033\n",
      "Training epoch 4477 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4477 ; accuracy: 0.7466666666666667; loss: 2.1279518604278564\n",
      "Training epoch 4478 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4478 ; accuracy: 0.7466666666666667; loss: 2.127995252609253\n",
      "Training epoch 4479 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4479 ; accuracy: 0.7466666666666667; loss: 2.128037452697754\n",
      "Training epoch 4480 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4480 ; accuracy: 0.7466666666666667; loss: 2.128082036972046\n",
      "Training epoch 4481 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4481 ; accuracy: 0.7466666666666667; loss: 2.1281259059906006\n",
      "Training epoch 4482 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4482 ; accuracy: 0.7466666666666667; loss: 2.1281886100769043\n",
      "Training epoch 4483 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4483 ; accuracy: 0.7466666666666667; loss: 2.128258466720581\n",
      "Training epoch 4484 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4484 ; accuracy: 0.7466666666666667; loss: 2.128324031829834\n",
      "Training epoch 4485 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4485 ; accuracy: 0.7466666666666667; loss: 2.128389835357666\n",
      "Training epoch 4486 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4486 ; accuracy: 0.7466666666666667; loss: 2.128455877304077\n",
      "Training epoch 4487 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4487 ; accuracy: 0.7466666666666667; loss: 2.128528118133545\n",
      "Training epoch 4488 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4488 ; accuracy: 0.7466666666666667; loss: 2.128599166870117\n",
      "Training epoch 4489 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4489 ; accuracy: 0.7466666666666667; loss: 2.1286683082580566\n",
      "Training epoch 4490 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4490 ; accuracy: 0.7466666666666667; loss: 2.128741979598999\n",
      "Training epoch 4491 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4491 ; accuracy: 0.7466666666666667; loss: 2.1288156509399414\n",
      "Training epoch 4492 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4492 ; accuracy: 0.7466666666666667; loss: 2.1288821697235107\n",
      "Training epoch 4493 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4493 ; accuracy: 0.7466666666666667; loss: 2.1289515495300293\n",
      "Training epoch 4494 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4494 ; accuracy: 0.7466666666666667; loss: 2.1290276050567627\n",
      "Training epoch 4495 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4495 ; accuracy: 0.7466666666666667; loss: 2.1291158199310303\n",
      "Training epoch 4496 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4496 ; accuracy: 0.7466666666666667; loss: 2.129201889038086\n",
      "Training epoch 4497 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4497 ; accuracy: 0.7466666666666667; loss: 2.12929105758667\n",
      "Training epoch 4498 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4498 ; accuracy: 0.7466666666666667; loss: 2.129377841949463\n",
      "Training epoch 4499 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4499 ; accuracy: 0.7466666666666667; loss: 2.1294517517089844\n",
      "Training epoch 4500 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4500 ; accuracy: 0.7466666666666667; loss: 2.1295247077941895\n",
      "Training epoch 4501 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4501 ; accuracy: 0.7466666666666667; loss: 2.1295907497406006\n",
      "Training epoch 4502 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4502 ; accuracy: 0.7466666666666667; loss: 2.129652261734009\n",
      "Training epoch 4503 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4503 ; accuracy: 0.7466666666666667; loss: 2.1297104358673096\n",
      "Training epoch 4504 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4504 ; accuracy: 0.7466666666666667; loss: 2.1297638416290283\n",
      "Training epoch 4505 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4505 ; accuracy: 0.7466666666666667; loss: 2.129810333251953\n",
      "Training epoch 4506 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4506 ; accuracy: 0.7466666666666667; loss: 2.1298599243164062\n",
      "Training epoch 4507 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4507 ; accuracy: 0.7466666666666667; loss: 2.1299123764038086\n",
      "Training epoch 4508 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4508 ; accuracy: 0.7466666666666667; loss: 2.1299586296081543\n",
      "Training epoch 4509 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4509 ; accuracy: 0.7466666666666667; loss: 2.130007266998291\n",
      "Training epoch 4510 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4510 ; accuracy: 0.7466666666666667; loss: 2.130049467086792\n",
      "Training epoch 4511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4511 ; accuracy: 0.7466666666666667; loss: 2.1300928592681885\n",
      "Training epoch 4512 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4512 ; accuracy: 0.7466666666666667; loss: 2.130138635635376\n",
      "Training epoch 4513 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4513 ; accuracy: 0.7466666666666667; loss: 2.1301658153533936\n",
      "Training epoch 4514 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4514 ; accuracy: 0.7466666666666667; loss: 2.130194902420044\n",
      "Training epoch 4515 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4515 ; accuracy: 0.7466666666666667; loss: 2.130216121673584\n",
      "Training epoch 4516 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4516 ; accuracy: 0.7466666666666667; loss: 2.130228281021118\n",
      "Training epoch 4517 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4517 ; accuracy: 0.7466666666666667; loss: 2.1302413940429688\n",
      "Training epoch 4518 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4518 ; accuracy: 0.7466666666666667; loss: 2.130258321762085\n",
      "Training epoch 4519 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4519 ; accuracy: 0.7466666666666667; loss: 2.130274534225464\n",
      "Training epoch 4520 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4520 ; accuracy: 0.7466666666666667; loss: 2.1302993297576904\n",
      "Training epoch 4521 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4521 ; accuracy: 0.7466666666666667; loss: 2.1303279399871826\n",
      "Training epoch 4522 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4522 ; accuracy: 0.7466666666666667; loss: 2.1303553581237793\n",
      "Training epoch 4523 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4523 ; accuracy: 0.7466666666666667; loss: 2.1303799152374268\n",
      "Training epoch 4524 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4524 ; accuracy: 0.7466666666666667; loss: 2.1304054260253906\n",
      "Training epoch 4525 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4525 ; accuracy: 0.7466666666666667; loss: 2.1304361820220947\n",
      "Training epoch 4526 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4526 ; accuracy: 0.7466666666666667; loss: 2.130481481552124\n",
      "Training epoch 4527 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4527 ; accuracy: 0.7466666666666667; loss: 2.130530595779419\n",
      "Training epoch 4528 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4528 ; accuracy: 0.7466666666666667; loss: 2.1305806636810303\n",
      "Training epoch 4529 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4529 ; accuracy: 0.7466666666666667; loss: 2.1306278705596924\n",
      "Training epoch 4530 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4530 ; accuracy: 0.7466666666666667; loss: 2.130687713623047\n",
      "Training epoch 4531 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4531 ; accuracy: 0.7466666666666667; loss: 2.130748748779297\n",
      "Training epoch 4532 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4532 ; accuracy: 0.7466666666666667; loss: 2.1308064460754395\n",
      "Training epoch 4533 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4533 ; accuracy: 0.7466666666666667; loss: 2.1308536529541016\n",
      "Training epoch 4534 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4534 ; accuracy: 0.7466666666666667; loss: 2.1308977603912354\n",
      "Training epoch 4535 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4535 ; accuracy: 0.7466666666666667; loss: 2.130944013595581\n",
      "Training epoch 4536 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4536 ; accuracy: 0.7466666666666667; loss: 2.130995988845825\n",
      "Training epoch 4537 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4537 ; accuracy: 0.7466666666666667; loss: 2.1310441493988037\n",
      "Training epoch 4538 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4538 ; accuracy: 0.7466666666666667; loss: 2.1310904026031494\n",
      "Training epoch 4539 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4539 ; accuracy: 0.7466666666666667; loss: 2.1311466693878174\n",
      "Training epoch 4540 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4540 ; accuracy: 0.7466666666666667; loss: 2.131199598312378\n",
      "Training epoch 4541 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4541 ; accuracy: 0.7466666666666667; loss: 2.1312575340270996\n",
      "Training epoch 4542 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4542 ; accuracy: 0.7466666666666667; loss: 2.1313135623931885\n",
      "Training epoch 4543 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4543 ; accuracy: 0.7466666666666667; loss: 2.1313705444335938\n",
      "Training epoch 4544 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4544 ; accuracy: 0.7466666666666667; loss: 2.1314539909362793\n",
      "Training epoch 4545 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4545 ; accuracy: 0.7466666666666667; loss: 2.131531000137329\n",
      "Training epoch 4546 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4546 ; accuracy: 0.7466666666666667; loss: 2.131605863571167\n",
      "Training epoch 4547 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4547 ; accuracy: 0.7466666666666667; loss: 2.131680727005005\n",
      "Training epoch 4548 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4548 ; accuracy: 0.7466666666666667; loss: 2.1317524909973145\n",
      "Training epoch 4549 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4549 ; accuracy: 0.7466666666666667; loss: 2.131819725036621\n",
      "Training epoch 4550 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4550 ; accuracy: 0.7466666666666667; loss: 2.1318893432617188\n",
      "Training epoch 4551 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4551 ; accuracy: 0.7466666666666667; loss: 2.1319475173950195\n",
      "Training epoch 4552 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4552 ; accuracy: 0.7466666666666667; loss: 2.132004737854004\n",
      "Training epoch 4553 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4553 ; accuracy: 0.7466666666666667; loss: 2.1320600509643555\n",
      "Training epoch 4554 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4554 ; accuracy: 0.7466666666666667; loss: 2.132113456726074\n",
      "Training epoch 4555 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4555 ; accuracy: 0.7466666666666667; loss: 2.1321606636047363\n",
      "Training epoch 4556 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4556 ; accuracy: 0.7466666666666667; loss: 2.1322081089019775\n",
      "Training epoch 4557 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4557 ; accuracy: 0.7466666666666667; loss: 2.1322574615478516\n",
      "Training epoch 4558 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4558 ; accuracy: 0.7466666666666667; loss: 2.132305383682251\n",
      "Training epoch 4559 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4559 ; accuracy: 0.7466666666666667; loss: 2.132347345352173\n",
      "Training epoch 4560 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4560 ; accuracy: 0.7466666666666667; loss: 2.1323890686035156\n",
      "Training epoch 4561 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4561 ; accuracy: 0.7466666666666667; loss: 2.1324164867401123\n",
      "Training epoch 4562 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4562 ; accuracy: 0.7466666666666667; loss: 2.1324427127838135\n",
      "Training epoch 4563 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4563 ; accuracy: 0.7466666666666667; loss: 2.1324622631073\n",
      "Training epoch 4564 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4564 ; accuracy: 0.7466666666666667; loss: 2.1324872970581055\n",
      "Training epoch 4565 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4565 ; accuracy: 0.7466666666666667; loss: 2.1325128078460693\n",
      "Training epoch 4566 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4566 ; accuracy: 0.7466666666666667; loss: 2.1325490474700928\n",
      "Training epoch 4567 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4567 ; accuracy: 0.7466666666666667; loss: 2.1325807571411133\n",
      "Training epoch 4568 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4568 ; accuracy: 0.7466666666666667; loss: 2.13261079788208\n",
      "Training epoch 4569 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4569 ; accuracy: 0.7466666666666667; loss: 2.1326441764831543\n",
      "Training epoch 4570 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4570 ; accuracy: 0.7466666666666667; loss: 2.132674217224121\n",
      "Training epoch 4571 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4571 ; accuracy: 0.7466666666666667; loss: 2.1327059268951416\n",
      "Training epoch 4572 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4572 ; accuracy: 0.7466666666666667; loss: 2.1327362060546875\n",
      "Training epoch 4573 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4573 ; accuracy: 0.7466666666666667; loss: 2.132765293121338\n",
      "Training epoch 4574 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4574 ; accuracy: 0.7466666666666667; loss: 2.1327948570251465\n",
      "Training epoch 4575 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4575 ; accuracy: 0.7466666666666667; loss: 2.1328256130218506\n",
      "Training epoch 4576 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4576 ; accuracy: 0.7466666666666667; loss: 2.132862091064453\n",
      "Training epoch 4577 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4577 ; accuracy: 0.7466666666666667; loss: 2.1328952312469482\n",
      "Training epoch 4578 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4578 ; accuracy: 0.7466666666666667; loss: 2.1329336166381836\n",
      "Training epoch 4579 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4579 ; accuracy: 0.7466666666666667; loss: 2.132967472076416\n",
      "Training epoch 4580 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4580 ; accuracy: 0.7466666666666667; loss: 2.133002281188965\n",
      "Training epoch 4581 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4581 ; accuracy: 0.7466666666666667; loss: 2.1330313682556152\n",
      "Training epoch 4582 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4582 ; accuracy: 0.7466666666666667; loss: 2.133057117462158\n",
      "Training epoch 4583 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4583 ; accuracy: 0.7466666666666667; loss: 2.1330833435058594\n",
      "Training epoch 4584 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4584 ; accuracy: 0.7466666666666667; loss: 2.1331100463867188\n",
      "Training epoch 4585 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4585 ; accuracy: 0.7466666666666667; loss: 2.133126974105835\n",
      "Training epoch 4586 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4586 ; accuracy: 0.7466666666666667; loss: 2.1331465244293213\n",
      "Training epoch 4587 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4587 ; accuracy: 0.7466666666666667; loss: 2.133164167404175\n",
      "Training epoch 4588 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4588 ; accuracy: 0.7466666666666667; loss: 2.1331825256347656\n",
      "Training epoch 4589 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4589 ; accuracy: 0.7466666666666667; loss: 2.133204460144043\n",
      "Training epoch 4590 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4590 ; accuracy: 0.7466666666666667; loss: 2.1332266330718994\n",
      "Training epoch 4591 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4591 ; accuracy: 0.7466666666666667; loss: 2.133249282836914\n",
      "Training epoch 4592 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4592 ; accuracy: 0.7466666666666667; loss: 2.133274555206299\n",
      "Training epoch 4593 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4593 ; accuracy: 0.7466666666666667; loss: 2.1333084106445312\n",
      "Training epoch 4594 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4594 ; accuracy: 0.7466666666666667; loss: 2.133341073989868\n",
      "Training epoch 4595 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4595 ; accuracy: 0.7466666666666667; loss: 2.133369207382202\n",
      "Training epoch 4596 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4596 ; accuracy: 0.7466666666666667; loss: 2.13340163230896\n",
      "Training epoch 4597 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4597 ; accuracy: 0.7466666666666667; loss: 2.1334447860717773\n",
      "Training epoch 4598 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4598 ; accuracy: 0.7466666666666667; loss: 2.13346529006958\n",
      "Training epoch 4599 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4599 ; accuracy: 0.7466666666666667; loss: 2.133488655090332\n",
      "Training epoch 4600 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4600 ; accuracy: 0.7466666666666667; loss: 2.133516550064087\n",
      "Training epoch 4601 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4601 ; accuracy: 0.7466666666666667; loss: 2.133554697036743\n",
      "Training epoch 4602 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4602 ; accuracy: 0.7466666666666667; loss: 2.1335904598236084\n",
      "Training epoch 4603 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4603 ; accuracy: 0.7466666666666667; loss: 2.1336331367492676\n",
      "Training epoch 4604 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4604 ; accuracy: 0.7466666666666667; loss: 2.1336770057678223\n",
      "Training epoch 4605 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4605 ; accuracy: 0.7466666666666667; loss: 2.133716344833374\n",
      "Training epoch 4606 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4606 ; accuracy: 0.7466666666666667; loss: 2.1337385177612305\n",
      "Training epoch 4607 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4607 ; accuracy: 0.7466666666666667; loss: 2.133760690689087\n",
      "Training epoch 4608 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4608 ; accuracy: 0.7466666666666667; loss: 2.1337897777557373\n",
      "Training epoch 4609 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4609 ; accuracy: 0.7466666666666667; loss: 2.1338112354278564\n",
      "Training epoch 4610 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4610 ; accuracy: 0.7466666666666667; loss: 2.133833169937134\n",
      "Training epoch 4611 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4611 ; accuracy: 0.7466666666666667; loss: 2.1338541507720947\n",
      "Training epoch 4612 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4612 ; accuracy: 0.7466666666666667; loss: 2.1338727474212646\n",
      "Training epoch 4613 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4613 ; accuracy: 0.7466666666666667; loss: 2.133892297744751\n",
      "Training epoch 4614 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4614 ; accuracy: 0.7466666666666667; loss: 2.133918523788452\n",
      "Training epoch 4615 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4615 ; accuracy: 0.7466666666666667; loss: 2.1339423656463623\n",
      "Training epoch 4616 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4616 ; accuracy: 0.7466666666666667; loss: 2.133974313735962\n",
      "Training epoch 4617 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4617 ; accuracy: 0.7466666666666667; loss: 2.134002208709717\n",
      "Training epoch 4618 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4618 ; accuracy: 0.7466666666666667; loss: 2.1340229511260986\n",
      "Training epoch 4619 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4619 ; accuracy: 0.7466666666666667; loss: 2.134045124053955\n",
      "Training epoch 4620 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4620 ; accuracy: 0.7466666666666667; loss: 2.1340575218200684\n",
      "Training epoch 4621 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4621 ; accuracy: 0.7466666666666667; loss: 2.134070634841919\n",
      "Training epoch 4622 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4622 ; accuracy: 0.7466666666666667; loss: 2.134085178375244\n",
      "Training epoch 4623 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4623 ; accuracy: 0.7466666666666667; loss: 2.13409686088562\n",
      "Training epoch 4624 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4624 ; accuracy: 0.7466666666666667; loss: 2.1341192722320557\n",
      "Training epoch 4625 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4625 ; accuracy: 0.7466666666666667; loss: 2.134141206741333\n",
      "Training epoch 4626 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4626 ; accuracy: 0.7466666666666667; loss: 2.1341629028320312\n",
      "Training epoch 4627 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4627 ; accuracy: 0.7466666666666667; loss: 2.134178638458252\n",
      "Training epoch 4628 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4628 ; accuracy: 0.7466666666666667; loss: 2.134190797805786\n",
      "Training epoch 4629 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4629 ; accuracy: 0.7466666666666667; loss: 2.1342012882232666\n",
      "Training epoch 4630 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4630 ; accuracy: 0.7466666666666667; loss: 2.1342101097106934\n",
      "Training epoch 4631 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4631 ; accuracy: 0.7466666666666667; loss: 2.1342220306396484\n",
      "Training epoch 4632 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4632 ; accuracy: 0.7466666666666667; loss: 2.1342289447784424\n",
      "Training epoch 4633 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4633 ; accuracy: 0.7466666666666667; loss: 2.1342415809631348\n",
      "Training epoch 4634 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4634 ; accuracy: 0.7466666666666667; loss: 2.134263515472412\n",
      "Training epoch 4635 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4635 ; accuracy: 0.7466666666666667; loss: 2.1342878341674805\n",
      "Training epoch 4636 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4636 ; accuracy: 0.7466666666666667; loss: 2.1343131065368652\n",
      "Training epoch 4637 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4637 ; accuracy: 0.7466666666666667; loss: 2.134342670440674\n",
      "Training epoch 4638 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4638 ; accuracy: 0.7466666666666667; loss: 2.1343648433685303\n",
      "Training epoch 4639 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4639 ; accuracy: 0.7466666666666667; loss: 2.1343936920166016\n",
      "Training epoch 4640 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4640 ; accuracy: 0.7466666666666667; loss: 2.134434461593628\n",
      "Training epoch 4641 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4641 ; accuracy: 0.7466666666666667; loss: 2.1344778537750244\n",
      "Training epoch 4642 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4642 ; accuracy: 0.7466666666666667; loss: 2.134511709213257\n",
      "Training epoch 4643 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4643 ; accuracy: 0.7466666666666667; loss: 2.134545087814331\n",
      "Training epoch 4644 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4644 ; accuracy: 0.7466666666666667; loss: 2.1345818042755127\n",
      "Training epoch 4645 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4645 ; accuracy: 0.7466666666666667; loss: 2.1346302032470703\n",
      "Training epoch 4646 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4646 ; accuracy: 0.7466666666666667; loss: 2.134683609008789\n",
      "Training epoch 4647 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4647 ; accuracy: 0.7466666666666667; loss: 2.134732484817505\n",
      "Training epoch 4648 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4648 ; accuracy: 0.7466666666666667; loss: 2.134782314300537\n",
      "Training epoch 4649 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4649 ; accuracy: 0.7466666666666667; loss: 2.134829044342041\n",
      "Training epoch 4650 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4650 ; accuracy: 0.7466666666666667; loss: 2.134871006011963\n",
      "Training epoch 4651 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4651 ; accuracy: 0.7466666666666667; loss: 2.1349148750305176\n",
      "Training epoch 4652 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4652 ; accuracy: 0.7466666666666667; loss: 2.134962797164917\n",
      "Training epoch 4653 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4653 ; accuracy: 0.7466666666666667; loss: 2.1350035667419434\n",
      "Training epoch 4654 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4654 ; accuracy: 0.7466666666666667; loss: 2.13504695892334\n",
      "Training epoch 4655 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4655 ; accuracy: 0.7466666666666667; loss: 2.135089874267578\n",
      "Training epoch 4656 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4656 ; accuracy: 0.7466666666666667; loss: 2.1351280212402344\n",
      "Training epoch 4657 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4657 ; accuracy: 0.7466666666666667; loss: 2.1352009773254395\n",
      "Training epoch 4658 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4658 ; accuracy: 0.7466666666666667; loss: 2.1352717876434326\n",
      "Training epoch 4659 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4659 ; accuracy: 0.7466666666666667; loss: 2.1353392601013184\n",
      "Training epoch 4660 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4660 ; accuracy: 0.7466666666666667; loss: 2.1354074478149414\n",
      "Training epoch 4661 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4661 ; accuracy: 0.7466666666666667; loss: 2.135468006134033\n",
      "Training epoch 4662 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4662 ; accuracy: 0.7466666666666667; loss: 2.1355226039886475\n",
      "Training epoch 4663 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4663 ; accuracy: 0.7466666666666667; loss: 2.13555645942688\n",
      "Training epoch 4664 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4664 ; accuracy: 0.7466666666666667; loss: 2.135605812072754\n",
      "Training epoch 4665 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4665 ; accuracy: 0.7466666666666667; loss: 2.135648488998413\n",
      "Training epoch 4666 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4666 ; accuracy: 0.7466666666666667; loss: 2.135699510574341\n",
      "Training epoch 4667 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4667 ; accuracy: 0.7466666666666667; loss: 2.135742664337158\n",
      "Training epoch 4668 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4668 ; accuracy: 0.7466666666666667; loss: 2.1357760429382324\n",
      "Training epoch 4669 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4669 ; accuracy: 0.7466666666666667; loss: 2.1357944011688232\n",
      "Training epoch 4670 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4670 ; accuracy: 0.7466666666666667; loss: 2.135817527770996\n",
      "Training epoch 4671 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4671 ; accuracy: 0.7466666666666667; loss: 2.1358492374420166\n",
      "Training epoch 4672 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4672 ; accuracy: 0.7466666666666667; loss: 2.135874032974243\n",
      "Training epoch 4673 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4673 ; accuracy: 0.7466666666666667; loss: 2.1359000205993652\n",
      "Training epoch 4674 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4674 ; accuracy: 0.7466666666666667; loss: 2.1359291076660156\n",
      "Training epoch 4675 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4675 ; accuracy: 0.7466666666666667; loss: 2.135958433151245\n",
      "Training epoch 4676 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4676 ; accuracy: 0.7466666666666667; loss: 2.1359899044036865\n",
      "Training epoch 4677 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4677 ; accuracy: 0.7466666666666667; loss: 2.136026382446289\n",
      "Training epoch 4678 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4678 ; accuracy: 0.7466666666666667; loss: 2.1360647678375244\n",
      "Training epoch 4679 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4679 ; accuracy: 0.7466666666666667; loss: 2.13610577583313\n",
      "Training epoch 4680 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4680 ; accuracy: 0.7466666666666667; loss: 2.136141300201416\n",
      "Training epoch 4681 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4681 ; accuracy: 0.7466666666666667; loss: 2.1361844539642334\n",
      "Training epoch 4682 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4682 ; accuracy: 0.7466666666666667; loss: 2.1362240314483643\n",
      "Training epoch 4683 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4683 ; accuracy: 0.7466666666666667; loss: 2.136260986328125\n",
      "Training epoch 4684 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4684 ; accuracy: 0.7466666666666667; loss: 2.1362948417663574\n",
      "Training epoch 4685 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4685 ; accuracy: 0.7466666666666667; loss: 2.1363279819488525\n",
      "Training epoch 4686 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4686 ; accuracy: 0.7466666666666667; loss: 2.1363561153411865\n",
      "Training epoch 4687 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4687 ; accuracy: 0.7466666666666667; loss: 2.136385440826416\n",
      "Training epoch 4688 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4688 ; accuracy: 0.7466666666666667; loss: 2.136411428451538\n",
      "Training epoch 4689 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4689 ; accuracy: 0.7466666666666667; loss: 2.1364378929138184\n",
      "Training epoch 4690 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4690 ; accuracy: 0.7466666666666667; loss: 2.1364619731903076\n",
      "Training epoch 4691 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4691 ; accuracy: 0.7466666666666667; loss: 2.136465549468994\n",
      "Training epoch 4692 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4692 ; accuracy: 0.7466666666666667; loss: 2.1364810466766357\n",
      "Training epoch 4693 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4693 ; accuracy: 0.7466666666666667; loss: 2.1364994049072266\n",
      "Training epoch 4694 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4694 ; accuracy: 0.7466666666666667; loss: 2.136526584625244\n",
      "Training epoch 4695 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4695 ; accuracy: 0.7466666666666667; loss: 2.1365575790405273\n",
      "Training epoch 4696 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4696 ; accuracy: 0.7466666666666667; loss: 2.1365890502929688\n",
      "Training epoch 4697 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4697 ; accuracy: 0.7466666666666667; loss: 2.1366195678710938\n",
      "Training epoch 4698 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4698 ; accuracy: 0.7466666666666667; loss: 2.1366560459136963\n",
      "Training epoch 4699 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4699 ; accuracy: 0.7466666666666667; loss: 2.1366989612579346\n",
      "Training epoch 4700 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4700 ; accuracy: 0.7466666666666667; loss: 2.1367363929748535\n",
      "Training epoch 4701 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4701 ; accuracy: 0.7466666666666667; loss: 2.1367735862731934\n",
      "Training epoch 4702 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4702 ; accuracy: 0.7466666666666667; loss: 2.136810302734375\n",
      "Training epoch 4703 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4703 ; accuracy: 0.7466666666666667; loss: 2.1368486881256104\n",
      "Training epoch 4704 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4704 ; accuracy: 0.7466666666666667; loss: 2.1368906497955322\n",
      "Training epoch 4705 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4705 ; accuracy: 0.7466666666666667; loss: 2.1369266510009766\n",
      "Training epoch 4706 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4706 ; accuracy: 0.7466666666666667; loss: 2.136960506439209\n",
      "Training epoch 4707 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4707 ; accuracy: 0.7466666666666667; loss: 2.1369926929473877\n",
      "Training epoch 4708 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4708 ; accuracy: 0.7466666666666667; loss: 2.13702392578125\n",
      "Training epoch 4709 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4709 ; accuracy: 0.7466666666666667; loss: 2.137056589126587\n",
      "Training epoch 4710 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4710 ; accuracy: 0.7466666666666667; loss: 2.1370842456817627\n",
      "Training epoch 4711 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4711 ; accuracy: 0.7466666666666667; loss: 2.137112855911255\n",
      "Training epoch 4712 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4712 ; accuracy: 0.7466666666666667; loss: 2.137148857116699\n",
      "Training epoch 4713 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4713 ; accuracy: 0.7466666666666667; loss: 2.137178897857666\n",
      "Training epoch 4714 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4714 ; accuracy: 0.7466666666666667; loss: 2.1372122764587402\n",
      "Training epoch 4715 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4715 ; accuracy: 0.7466666666666667; loss: 2.1372480392456055\n",
      "Training epoch 4716 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4716 ; accuracy: 0.7466666666666667; loss: 2.1372742652893066\n",
      "Training epoch 4717 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4717 ; accuracy: 0.7466666666666667; loss: 2.137302875518799\n",
      "Training epoch 4718 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4718 ; accuracy: 0.7466666666666667; loss: 2.1373302936553955\n",
      "Training epoch 4719 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4719 ; accuracy: 0.7466666666666667; loss: 2.137364149093628\n",
      "Training epoch 4720 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4720 ; accuracy: 0.7466666666666667; loss: 2.1373963356018066\n",
      "Training epoch 4721 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4721 ; accuracy: 0.7466666666666667; loss: 2.13743257522583\n",
      "Training epoch 4722 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4722 ; accuracy: 0.7466666666666667; loss: 2.1374671459198\n",
      "Training epoch 4723 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4723 ; accuracy: 0.7466666666666667; loss: 2.137491464614868\n",
      "Training epoch 4724 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4724 ; accuracy: 0.7466666666666667; loss: 2.137515068054199\n",
      "Training epoch 4725 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4725 ; accuracy: 0.7466666666666667; loss: 2.1375434398651123\n",
      "Training epoch 4726 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4726 ; accuracy: 0.7466666666666667; loss: 2.137573003768921\n",
      "Training epoch 4727 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4727 ; accuracy: 0.7466666666666667; loss: 2.1376044750213623\n",
      "Training epoch 4728 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4728 ; accuracy: 0.7466666666666667; loss: 2.1376380920410156\n",
      "Training epoch 4729 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4729 ; accuracy: 0.7466666666666667; loss: 2.13767147064209\n",
      "Training epoch 4730 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4730 ; accuracy: 0.7466666666666667; loss: 2.1377084255218506\n",
      "Training epoch 4731 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4731 ; accuracy: 0.7466666666666667; loss: 2.1377511024475098\n",
      "Training epoch 4732 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4732 ; accuracy: 0.7466666666666667; loss: 2.1377930641174316\n",
      "Training epoch 4733 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4733 ; accuracy: 0.7466666666666667; loss: 2.1378462314605713\n",
      "Training epoch 4734 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4734 ; accuracy: 0.7466666666666667; loss: 2.13789701461792\n",
      "Training epoch 4735 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4735 ; accuracy: 0.7466666666666667; loss: 2.1379475593566895\n",
      "Training epoch 4736 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4736 ; accuracy: 0.7466666666666667; loss: 2.137997627258301\n",
      "Training epoch 4737 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4737 ; accuracy: 0.7466666666666667; loss: 2.138028860092163\n",
      "Training epoch 4738 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4738 ; accuracy: 0.7466666666666667; loss: 2.1380667686462402\n",
      "Training epoch 4739 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4739 ; accuracy: 0.7466666666666667; loss: 2.1381115913391113\n",
      "Training epoch 4740 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4740 ; accuracy: 0.7466666666666667; loss: 2.1381521224975586\n",
      "Training epoch 4741 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4741 ; accuracy: 0.7466666666666667; loss: 2.138192653656006\n",
      "Training epoch 4742 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4742 ; accuracy: 0.7466666666666667; loss: 2.138232707977295\n",
      "Training epoch 4743 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4743 ; accuracy: 0.7466666666666667; loss: 2.138272285461426\n",
      "Training epoch 4744 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4744 ; accuracy: 0.7466666666666667; loss: 2.1383097171783447\n",
      "Training epoch 4745 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4745 ; accuracy: 0.7466666666666667; loss: 2.1383442878723145\n",
      "Training epoch 4746 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4746 ; accuracy: 0.7466666666666667; loss: 2.1383795738220215\n",
      "Training epoch 4747 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4747 ; accuracy: 0.7466666666666667; loss: 2.1384167671203613\n",
      "Training epoch 4748 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4748 ; accuracy: 0.7466666666666667; loss: 2.138453245162964\n",
      "Training epoch 4749 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4749 ; accuracy: 0.7466666666666667; loss: 2.138489246368408\n",
      "Training epoch 4750 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4750 ; accuracy: 0.7466666666666667; loss: 2.1385140419006348\n",
      "Training epoch 4751 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4751 ; accuracy: 0.7466666666666667; loss: 2.1385369300842285\n",
      "Training epoch 4752 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4752 ; accuracy: 0.7466666666666667; loss: 2.1385695934295654\n",
      "Training epoch 4753 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4753 ; accuracy: 0.7466666666666667; loss: 2.1386046409606934\n",
      "Training epoch 4754 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4754 ; accuracy: 0.7466666666666667; loss: 2.138640880584717\n",
      "Training epoch 4755 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4755 ; accuracy: 0.7466666666666667; loss: 2.1386749744415283\n",
      "Training epoch 4756 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4756 ; accuracy: 0.7466666666666667; loss: 2.138716220855713\n",
      "Training epoch 4757 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4757 ; accuracy: 0.7466666666666667; loss: 2.138758420944214\n",
      "Training epoch 4758 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4758 ; accuracy: 0.7466666666666667; loss: 2.138803482055664\n",
      "Training epoch 4759 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4759 ; accuracy: 0.7466666666666667; loss: 2.1388444900512695\n",
      "Training epoch 4760 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4760 ; accuracy: 0.7466666666666667; loss: 2.1388819217681885\n",
      "Training epoch 4761 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4761 ; accuracy: 0.7466666666666667; loss: 2.1389095783233643\n",
      "Training epoch 4762 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4762 ; accuracy: 0.7466666666666667; loss: 2.1389362812042236\n",
      "Training epoch 4763 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4763 ; accuracy: 0.7466666666666667; loss: 2.1389636993408203\n",
      "Training epoch 4764 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4764 ; accuracy: 0.7466666666666667; loss: 2.138990879058838\n",
      "Training epoch 4765 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4765 ; accuracy: 0.7466666666666667; loss: 2.1390187740325928\n",
      "Training epoch 4766 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4766 ; accuracy: 0.7466666666666667; loss: 2.1390464305877686\n",
      "Training epoch 4767 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4767 ; accuracy: 0.7466666666666667; loss: 2.139082908630371\n",
      "Training epoch 4768 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4768 ; accuracy: 0.7466666666666667; loss: 2.1391189098358154\n",
      "Training epoch 4769 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4769 ; accuracy: 0.7466666666666667; loss: 2.1391608715057373\n",
      "Training epoch 4770 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4770 ; accuracy: 0.7466666666666667; loss: 2.1392006874084473\n",
      "Training epoch 4771 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4771 ; accuracy: 0.7466666666666667; loss: 2.13923716545105\n",
      "Training epoch 4772 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4772 ; accuracy: 0.7466666666666667; loss: 2.1392698287963867\n",
      "Training epoch 4773 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4773 ; accuracy: 0.7466666666666667; loss: 2.139293909072876\n",
      "Training epoch 4774 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4774 ; accuracy: 0.7466666666666667; loss: 2.139317035675049\n",
      "Training epoch 4775 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4775 ; accuracy: 0.7466666666666667; loss: 2.1393439769744873\n",
      "Training epoch 4776 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4776 ; accuracy: 0.7466666666666667; loss: 2.1393723487854004\n",
      "Training epoch 4777 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4777 ; accuracy: 0.7466666666666667; loss: 2.1393983364105225\n",
      "Training epoch 4778 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4778 ; accuracy: 0.7466666666666667; loss: 2.139425754547119\n",
      "Training epoch 4779 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4779 ; accuracy: 0.7466666666666667; loss: 2.1394522190093994\n",
      "Training epoch 4780 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4780 ; accuracy: 0.7466666666666667; loss: 2.1394801139831543\n",
      "Training epoch 4781 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4781 ; accuracy: 0.7466666666666667; loss: 2.1395108699798584\n",
      "Training epoch 4782 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4782 ; accuracy: 0.7466666666666667; loss: 2.1395463943481445\n",
      "Training epoch 4783 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4783 ; accuracy: 0.7466666666666667; loss: 2.139582633972168\n",
      "Training epoch 4784 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 4784 ; accuracy: 0.7466666666666667; loss: 2.1397039890289307\n",
      "Training epoch 4785 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4785 ; accuracy: 0.7466666666666667; loss: 2.139805793762207\n",
      "Training epoch 4786 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4786 ; accuracy: 0.7466666666666667; loss: 2.1398911476135254\n",
      "Training epoch 4787 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4787 ; accuracy: 0.7466666666666667; loss: 2.1399741172790527\n",
      "Training epoch 4788 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4788 ; accuracy: 0.7466666666666667; loss: 2.14005446434021\n",
      "Training epoch 4789 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4789 ; accuracy: 0.7466666666666667; loss: 2.1401305198669434\n",
      "Training epoch 4790 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4790 ; accuracy: 0.7466666666666667; loss: 2.1402063369750977\n",
      "Training epoch 4791 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4791 ; accuracy: 0.7466666666666667; loss: 2.1402833461761475\n",
      "Training epoch 4792 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4792 ; accuracy: 0.7466666666666667; loss: 2.140350103378296\n",
      "Training epoch 4793 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4793 ; accuracy: 0.7466666666666667; loss: 2.140413522720337\n",
      "Training epoch 4794 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4794 ; accuracy: 0.7466666666666667; loss: 2.1404786109924316\n",
      "Training epoch 4795 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4795 ; accuracy: 0.7466666666666667; loss: 2.1405375003814697\n",
      "Training epoch 4796 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4796 ; accuracy: 0.7466666666666667; loss: 2.140551805496216\n",
      "Training epoch 4797 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4797 ; accuracy: 0.7466666666666667; loss: 2.140568733215332\n",
      "Training epoch 4798 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4798 ; accuracy: 0.7466666666666667; loss: 2.1405932903289795\n",
      "Training epoch 4799 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4799 ; accuracy: 0.7466666666666667; loss: 2.140626907348633\n",
      "Training epoch 4800 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4800 ; accuracy: 0.7466666666666667; loss: 2.140657663345337\n",
      "Training epoch 4801 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4801 ; accuracy: 0.7466666666666667; loss: 2.140688896179199\n",
      "Training epoch 4802 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4802 ; accuracy: 0.7466666666666667; loss: 2.1407227516174316\n",
      "Training epoch 4803 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4803 ; accuracy: 0.7466666666666667; loss: 2.1407580375671387\n",
      "Training epoch 4804 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4804 ; accuracy: 0.7466666666666667; loss: 2.14079213142395\n",
      "Training epoch 4805 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4805 ; accuracy: 0.7466666666666667; loss: 2.1408307552337646\n",
      "Training epoch 4806 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4806 ; accuracy: 0.7466666666666667; loss: 2.1408684253692627\n",
      "Training epoch 4807 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4807 ; accuracy: 0.7466666666666667; loss: 2.140902042388916\n",
      "Training epoch 4808 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4808 ; accuracy: 0.7466666666666667; loss: 2.1409170627593994\n",
      "Training epoch 4809 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4809 ; accuracy: 0.7466666666666667; loss: 2.140932559967041\n",
      "Training epoch 4810 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4810 ; accuracy: 0.7466666666666667; loss: 2.1409518718719482\n",
      "Training epoch 4811 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4811 ; accuracy: 0.7466666666666667; loss: 2.1409738063812256\n",
      "Training epoch 4812 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4812 ; accuracy: 0.7466666666666667; loss: 2.1409835815429688\n",
      "Training epoch 4813 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4813 ; accuracy: 0.7466666666666667; loss: 2.1409945487976074\n",
      "Training epoch 4814 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4814 ; accuracy: 0.7466666666666667; loss: 2.141038656234741\n",
      "Training epoch 4815 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4815 ; accuracy: 0.7466666666666667; loss: 2.141078233718872\n",
      "Training epoch 4816 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4816 ; accuracy: 0.7466666666666667; loss: 2.1411168575286865\n",
      "Training epoch 4817 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4817 ; accuracy: 0.7466666666666667; loss: 2.1411542892456055\n",
      "Training epoch 4818 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4818 ; accuracy: 0.7466666666666667; loss: 2.141191005706787\n",
      "Training epoch 4819 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4819 ; accuracy: 0.7466666666666667; loss: 2.1412272453308105\n",
      "Training epoch 4820 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4820 ; accuracy: 0.7466666666666667; loss: 2.1412580013275146\n",
      "Training epoch 4821 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4821 ; accuracy: 0.7466666666666667; loss: 2.141287326812744\n",
      "Training epoch 4822 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4822 ; accuracy: 0.7466666666666667; loss: 2.1413137912750244\n",
      "Training epoch 4823 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4823 ; accuracy: 0.7466666666666667; loss: 2.141339063644409\n",
      "Training epoch 4824 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4824 ; accuracy: 0.7466666666666667; loss: 2.141364812850952\n",
      "Training epoch 4825 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4825 ; accuracy: 0.7466666666666667; loss: 2.14139986038208\n",
      "Training epoch 4826 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4826 ; accuracy: 0.7466666666666667; loss: 2.1414382457733154\n",
      "Training epoch 4827 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4827 ; accuracy: 0.7466666666666667; loss: 2.141493797302246\n",
      "Training epoch 4828 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4828 ; accuracy: 0.7466666666666667; loss: 2.1415486335754395\n",
      "Training epoch 4829 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4829 ; accuracy: 0.7466666666666667; loss: 2.1416051387786865\n",
      "Training epoch 4830 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4830 ; accuracy: 0.7466666666666667; loss: 2.141662359237671\n",
      "Training epoch 4831 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4831 ; accuracy: 0.7466666666666667; loss: 2.141718864440918\n",
      "Training epoch 4832 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4832 ; accuracy: 0.7466666666666667; loss: 2.141782760620117\n",
      "Training epoch 4833 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4833 ; accuracy: 0.7466666666666667; loss: 2.1418404579162598\n",
      "Training epoch 4834 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4834 ; accuracy: 0.7466666666666667; loss: 2.1418936252593994\n",
      "Training epoch 4835 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4835 ; accuracy: 0.7466666666666667; loss: 2.1419484615325928\n",
      "Training epoch 4836 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4836 ; accuracy: 0.7466666666666667; loss: 2.1420114040374756\n",
      "Training epoch 4837 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4837 ; accuracy: 0.7466666666666667; loss: 2.142071008682251\n",
      "Training epoch 4838 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4838 ; accuracy: 0.7466666666666667; loss: 2.142124891281128\n",
      "Training epoch 4839 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4839 ; accuracy: 0.7466666666666667; loss: 2.14217209815979\n",
      "Training epoch 4840 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4840 ; accuracy: 0.7466666666666667; loss: 2.1422228813171387\n",
      "Training epoch 4841 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4841 ; accuracy: 0.7466666666666667; loss: 2.1422789096832275\n",
      "Training epoch 4842 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4842 ; accuracy: 0.7466666666666667; loss: 2.142331600189209\n",
      "Training epoch 4843 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4843 ; accuracy: 0.7466666666666667; loss: 2.1423723697662354\n",
      "Training epoch 4844 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4844 ; accuracy: 0.7466666666666667; loss: 2.1424174308776855\n",
      "Training epoch 4845 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4845 ; accuracy: 0.7466666666666667; loss: 2.142460346221924\n",
      "Training epoch 4846 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4846 ; accuracy: 0.7466666666666667; loss: 2.142508029937744\n",
      "Training epoch 4847 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4847 ; accuracy: 0.7466666666666667; loss: 2.142559289932251\n",
      "Training epoch 4848 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4848 ; accuracy: 0.7466666666666667; loss: 2.142609119415283\n",
      "Training epoch 4849 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4849 ; accuracy: 0.7466666666666667; loss: 2.1426620483398438\n",
      "Training epoch 4850 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4850 ; accuracy: 0.7466666666666667; loss: 2.1427202224731445\n",
      "Training epoch 4851 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4851 ; accuracy: 0.7466666666666667; loss: 2.142775297164917\n",
      "Training epoch 4852 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4852 ; accuracy: 0.7466666666666667; loss: 2.142825126647949\n",
      "Training epoch 4853 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4853 ; accuracy: 0.7466666666666667; loss: 2.142874002456665\n",
      "Training epoch 4854 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4854 ; accuracy: 0.7466666666666667; loss: 2.1429173946380615\n",
      "Training epoch 4855 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4855 ; accuracy: 0.7466666666666667; loss: 2.142965793609619\n",
      "Training epoch 4856 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4856 ; accuracy: 0.7466666666666667; loss: 2.143012285232544\n",
      "Training epoch 4857 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4857 ; accuracy: 0.7466666666666667; loss: 2.1430675983428955\n",
      "Training epoch 4858 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4858 ; accuracy: 0.7466666666666667; loss: 2.143127202987671\n",
      "Training epoch 4859 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4859 ; accuracy: 0.7466666666666667; loss: 2.143183708190918\n",
      "Training epoch 4860 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4860 ; accuracy: 0.7466666666666667; loss: 2.143237829208374\n",
      "Training epoch 4861 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4861 ; accuracy: 0.7466666666666667; loss: 2.143293857574463\n",
      "Training epoch 4862 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4862 ; accuracy: 0.7466666666666667; loss: 2.1433470249176025\n",
      "Training epoch 4863 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4863 ; accuracy: 0.7466666666666667; loss: 2.143397331237793\n",
      "Training epoch 4864 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4864 ; accuracy: 0.7466666666666667; loss: 2.143446683883667\n",
      "Training epoch 4865 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4865 ; accuracy: 0.7466666666666667; loss: 2.143497943878174\n",
      "Training epoch 4866 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4866 ; accuracy: 0.7466666666666667; loss: 2.143540859222412\n",
      "Training epoch 4867 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4867 ; accuracy: 0.7466666666666667; loss: 2.1435861587524414\n",
      "Training epoch 4868 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4868 ; accuracy: 0.7466666666666667; loss: 2.143630027770996\n",
      "Training epoch 4869 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4869 ; accuracy: 0.7466666666666667; loss: 2.1436760425567627\n",
      "Training epoch 4870 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4870 ; accuracy: 0.7466666666666667; loss: 2.1437325477600098\n",
      "Training epoch 4871 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4871 ; accuracy: 0.7466666666666667; loss: 2.143786907196045\n",
      "Training epoch 4872 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4872 ; accuracy: 0.7466666666666667; loss: 2.143845319747925\n",
      "Training epoch 4873 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4873 ; accuracy: 0.7466666666666667; loss: 2.14388108253479\n",
      "Training epoch 4874 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4874 ; accuracy: 0.7466666666666667; loss: 2.1439192295074463\n",
      "Training epoch 4875 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4875 ; accuracy: 0.7466666666666667; loss: 2.143967628479004\n",
      "Training epoch 4876 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4876 ; accuracy: 0.7466666666666667; loss: 2.1440162658691406\n",
      "Training epoch 4877 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4877 ; accuracy: 0.7466666666666667; loss: 2.1440625190734863\n",
      "Training epoch 4878 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4878 ; accuracy: 0.7466666666666667; loss: 2.1441450119018555\n",
      "Training epoch 4879 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4879 ; accuracy: 0.7466666666666667; loss: 2.1442253589630127\n",
      "Training epoch 4880 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4880 ; accuracy: 0.7466666666666667; loss: 2.1442983150482178\n",
      "Training epoch 4881 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4881 ; accuracy: 0.7466666666666667; loss: 2.1443686485290527\n",
      "Training epoch 4882 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4882 ; accuracy: 0.7466666666666667; loss: 2.144472360610962\n",
      "Training epoch 4883 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4883 ; accuracy: 0.7466666666666667; loss: 2.1445701122283936\n",
      "Training epoch 4884 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4884 ; accuracy: 0.7466666666666667; loss: 2.144655227661133\n",
      "Training epoch 4885 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4885 ; accuracy: 0.7466666666666667; loss: 2.144739866256714\n",
      "Training epoch 4886 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4886 ; accuracy: 0.7466666666666667; loss: 2.144817590713501\n",
      "Training epoch 4887 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4887 ; accuracy: 0.7466666666666667; loss: 2.144895553588867\n",
      "Training epoch 4888 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4888 ; accuracy: 0.7466666666666667; loss: 2.144972085952759\n",
      "Training epoch 4889 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4889 ; accuracy: 0.7466666666666667; loss: 2.1450445652008057\n",
      "Training epoch 4890 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4890 ; accuracy: 0.7466666666666667; loss: 2.1451072692871094\n",
      "Training epoch 4891 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4891 ; accuracy: 0.7466666666666667; loss: 2.1451516151428223\n",
      "Training epoch 4892 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4892 ; accuracy: 0.7466666666666667; loss: 2.145188331604004\n",
      "Training epoch 4893 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4893 ; accuracy: 0.7466666666666667; loss: 2.145225763320923\n",
      "Training epoch 4894 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4894 ; accuracy: 0.7466666666666667; loss: 2.1452696323394775\n",
      "Training epoch 4895 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4895 ; accuracy: 0.7466666666666667; loss: 2.14532208442688\n",
      "Training epoch 4896 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4896 ; accuracy: 0.7466666666666667; loss: 2.1454029083251953\n",
      "Training epoch 4897 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4897 ; accuracy: 0.7466666666666667; loss: 2.145475149154663\n",
      "Training epoch 4898 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4898 ; accuracy: 0.7466666666666667; loss: 2.1455419063568115\n",
      "Training epoch 4899 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4899 ; accuracy: 0.7466666666666667; loss: 2.145601511001587\n",
      "Training epoch 4900 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4900 ; accuracy: 0.7466666666666667; loss: 2.1456522941589355\n",
      "Training epoch 4901 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4901 ; accuracy: 0.7466666666666667; loss: 2.145698308944702\n",
      "Training epoch 4902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4902 ; accuracy: 0.7466666666666667; loss: 2.1457436084747314\n",
      "Training epoch 4903 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4903 ; accuracy: 0.7466666666666667; loss: 2.1457858085632324\n",
      "Training epoch 4904 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4904 ; accuracy: 0.7466666666666667; loss: 2.1458370685577393\n",
      "Training epoch 4905 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4905 ; accuracy: 0.7466666666666667; loss: 2.1458940505981445\n",
      "Training epoch 4906 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4906 ; accuracy: 0.7466666666666667; loss: 2.145951747894287\n",
      "Training epoch 4907 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4907 ; accuracy: 0.7466666666666667; loss: 2.1460065841674805\n",
      "Training epoch 4908 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4908 ; accuracy: 0.7466666666666667; loss: 2.146057605743408\n",
      "Training epoch 4909 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4909 ; accuracy: 0.7466666666666667; loss: 2.146117687225342\n",
      "Training epoch 4910 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4910 ; accuracy: 0.7466666666666667; loss: 2.1461739540100098\n",
      "Training epoch 4911 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4911 ; accuracy: 0.7466666666666667; loss: 2.146223545074463\n",
      "Training epoch 4912 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4912 ; accuracy: 0.7466666666666667; loss: 2.146261692047119\n",
      "Training epoch 4913 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4913 ; accuracy: 0.7466666666666667; loss: 2.1463000774383545\n",
      "Training epoch 4914 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4914 ; accuracy: 0.7466666666666667; loss: 2.1463372707366943\n",
      "Training epoch 4915 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4915 ; accuracy: 0.7466666666666667; loss: 2.146372079849243\n",
      "Training epoch 4916 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4916 ; accuracy: 0.7466666666666667; loss: 2.146411180496216\n",
      "Training epoch 4917 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4917 ; accuracy: 0.7466666666666667; loss: 2.146455764770508\n",
      "Training epoch 4918 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4918 ; accuracy: 0.7466666666666667; loss: 2.1465015411376953\n",
      "Training epoch 4919 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4919 ; accuracy: 0.7466666666666667; loss: 2.146554470062256\n",
      "Training epoch 4920 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4920 ; accuracy: 0.7466666666666667; loss: 2.146603584289551\n",
      "Training epoch 4921 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4921 ; accuracy: 0.7466666666666667; loss: 2.146656036376953\n",
      "Training epoch 4922 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4922 ; accuracy: 0.7466666666666667; loss: 2.146723508834839\n",
      "Training epoch 4923 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4923 ; accuracy: 0.7466666666666667; loss: 2.1467907428741455\n",
      "Training epoch 4924 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4924 ; accuracy: 0.7466666666666667; loss: 2.1468636989593506\n",
      "Training epoch 4925 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4925 ; accuracy: 0.7466666666666667; loss: 2.146942138671875\n",
      "Training epoch 4926 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4926 ; accuracy: 0.7466666666666667; loss: 2.147017478942871\n",
      "Training epoch 4927 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4927 ; accuracy: 0.7466666666666667; loss: 2.1470870971679688\n",
      "Training epoch 4928 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 4928 ; accuracy: 0.7466666666666667; loss: 2.1471855640411377\n",
      "Training epoch 4929 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4929 ; accuracy: 0.7466666666666667; loss: 2.147273063659668\n",
      "Training epoch 4930 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4930 ; accuracy: 0.7466666666666667; loss: 2.147348165512085\n",
      "Training epoch 4931 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4931 ; accuracy: 0.7466666666666667; loss: 2.147420644760132\n",
      "Training epoch 4932 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4932 ; accuracy: 0.7466666666666667; loss: 2.1474881172180176\n",
      "Training epoch 4933 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4933 ; accuracy: 0.7466666666666667; loss: 2.1475443840026855\n",
      "Training epoch 4934 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4934 ; accuracy: 0.7466666666666667; loss: 2.147596836090088\n",
      "Training epoch 4935 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4935 ; accuracy: 0.7466666666666667; loss: 2.147650957107544\n",
      "Training epoch 4936 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4936 ; accuracy: 0.7466666666666667; loss: 2.1477012634277344\n",
      "Training epoch 4937 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4937 ; accuracy: 0.7466666666666667; loss: 2.1477527618408203\n",
      "Training epoch 4938 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4938 ; accuracy: 0.7466666666666667; loss: 2.1478075981140137\n",
      "Training epoch 4939 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4939 ; accuracy: 0.7466666666666667; loss: 2.147876739501953\n",
      "Training epoch 4940 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4940 ; accuracy: 0.7466666666666667; loss: 2.1479380130767822\n",
      "Training epoch 4941 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4941 ; accuracy: 0.7466666666666667; loss: 2.147996664047241\n",
      "Training epoch 4942 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4942 ; accuracy: 0.7466666666666667; loss: 2.14805006980896\n",
      "Training epoch 4943 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4943 ; accuracy: 0.7466666666666667; loss: 2.1481025218963623\n",
      "Training epoch 4944 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4944 ; accuracy: 0.7466666666666667; loss: 2.148149013519287\n",
      "Training epoch 4945 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4945 ; accuracy: 0.7466666666666667; loss: 2.1481990814208984\n",
      "Training epoch 4946 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4946 ; accuracy: 0.7466666666666667; loss: 2.148245334625244\n",
      "Training epoch 4947 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4947 ; accuracy: 0.7466666666666667; loss: 2.1482911109924316\n",
      "Training epoch 4948 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4948 ; accuracy: 0.7466666666666667; loss: 2.1483328342437744\n",
      "Training epoch 4949 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4949 ; accuracy: 0.7466666666666667; loss: 2.148378849029541\n",
      "Training epoch 4950 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4950 ; accuracy: 0.7466666666666667; loss: 2.1484227180480957\n",
      "Training epoch 4951 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4951 ; accuracy: 0.7466666666666667; loss: 2.1484644412994385\n",
      "Training epoch 4952 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4952 ; accuracy: 0.7466666666666667; loss: 2.148512601852417\n",
      "Training epoch 4953 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4953 ; accuracy: 0.7466666666666667; loss: 2.1485650539398193\n",
      "Training epoch 4954 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4954 ; accuracy: 0.7466666666666667; loss: 2.148622512817383\n",
      "Training epoch 4955 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4955 ; accuracy: 0.7466666666666667; loss: 2.148676633834839\n",
      "Training epoch 4956 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 4956 ; accuracy: 0.7466666666666667; loss: 2.1487889289855957\n",
      "Training epoch 4957 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4957 ; accuracy: 0.7466666666666667; loss: 2.1489148139953613\n",
      "Training epoch 4958 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4958 ; accuracy: 0.7466666666666667; loss: 2.149031639099121\n",
      "Training epoch 4959 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4959 ; accuracy: 0.7466666666666667; loss: 2.149153709411621\n",
      "Training epoch 4960 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4960 ; accuracy: 0.7466666666666667; loss: 2.14927077293396\n",
      "Training epoch 4961 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4961 ; accuracy: 0.7466666666666667; loss: 2.1493759155273438\n",
      "Training epoch 4962 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4962 ; accuracy: 0.7466666666666667; loss: 2.149482488632202\n",
      "Training epoch 4963 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4963 ; accuracy: 0.7466666666666667; loss: 2.1495845317840576\n",
      "Training epoch 4964 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4964 ; accuracy: 0.7466666666666667; loss: 2.1496810913085938\n",
      "Training epoch 4965 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4965 ; accuracy: 0.7466666666666667; loss: 2.1497642993927\n",
      "Training epoch 4966 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4966 ; accuracy: 0.7466666666666667; loss: 2.149839162826538\n",
      "Training epoch 4967 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4967 ; accuracy: 0.7466666666666667; loss: 2.149909019470215\n",
      "Training epoch 4968 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4968 ; accuracy: 0.7466666666666667; loss: 2.149982452392578\n",
      "Training epoch 4969 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4969 ; accuracy: 0.7466666666666667; loss: 2.1500535011291504\n",
      "Training epoch 4970 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4970 ; accuracy: 0.7466666666666667; loss: 2.15012526512146\n",
      "Training epoch 4971 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4971 ; accuracy: 0.7466666666666667; loss: 2.150195360183716\n",
      "Training epoch 4972 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4972 ; accuracy: 0.7466666666666667; loss: 2.1502625942230225\n",
      "Training epoch 4973 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4973 ; accuracy: 0.7466666666666667; loss: 2.1503288745880127\n",
      "Training epoch 4974 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4974 ; accuracy: 0.7466666666666667; loss: 2.1503961086273193\n",
      "Training epoch 4975 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4975 ; accuracy: 0.7466666666666667; loss: 2.1504595279693604\n",
      "Training epoch 4976 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4976 ; accuracy: 0.7466666666666667; loss: 2.1505231857299805\n",
      "Training epoch 4977 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4977 ; accuracy: 0.7466666666666667; loss: 2.1505799293518066\n",
      "Training epoch 4978 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4978 ; accuracy: 0.7466666666666667; loss: 2.1506357192993164\n",
      "Training epoch 4979 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4979 ; accuracy: 0.7466666666666667; loss: 2.1506903171539307\n",
      "Training epoch 4980 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4980 ; accuracy: 0.7466666666666667; loss: 2.1507375240325928\n",
      "Training epoch 4981 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4981 ; accuracy: 0.7466666666666667; loss: 2.1507837772369385\n",
      "Training epoch 4982 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4982 ; accuracy: 0.7466666666666667; loss: 2.150827646255493\n",
      "Training epoch 4983 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4983 ; accuracy: 0.7466666666666667; loss: 2.150869131088257\n",
      "Training epoch 4984 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4984 ; accuracy: 0.7466666666666667; loss: 2.150908946990967\n",
      "Training epoch 4985 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4985 ; accuracy: 0.7466666666666667; loss: 2.15093994140625\n",
      "Training epoch 4986 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4986 ; accuracy: 0.7466666666666667; loss: 2.1509697437286377\n",
      "Training epoch 4987 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4987 ; accuracy: 0.7466666666666667; loss: 2.1509969234466553\n",
      "Training epoch 4988 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4988 ; accuracy: 0.7466666666666667; loss: 2.1510307788848877\n",
      "Training epoch 4989 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 4989 ; accuracy: 0.7466666666666667; loss: 2.1510961055755615\n",
      "Training epoch 4990 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4990 ; accuracy: 0.7466666666666667; loss: 2.151158571243286\n",
      "Training epoch 4991 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 4991 ; accuracy: 0.7466666666666667; loss: 2.151212453842163\n",
      "Training epoch 4992 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4992 ; accuracy: 0.7433333333333333; loss: 2.1512601375579834\n",
      "Training epoch 4993 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4993 ; accuracy: 0.7433333333333333; loss: 2.1513261795043945\n",
      "Training epoch 4994 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4994 ; accuracy: 0.7433333333333333; loss: 2.1513872146606445\n",
      "Training epoch 4995 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4995 ; accuracy: 0.7433333333333333; loss: 2.151432514190674\n",
      "Training epoch 4996 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4996 ; accuracy: 0.7433333333333333; loss: 2.1514761447906494\n",
      "Training epoch 4997 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 4997 ; accuracy: 0.7433333333333333; loss: 2.1515250205993652\n",
      "Training epoch 4998 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 4998 ; accuracy: 0.7433333333333333; loss: 2.1515676975250244\n",
      "Training epoch 4999 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 4999 ; accuracy: 0.7433333333333333; loss: 2.1516101360321045\n",
      "Training epoch 5000 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5000 ; accuracy: 0.7433333333333333; loss: 2.151654005050659\n",
      "Training epoch 5001 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5001 ; accuracy: 0.7433333333333333; loss: 2.1516923904418945\n",
      "Training epoch 5002 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5002 ; accuracy: 0.7433333333333333; loss: 2.1517345905303955\n",
      "Training epoch 5003 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5003 ; accuracy: 0.7433333333333333; loss: 2.1517813205718994\n",
      "Training epoch 5004 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5004 ; accuracy: 0.7433333333333333; loss: 2.1518378257751465\n",
      "Training epoch 5005 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5005 ; accuracy: 0.7433333333333333; loss: 2.15190052986145\n",
      "Training epoch 5006 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5006 ; accuracy: 0.7433333333333333; loss: 2.1519649028778076\n",
      "Training epoch 5007 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5007 ; accuracy: 0.7433333333333333; loss: 2.1520285606384277\n",
      "Training epoch 5008 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5008 ; accuracy: 0.7433333333333333; loss: 2.152088165283203\n",
      "Training epoch 5009 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5009 ; accuracy: 0.7433333333333333; loss: 2.1521337032318115\n",
      "Training epoch 5010 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5010 ; accuracy: 0.7433333333333333; loss: 2.152179479598999\n",
      "Training epoch 5011 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5011 ; accuracy: 0.7433333333333333; loss: 2.1522223949432373\n",
      "Training epoch 5012 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5012 ; accuracy: 0.7433333333333333; loss: 2.1522607803344727\n",
      "Training epoch 5013 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5013 ; accuracy: 0.7433333333333333; loss: 2.1522982120513916\n",
      "Training epoch 5014 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5014 ; accuracy: 0.7433333333333333; loss: 2.1523327827453613\n",
      "Training epoch 5015 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5015 ; accuracy: 0.7433333333333333; loss: 2.1523642539978027\n",
      "Training epoch 5016 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5016 ; accuracy: 0.7433333333333333; loss: 2.152400016784668\n",
      "Training epoch 5017 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5017 ; accuracy: 0.7433333333333333; loss: 2.1524341106414795\n",
      "Training epoch 5018 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5018 ; accuracy: 0.7433333333333333; loss: 2.152463674545288\n",
      "Training epoch 5019 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5019 ; accuracy: 0.7433333333333333; loss: 2.152498722076416\n",
      "Training epoch 5020 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5020 ; accuracy: 0.7433333333333333; loss: 2.1525378227233887\n",
      "Training epoch 5021 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5021 ; accuracy: 0.7433333333333333; loss: 2.152576208114624\n",
      "Training epoch 5022 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5022 ; accuracy: 0.7433333333333333; loss: 2.152613878250122\n",
      "Training epoch 5023 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5023 ; accuracy: 0.7433333333333333; loss: 2.152649164199829\n",
      "Training epoch 5024 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5024 ; accuracy: 0.7433333333333333; loss: 2.152681827545166\n",
      "Training epoch 5025 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5025 ; accuracy: 0.7433333333333333; loss: 2.152705430984497\n",
      "Training epoch 5026 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5026 ; accuracy: 0.7433333333333333; loss: 2.152733087539673\n",
      "Training epoch 5027 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5027 ; accuracy: 0.7433333333333333; loss: 2.152761936187744\n",
      "Training epoch 5028 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5028 ; accuracy: 0.7433333333333333; loss: 2.152789831161499\n",
      "Training epoch 5029 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5029 ; accuracy: 0.7433333333333333; loss: 2.1528258323669434\n",
      "Training epoch 5030 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5030 ; accuracy: 0.7433333333333333; loss: 2.1528563499450684\n",
      "Training epoch 5031 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5031 ; accuracy: 0.7433333333333333; loss: 2.1528894901275635\n",
      "Training epoch 5032 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5032 ; accuracy: 0.7433333333333333; loss: 2.1529219150543213\n",
      "Training epoch 5033 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5033 ; accuracy: 0.7433333333333333; loss: 2.1529505252838135\n",
      "Training epoch 5034 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5034 ; accuracy: 0.7433333333333333; loss: 2.152979850769043\n",
      "Training epoch 5035 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5035 ; accuracy: 0.7433333333333333; loss: 2.1530072689056396\n",
      "Training epoch 5036 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5036 ; accuracy: 0.7433333333333333; loss: 2.1530251502990723\n",
      "Training epoch 5037 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5037 ; accuracy: 0.7433333333333333; loss: 2.153052806854248\n",
      "Training epoch 5038 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5038 ; accuracy: 0.7433333333333333; loss: 2.1530704498291016\n",
      "Training epoch 5039 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5039 ; accuracy: 0.7433333333333333; loss: 2.1530957221984863\n",
      "Training epoch 5040 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5040 ; accuracy: 0.7433333333333333; loss: 2.1531288623809814\n",
      "Training epoch 5041 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5041 ; accuracy: 0.7433333333333333; loss: 2.1531503200531006\n",
      "Training epoch 5042 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5042 ; accuracy: 0.7433333333333333; loss: 2.1531717777252197\n",
      "Training epoch 5043 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5043 ; accuracy: 0.7433333333333333; loss: 2.153191566467285\n",
      "Training epoch 5044 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5044 ; accuracy: 0.7433333333333333; loss: 2.153210163116455\n",
      "Training epoch 5045 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5045 ; accuracy: 0.7433333333333333; loss: 2.153228282928467\n",
      "Training epoch 5046 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5046 ; accuracy: 0.7433333333333333; loss: 2.153257131576538\n",
      "Training epoch 5047 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5047 ; accuracy: 0.7433333333333333; loss: 2.153287649154663\n",
      "Training epoch 5048 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5048 ; accuracy: 0.7433333333333333; loss: 2.1533102989196777\n",
      "Training epoch 5049 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5049 ; accuracy: 0.7433333333333333; loss: 2.153346061706543\n",
      "Training epoch 5050 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5050 ; accuracy: 0.7433333333333333; loss: 2.1533777713775635\n",
      "Training epoch 5051 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5051 ; accuracy: 0.7433333333333333; loss: 2.153411626815796\n",
      "Training epoch 5052 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5052 ; accuracy: 0.7433333333333333; loss: 2.153446674346924\n",
      "Training epoch 5053 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5053 ; accuracy: 0.7433333333333333; loss: 2.153475284576416\n",
      "Training epoch 5054 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5054 ; accuracy: 0.7433333333333333; loss: 2.153502941131592\n",
      "Training epoch 5055 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5055 ; accuracy: 0.7433333333333333; loss: 2.1535332202911377\n",
      "Training epoch 5056 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5056 ; accuracy: 0.7433333333333333; loss: 2.1535608768463135\n",
      "Training epoch 5057 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5057 ; accuracy: 0.7433333333333333; loss: 2.15358829498291\n",
      "Training epoch 5058 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5058 ; accuracy: 0.7466666666666667; loss: 2.153608560562134\n",
      "Training epoch 5059 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5059 ; accuracy: 0.7466666666666667; loss: 2.153635263442993\n",
      "Training epoch 5060 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5060 ; accuracy: 0.7466666666666667; loss: 2.1536612510681152\n",
      "Training epoch 5061 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5061 ; accuracy: 0.7466666666666667; loss: 2.1536829471588135\n",
      "Training epoch 5062 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5062 ; accuracy: 0.7466666666666667; loss: 2.1537065505981445\n",
      "Training epoch 5063 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5063 ; accuracy: 0.7466666666666667; loss: 2.1537234783172607\n",
      "Training epoch 5064 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5064 ; accuracy: 0.7466666666666667; loss: 2.1537399291992188\n",
      "Training epoch 5065 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5065 ; accuracy: 0.7466666666666667; loss: 2.1537511348724365\n",
      "Training epoch 5066 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5066 ; accuracy: 0.7466666666666667; loss: 2.1537561416625977\n",
      "Training epoch 5067 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5067 ; accuracy: 0.7466666666666667; loss: 2.153761625289917\n",
      "Training epoch 5068 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5068 ; accuracy: 0.7466666666666667; loss: 2.15376615524292\n",
      "Training epoch 5069 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5069 ; accuracy: 0.7466666666666667; loss: 2.1537749767303467\n",
      "Training epoch 5070 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5070 ; accuracy: 0.7466666666666667; loss: 2.153780221939087\n",
      "Training epoch 5071 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5071 ; accuracy: 0.7466666666666667; loss: 2.153787136077881\n",
      "Training epoch 5072 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5072 ; accuracy: 0.7466666666666667; loss: 2.1537890434265137\n",
      "Training epoch 5073 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 5073 ; accuracy: 0.7466666666666667; loss: 2.153778314590454\n",
      "Training epoch 5074 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5074 ; accuracy: 0.7466666666666667; loss: 2.1537723541259766\n",
      "Training epoch 5075 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5075 ; accuracy: 0.7466666666666667; loss: 2.1537704467773438\n",
      "Training epoch 5076 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5076 ; accuracy: 0.7466666666666667; loss: 2.153778553009033\n",
      "Training epoch 5077 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5077 ; accuracy: 0.7466666666666667; loss: 2.1537837982177734\n",
      "Training epoch 5078 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5078 ; accuracy: 0.7466666666666667; loss: 2.1537892818450928\n",
      "Training epoch 5079 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5079 ; accuracy: 0.7466666666666667; loss: 2.1537926197052\n",
      "Training epoch 5080 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5080 ; accuracy: 0.7466666666666667; loss: 2.15380859375\n",
      "Training epoch 5081 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5081 ; accuracy: 0.7466666666666667; loss: 2.1538352966308594\n",
      "Training epoch 5082 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5082 ; accuracy: 0.7466666666666667; loss: 2.1538591384887695\n",
      "Training epoch 5083 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5083 ; accuracy: 0.7466666666666667; loss: 2.153879404067993\n",
      "Training epoch 5084 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5084 ; accuracy: 0.7466666666666667; loss: 2.153904676437378\n",
      "Training epoch 5085 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5085 ; accuracy: 0.7466666666666667; loss: 2.153934955596924\n",
      "Training epoch 5086 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5086 ; accuracy: 0.7466666666666667; loss: 2.153963565826416\n",
      "Training epoch 5087 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5087 ; accuracy: 0.7466666666666667; loss: 2.1539978981018066\n",
      "Training epoch 5088 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5088 ; accuracy: 0.7466666666666667; loss: 2.154029369354248\n",
      "Training epoch 5089 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5089 ; accuracy: 0.7466666666666667; loss: 2.154062509536743\n",
      "Training epoch 5090 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5090 ; accuracy: 0.7466666666666667; loss: 2.15409517288208\n",
      "Training epoch 5091 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5091 ; accuracy: 0.7466666666666667; loss: 2.154123544692993\n",
      "Training epoch 5092 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5092 ; accuracy: 0.7466666666666667; loss: 2.1541547775268555\n",
      "Training epoch 5093 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5093 ; accuracy: 0.7466666666666667; loss: 2.154189348220825\n",
      "Training epoch 5094 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5094 ; accuracy: 0.7466666666666667; loss: 2.1542279720306396\n",
      "Training epoch 5095 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5095 ; accuracy: 0.7466666666666667; loss: 2.1542723178863525\n",
      "Training epoch 5096 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5096 ; accuracy: 0.7466666666666667; loss: 2.1543142795562744\n",
      "Training epoch 5097 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5097 ; accuracy: 0.7466666666666667; loss: 2.154350519180298\n",
      "Training epoch 5098 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5098 ; accuracy: 0.7466666666666667; loss: 2.1543874740600586\n",
      "Training epoch 5099 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5099 ; accuracy: 0.7466666666666667; loss: 2.1544253826141357\n",
      "Training epoch 5100 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5100 ; accuracy: 0.7466666666666667; loss: 2.1544675827026367\n",
      "Training epoch 5101 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5101 ; accuracy: 0.7466666666666667; loss: 2.154500722885132\n",
      "Training epoch 5102 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5102 ; accuracy: 0.7466666666666667; loss: 2.1545321941375732\n",
      "Training epoch 5103 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5103 ; accuracy: 0.7466666666666667; loss: 2.154571533203125\n",
      "Training epoch 5104 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5104 ; accuracy: 0.7466666666666667; loss: 2.15461802482605\n",
      "Training epoch 5105 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5105 ; accuracy: 0.7466666666666667; loss: 2.154665231704712\n",
      "Training epoch 5106 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5106 ; accuracy: 0.7466666666666667; loss: 2.154709577560425\n",
      "Training epoch 5107 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5107 ; accuracy: 0.7466666666666667; loss: 2.1547484397888184\n",
      "Training epoch 5108 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5108 ; accuracy: 0.7466666666666667; loss: 2.1547837257385254\n",
      "Training epoch 5109 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5109 ; accuracy: 0.7466666666666667; loss: 2.1548123359680176\n",
      "Training epoch 5110 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5110 ; accuracy: 0.7466666666666667; loss: 2.1548495292663574\n",
      "Training epoch 5111 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5111 ; accuracy: 0.7466666666666667; loss: 2.1548871994018555\n",
      "Training epoch 5112 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5112 ; accuracy: 0.7466666666666667; loss: 2.1549274921417236\n",
      "Training epoch 5113 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5113 ; accuracy: 0.7466666666666667; loss: 2.154963970184326\n",
      "Training epoch 5114 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5114 ; accuracy: 0.7466666666666667; loss: 2.155000925064087\n",
      "Training epoch 5115 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5115 ; accuracy: 0.7466666666666667; loss: 2.155034303665161\n",
      "Training epoch 5116 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5116 ; accuracy: 0.7466666666666667; loss: 2.1550753116607666\n",
      "Training epoch 5117 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5117 ; accuracy: 0.7466666666666667; loss: 2.1551146507263184\n",
      "Training epoch 5118 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5118 ; accuracy: 0.7466666666666667; loss: 2.155158281326294\n",
      "Training epoch 5119 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5119 ; accuracy: 0.7466666666666667; loss: 2.155210256576538\n",
      "Training epoch 5120 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5120 ; accuracy: 0.7466666666666667; loss: 2.155259370803833\n",
      "Training epoch 5121 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5121 ; accuracy: 0.7466666666666667; loss: 2.155306100845337\n",
      "Training epoch 5122 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5122 ; accuracy: 0.7466666666666667; loss: 2.155348062515259\n",
      "Training epoch 5123 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5123 ; accuracy: 0.7466666666666667; loss: 2.155383825302124\n",
      "Training epoch 5124 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5124 ; accuracy: 0.7466666666666667; loss: 2.1554219722747803\n",
      "Training epoch 5125 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5125 ; accuracy: 0.7466666666666667; loss: 2.155458688735962\n",
      "Training epoch 5126 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5126 ; accuracy: 0.7466666666666667; loss: 2.15549373626709\n",
      "Training epoch 5127 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5127 ; accuracy: 0.7466666666666667; loss: 2.155522346496582\n",
      "Training epoch 5128 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5128 ; accuracy: 0.7466666666666667; loss: 2.155550956726074\n",
      "Training epoch 5129 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5129 ; accuracy: 0.7466666666666667; loss: 2.155581474304199\n",
      "Training epoch 5130 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5130 ; accuracy: 0.7466666666666667; loss: 2.155611991882324\n",
      "Training epoch 5131 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5131 ; accuracy: 0.7466666666666667; loss: 2.1556453704833984\n",
      "Training epoch 5132 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5132 ; accuracy: 0.7466666666666667; loss: 2.1556615829467773\n",
      "Training epoch 5133 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5133 ; accuracy: 0.7466666666666667; loss: 2.155684471130371\n",
      "Training epoch 5134 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5134 ; accuracy: 0.7466666666666667; loss: 2.1557087898254395\n",
      "Training epoch 5135 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5135 ; accuracy: 0.7466666666666667; loss: 2.1557343006134033\n",
      "Training epoch 5136 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5136 ; accuracy: 0.7466666666666667; loss: 2.155763864517212\n",
      "Training epoch 5137 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5137 ; accuracy: 0.7466666666666667; loss: 2.1557981967926025\n",
      "Training epoch 5138 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5138 ; accuracy: 0.7466666666666667; loss: 2.1558425426483154\n",
      "Training epoch 5139 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5139 ; accuracy: 0.7466666666666667; loss: 2.155888319015503\n",
      "Training epoch 5140 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5140 ; accuracy: 0.7466666666666667; loss: 2.155942440032959\n",
      "Training epoch 5141 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5141 ; accuracy: 0.7466666666666667; loss: 2.1559906005859375\n",
      "Training epoch 5142 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5142 ; accuracy: 0.7466666666666667; loss: 2.156031608581543\n",
      "Training epoch 5143 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5143 ; accuracy: 0.7466666666666667; loss: 2.1560630798339844\n",
      "Training epoch 5144 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5144 ; accuracy: 0.7466666666666667; loss: 2.156090021133423\n",
      "Training epoch 5145 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5145 ; accuracy: 0.7466666666666667; loss: 2.156120538711548\n",
      "Training epoch 5146 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5146 ; accuracy: 0.7466666666666667; loss: 2.156149387359619\n",
      "Training epoch 5147 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5147 ; accuracy: 0.7466666666666667; loss: 2.1561801433563232\n",
      "Training epoch 5148 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5148 ; accuracy: 0.7466666666666667; loss: 2.156208038330078\n",
      "Training epoch 5149 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5149 ; accuracy: 0.7466666666666667; loss: 2.1562299728393555\n",
      "Training epoch 5150 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5150 ; accuracy: 0.7466666666666667; loss: 2.1562564373016357\n",
      "Training epoch 5151 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5151 ; accuracy: 0.7466666666666667; loss: 2.1562767028808594\n",
      "Training epoch 5152 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5152 ; accuracy: 0.7466666666666667; loss: 2.1562979221343994\n",
      "Training epoch 5153 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5153 ; accuracy: 0.7466666666666667; loss: 2.156322479248047\n",
      "Training epoch 5154 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5154 ; accuracy: 0.7466666666666667; loss: 2.156343698501587\n",
      "Training epoch 5155 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5155 ; accuracy: 0.7466666666666667; loss: 2.156367301940918\n",
      "Training epoch 5156 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5156 ; accuracy: 0.7466666666666667; loss: 2.156390905380249\n",
      "Training epoch 5157 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5157 ; accuracy: 0.7466666666666667; loss: 2.156421422958374\n",
      "Training epoch 5158 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5158 ; accuracy: 0.7466666666666667; loss: 2.156451940536499\n",
      "Training epoch 5159 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5159 ; accuracy: 0.7466666666666667; loss: 2.1564812660217285\n",
      "Training epoch 5160 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5160 ; accuracy: 0.7466666666666667; loss: 2.156514883041382\n",
      "Training epoch 5161 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5161 ; accuracy: 0.7466666666666667; loss: 2.1565515995025635\n",
      "Training epoch 5162 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5162 ; accuracy: 0.7466666666666667; loss: 2.1565840244293213\n",
      "Training epoch 5163 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5163 ; accuracy: 0.7466666666666667; loss: 2.156614065170288\n",
      "Training epoch 5164 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5164 ; accuracy: 0.7466666666666667; loss: 2.1566414833068848\n",
      "Training epoch 5165 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5165 ; accuracy: 0.7466666666666667; loss: 2.1566765308380127\n",
      "Training epoch 5166 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5166 ; accuracy: 0.7466666666666667; loss: 2.156710386276245\n",
      "Training epoch 5167 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5167 ; accuracy: 0.7466666666666667; loss: 2.156742572784424\n",
      "Training epoch 5168 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5168 ; accuracy: 0.7466666666666667; loss: 2.1567790508270264\n",
      "Training epoch 5169 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5169 ; accuracy: 0.7466666666666667; loss: 2.1568355560302734\n",
      "Training epoch 5170 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5170 ; accuracy: 0.7466666666666667; loss: 2.156890869140625\n",
      "Training epoch 5171 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5171 ; accuracy: 0.7466666666666667; loss: 2.156944513320923\n",
      "Training epoch 5172 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5172 ; accuracy: 0.7466666666666667; loss: 2.1569926738739014\n",
      "Training epoch 5173 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5173 ; accuracy: 0.7466666666666667; loss: 2.1570398807525635\n",
      "Training epoch 5174 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5174 ; accuracy: 0.7466666666666667; loss: 2.157088279724121\n",
      "Training epoch 5175 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5175 ; accuracy: 0.7466666666666667; loss: 2.1571309566497803\n",
      "Training epoch 5176 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5176 ; accuracy: 0.7466666666666667; loss: 2.1571788787841797\n",
      "Training epoch 5177 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5177 ; accuracy: 0.7466666666666667; loss: 2.157223701477051\n",
      "Training epoch 5178 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5178 ; accuracy: 0.7466666666666667; loss: 2.1572601795196533\n",
      "Training epoch 5179 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5179 ; accuracy: 0.7466666666666667; loss: 2.157299757003784\n",
      "Training epoch 5180 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5180 ; accuracy: 0.7466666666666667; loss: 2.1573486328125\n",
      "Training epoch 5181 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5181 ; accuracy: 0.7466666666666667; loss: 2.1574158668518066\n",
      "Training epoch 5182 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5182 ; accuracy: 0.7466666666666667; loss: 2.1574783325195312\n",
      "Training epoch 5183 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5183 ; accuracy: 0.7466666666666667; loss: 2.157536745071411\n",
      "Training epoch 5184 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 5184 ; accuracy: 0.7466666666666667; loss: 2.157630681991577\n",
      "Training epoch 5185 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5185 ; accuracy: 0.7466666666666667; loss: 2.157702684402466\n",
      "Training epoch 5186 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5186 ; accuracy: 0.7466666666666667; loss: 2.1577742099761963\n",
      "Training epoch 5187 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5187 ; accuracy: 0.7466666666666667; loss: 2.1578445434570312\n",
      "Training epoch 5188 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5188 ; accuracy: 0.7466666666666667; loss: 2.157912254333496\n",
      "Training epoch 5189 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5189 ; accuracy: 0.7466666666666667; loss: 2.157986879348755\n",
      "Training epoch 5190 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5190 ; accuracy: 0.7466666666666667; loss: 2.1580564975738525\n",
      "Training epoch 5191 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5191 ; accuracy: 0.7466666666666667; loss: 2.158121109008789\n",
      "Training epoch 5192 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5192 ; accuracy: 0.7433333333333333; loss: 2.1581807136535645\n",
      "Training epoch 5193 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5193 ; accuracy: 0.7433333333333333; loss: 2.1582391262054443\n",
      "Training epoch 5194 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5194 ; accuracy: 0.7433333333333333; loss: 2.1583011150360107\n",
      "Training epoch 5195 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5195 ; accuracy: 0.7433333333333333; loss: 2.158360481262207\n",
      "Training epoch 5196 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5196 ; accuracy: 0.7433333333333333; loss: 2.1584131717681885\n",
      "Training epoch 5197 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5197 ; accuracy: 0.7433333333333333; loss: 2.1584620475769043\n",
      "Training epoch 5198 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5198 ; accuracy: 0.7433333333333333; loss: 2.158511161804199\n",
      "Training epoch 5199 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5199 ; accuracy: 0.7433333333333333; loss: 2.158557653427124\n",
      "Training epoch 5200 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5200 ; accuracy: 0.7433333333333333; loss: 2.158604145050049\n",
      "Training epoch 5201 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5201 ; accuracy: 0.7433333333333333; loss: 2.1586453914642334\n",
      "Training epoch 5202 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5202 ; accuracy: 0.7433333333333333; loss: 2.1586828231811523\n",
      "Training epoch 5203 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5203 ; accuracy: 0.7433333333333333; loss: 2.1587140560150146\n",
      "Training epoch 5204 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5204 ; accuracy: 0.7433333333333333; loss: 2.1587417125701904\n",
      "Training epoch 5205 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5205 ; accuracy: 0.7433333333333333; loss: 2.1587631702423096\n",
      "Training epoch 5206 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5206 ; accuracy: 0.7433333333333333; loss: 2.1587865352630615\n",
      "Training epoch 5207 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5207 ; accuracy: 0.7433333333333333; loss: 2.1588134765625\n",
      "Training epoch 5208 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5208 ; accuracy: 0.7433333333333333; loss: 2.1588423252105713\n",
      "Training epoch 5209 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5209 ; accuracy: 0.7433333333333333; loss: 2.1588757038116455\n",
      "Training epoch 5210 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5210 ; accuracy: 0.7433333333333333; loss: 2.1589126586914062\n",
      "Training epoch 5211 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5211 ; accuracy: 0.7433333333333333; loss: 2.1589512825012207\n",
      "Training epoch 5212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5212 ; accuracy: 0.7433333333333333; loss: 2.158986806869507\n",
      "Training epoch 5213 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5213 ; accuracy: 0.7433333333333333; loss: 2.159022569656372\n",
      "Training epoch 5214 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5214 ; accuracy: 0.7433333333333333; loss: 2.159052610397339\n",
      "Training epoch 5215 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5215 ; accuracy: 0.7433333333333333; loss: 2.159076452255249\n",
      "Training epoch 5216 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5216 ; accuracy: 0.7433333333333333; loss: 2.159095525741577\n",
      "Training epoch 5217 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5217 ; accuracy: 0.7433333333333333; loss: 2.159111738204956\n",
      "Training epoch 5218 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5218 ; accuracy: 0.7433333333333333; loss: 2.1591289043426514\n",
      "Training epoch 5219 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5219 ; accuracy: 0.7433333333333333; loss: 2.1591503620147705\n",
      "Training epoch 5220 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5220 ; accuracy: 0.7433333333333333; loss: 2.1591742038726807\n",
      "Training epoch 5221 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5221 ; accuracy: 0.7433333333333333; loss: 2.1591997146606445\n",
      "Training epoch 5222 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 5222 ; accuracy: 0.7433333333333333; loss: 2.159268856048584\n",
      "Training epoch 5223 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5223 ; accuracy: 0.7433333333333333; loss: 2.159332752227783\n",
      "Training epoch 5224 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5224 ; accuracy: 0.7433333333333333; loss: 2.1593940258026123\n",
      "Training epoch 5225 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5225 ; accuracy: 0.7433333333333333; loss: 2.1594533920288086\n",
      "Training epoch 5226 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5226 ; accuracy: 0.7433333333333333; loss: 2.1595077514648438\n",
      "Training epoch 5227 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5227 ; accuracy: 0.7433333333333333; loss: 2.1595618724823\n",
      "Training epoch 5228 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5228 ; accuracy: 0.7433333333333333; loss: 2.159611940383911\n",
      "Training epoch 5229 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5229 ; accuracy: 0.7433333333333333; loss: 2.1596620082855225\n",
      "Training epoch 5230 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5230 ; accuracy: 0.7433333333333333; loss: 2.1597061157226562\n",
      "Training epoch 5231 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5231 ; accuracy: 0.7433333333333333; loss: 2.1597399711608887\n",
      "Training epoch 5232 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5232 ; accuracy: 0.7433333333333333; loss: 2.1597790718078613\n",
      "Training epoch 5233 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5233 ; accuracy: 0.7433333333333333; loss: 2.1598191261291504\n",
      "Training epoch 5234 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5234 ; accuracy: 0.7433333333333333; loss: 2.1598551273345947\n",
      "Training epoch 5235 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5235 ; accuracy: 0.7433333333333333; loss: 2.1598923206329346\n",
      "Training epoch 5236 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5236 ; accuracy: 0.7433333333333333; loss: 2.1599278450012207\n",
      "Training epoch 5237 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5237 ; accuracy: 0.7433333333333333; loss: 2.159966230392456\n",
      "Training epoch 5238 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5238 ; accuracy: 0.7466666666666667; loss: 2.159999132156372\n",
      "Training epoch 5239 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5239 ; accuracy: 0.7466666666666667; loss: 2.1600334644317627\n",
      "Training epoch 5240 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5240 ; accuracy: 0.7466666666666667; loss: 2.1600582599639893\n",
      "Training epoch 5241 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5241 ; accuracy: 0.7466666666666667; loss: 2.1600871086120605\n",
      "Training epoch 5242 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5242 ; accuracy: 0.7466666666666667; loss: 2.160115957260132\n",
      "Training epoch 5243 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5243 ; accuracy: 0.7466666666666667; loss: 2.160139322280884\n",
      "Training epoch 5244 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5244 ; accuracy: 0.7466666666666667; loss: 2.160168409347534\n",
      "Training epoch 5245 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5245 ; accuracy: 0.7466666666666667; loss: 2.1601943969726562\n",
      "Training epoch 5246 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5246 ; accuracy: 0.7466666666666667; loss: 2.1602301597595215\n",
      "Training epoch 5247 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5247 ; accuracy: 0.7466666666666667; loss: 2.1602635383605957\n",
      "Training epoch 5248 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5248 ; accuracy: 0.7466666666666667; loss: 2.160296678543091\n",
      "Training epoch 5249 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5249 ; accuracy: 0.7466666666666667; loss: 2.1603353023529053\n",
      "Training epoch 5250 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5250 ; accuracy: 0.7466666666666667; loss: 2.160382032394409\n",
      "Training epoch 5251 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5251 ; accuracy: 0.7466666666666667; loss: 2.160426139831543\n",
      "Training epoch 5252 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5252 ; accuracy: 0.7466666666666667; loss: 2.160471200942993\n",
      "Training epoch 5253 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5253 ; accuracy: 0.7466666666666667; loss: 2.160510778427124\n",
      "Training epoch 5254 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5254 ; accuracy: 0.7466666666666667; loss: 2.1605536937713623\n",
      "Training epoch 5255 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5255 ; accuracy: 0.7466666666666667; loss: 2.160598039627075\n",
      "Training epoch 5256 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5256 ; accuracy: 0.7466666666666667; loss: 2.1606409549713135\n",
      "Training epoch 5257 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5257 ; accuracy: 0.7466666666666667; loss: 2.1606900691986084\n",
      "Training epoch 5258 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5258 ; accuracy: 0.7466666666666667; loss: 2.160733699798584\n",
      "Training epoch 5259 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5259 ; accuracy: 0.7466666666666667; loss: 2.160755157470703\n",
      "Training epoch 5260 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5260 ; accuracy: 0.7466666666666667; loss: 2.160787343978882\n",
      "Training epoch 5261 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5261 ; accuracy: 0.7466666666666667; loss: 2.1608150005340576\n",
      "Training epoch 5262 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5262 ; accuracy: 0.7466666666666667; loss: 2.1608426570892334\n",
      "Training epoch 5263 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5263 ; accuracy: 0.7466666666666667; loss: 2.1608657836914062\n",
      "Training epoch 5264 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5264 ; accuracy: 0.7466666666666667; loss: 2.1608879566192627\n",
      "Training epoch 5265 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5265 ; accuracy: 0.7466666666666667; loss: 2.16091251373291\n",
      "Training epoch 5266 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5266 ; accuracy: 0.7466666666666667; loss: 2.160938262939453\n",
      "Training epoch 5267 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 5267 ; accuracy: 0.7466666666666667; loss: 2.1610682010650635\n",
      "Training epoch 5268 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5268 ; accuracy: 0.7466666666666667; loss: 2.1611855030059814\n",
      "Training epoch 5269 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5269 ; accuracy: 0.7466666666666667; loss: 2.161288022994995\n",
      "Training epoch 5270 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5270 ; accuracy: 0.7466666666666667; loss: 2.161388874053955\n",
      "Training epoch 5271 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5271 ; accuracy: 0.7466666666666667; loss: 2.161487340927124\n",
      "Training epoch 5272 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5272 ; accuracy: 0.7433333333333333; loss: 2.1615772247314453\n",
      "Training epoch 5273 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5273 ; accuracy: 0.7433333333333333; loss: 2.161661148071289\n",
      "Training epoch 5274 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5274 ; accuracy: 0.7433333333333333; loss: 2.1617252826690674\n",
      "Training epoch 5275 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5275 ; accuracy: 0.7433333333333333; loss: 2.1617817878723145\n",
      "Training epoch 5276 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5276 ; accuracy: 0.7433333333333333; loss: 2.1618354320526123\n",
      "Training epoch 5277 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5277 ; accuracy: 0.7433333333333333; loss: 2.1618826389312744\n",
      "Training epoch 5278 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5278 ; accuracy: 0.7433333333333333; loss: 2.1619269847869873\n",
      "Training epoch 5279 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5279 ; accuracy: 0.7433333333333333; loss: 2.161965847015381\n",
      "Training epoch 5280 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5280 ; accuracy: 0.7433333333333333; loss: 2.1619906425476074\n",
      "Training epoch 5281 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5281 ; accuracy: 0.7433333333333333; loss: 2.1620213985443115\n",
      "Training epoch 5282 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5282 ; accuracy: 0.7433333333333333; loss: 2.162053346633911\n",
      "Training epoch 5283 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5283 ; accuracy: 0.7433333333333333; loss: 2.1620798110961914\n",
      "Training epoch 5284 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5284 ; accuracy: 0.7433333333333333; loss: 2.162128210067749\n",
      "Training epoch 5285 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5285 ; accuracy: 0.7433333333333333; loss: 2.162182092666626\n",
      "Training epoch 5286 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5286 ; accuracy: 0.7433333333333333; loss: 2.162229061126709\n",
      "Training epoch 5287 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5287 ; accuracy: 0.7466666666666667; loss: 2.1622745990753174\n",
      "Training epoch 5288 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5288 ; accuracy: 0.7466666666666667; loss: 2.1623172760009766\n",
      "Training epoch 5289 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5289 ; accuracy: 0.7466666666666667; loss: 2.162358283996582\n",
      "Training epoch 5290 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5290 ; accuracy: 0.7466666666666667; loss: 2.1623940467834473\n",
      "Training epoch 5291 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5291 ; accuracy: 0.7466666666666667; loss: 2.162426233291626\n",
      "Training epoch 5292 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5292 ; accuracy: 0.7466666666666667; loss: 2.162459373474121\n",
      "Training epoch 5293 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5293 ; accuracy: 0.7466666666666667; loss: 2.1624999046325684\n",
      "Training epoch 5294 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5294 ; accuracy: 0.7466666666666667; loss: 2.1625430583953857\n",
      "Training epoch 5295 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5295 ; accuracy: 0.7466666666666667; loss: 2.162585735321045\n",
      "Training epoch 5296 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5296 ; accuracy: 0.7466666666666667; loss: 2.162630319595337\n",
      "Training epoch 5297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5297 ; accuracy: 0.7466666666666667; loss: 2.162677764892578\n",
      "Training epoch 5298 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5298 ; accuracy: 0.7466666666666667; loss: 2.1627328395843506\n",
      "Training epoch 5299 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5299 ; accuracy: 0.7466666666666667; loss: 2.1627843379974365\n",
      "Training epoch 5300 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5300 ; accuracy: 0.7466666666666667; loss: 2.1628243923187256\n",
      "Training epoch 5301 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5301 ; accuracy: 0.7466666666666667; loss: 2.162853717803955\n",
      "Training epoch 5302 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5302 ; accuracy: 0.7466666666666667; loss: 2.1628830432891846\n",
      "Training epoch 5303 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5303 ; accuracy: 0.7466666666666667; loss: 2.162912368774414\n",
      "Training epoch 5304 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5304 ; accuracy: 0.7466666666666667; loss: 2.1629514694213867\n",
      "Training epoch 5305 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5305 ; accuracy: 0.7433333333333333; loss: 2.162984848022461\n",
      "Training epoch 5306 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5306 ; accuracy: 0.7433333333333333; loss: 2.1630234718322754\n",
      "Training epoch 5307 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5307 ; accuracy: 0.7433333333333333; loss: 2.1630587577819824\n",
      "Training epoch 5308 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5308 ; accuracy: 0.7433333333333333; loss: 2.1630895137786865\n",
      "Training epoch 5309 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5309 ; accuracy: 0.7433333333333333; loss: 2.1631195545196533\n",
      "Training epoch 5310 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5310 ; accuracy: 0.7433333333333333; loss: 2.163151741027832\n",
      "Training epoch 5311 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5311 ; accuracy: 0.7433333333333333; loss: 2.1631851196289062\n",
      "Training epoch 5312 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5312 ; accuracy: 0.7433333333333333; loss: 2.1632156372070312\n",
      "Training epoch 5313 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5313 ; accuracy: 0.7433333333333333; loss: 2.163250684738159\n",
      "Training epoch 5314 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5314 ; accuracy: 0.7433333333333333; loss: 2.16329026222229\n",
      "Training epoch 5315 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5315 ; accuracy: 0.7433333333333333; loss: 2.163315534591675\n",
      "Training epoch 5316 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5316 ; accuracy: 0.7433333333333333; loss: 2.163336753845215\n",
      "Training epoch 5317 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5317 ; accuracy: 0.7433333333333333; loss: 2.1633541584014893\n",
      "Training epoch 5318 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5318 ; accuracy: 0.7433333333333333; loss: 2.163374662399292\n",
      "Training epoch 5319 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5319 ; accuracy: 0.7433333333333333; loss: 2.1633918285369873\n",
      "Training epoch 5320 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 5320 ; accuracy: 0.7433333333333333; loss: 2.1634726524353027\n",
      "Training epoch 5321 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5321 ; accuracy: 0.7433333333333333; loss: 2.163551092147827\n",
      "Training epoch 5322 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5322 ; accuracy: 0.7433333333333333; loss: 2.163627862930298\n",
      "Training epoch 5323 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5323 ; accuracy: 0.7433333333333333; loss: 2.1636950969696045\n",
      "Training epoch 5324 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5324 ; accuracy: 0.7433333333333333; loss: 2.1637516021728516\n",
      "Training epoch 5325 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5325 ; accuracy: 0.7433333333333333; loss: 2.1638023853302\n",
      "Training epoch 5326 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5326 ; accuracy: 0.7433333333333333; loss: 2.163848638534546\n",
      "Training epoch 5327 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5327 ; accuracy: 0.7433333333333333; loss: 2.1638972759246826\n",
      "Training epoch 5328 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5328 ; accuracy: 0.7433333333333333; loss: 2.163954496383667\n",
      "Training epoch 5329 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5329 ; accuracy: 0.7433333333333333; loss: 2.164004325866699\n",
      "Training epoch 5330 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5330 ; accuracy: 0.7433333333333333; loss: 2.164055109024048\n",
      "Training epoch 5331 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5331 ; accuracy: 0.7433333333333333; loss: 2.1641061305999756\n",
      "Training epoch 5332 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5332 ; accuracy: 0.7433333333333333; loss: 2.164151906967163\n",
      "Training epoch 5333 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5333 ; accuracy: 0.7433333333333333; loss: 2.164193868637085\n",
      "Training epoch 5334 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5334 ; accuracy: 0.7466666666666667; loss: 2.1642463207244873\n",
      "Training epoch 5335 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5335 ; accuracy: 0.7466666666666667; loss: 2.1642942428588867\n",
      "Training epoch 5336 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5336 ; accuracy: 0.7466666666666667; loss: 2.164342164993286\n",
      "Training epoch 5337 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5337 ; accuracy: 0.7466666666666667; loss: 2.164401054382324\n",
      "Training epoch 5338 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5338 ; accuracy: 0.7466666666666667; loss: 2.164457082748413\n",
      "Training epoch 5339 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5339 ; accuracy: 0.7466666666666667; loss: 2.164518117904663\n",
      "Training epoch 5340 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5340 ; accuracy: 0.7466666666666667; loss: 2.164579153060913\n",
      "Training epoch 5341 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5341 ; accuracy: 0.7466666666666667; loss: 2.1646366119384766\n",
      "Training epoch 5342 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5342 ; accuracy: 0.7466666666666667; loss: 2.1646969318389893\n",
      "Training epoch 5343 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5343 ; accuracy: 0.7466666666666667; loss: 2.1647567749023438\n",
      "Training epoch 5344 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5344 ; accuracy: 0.7466666666666667; loss: 2.164818048477173\n",
      "Training epoch 5345 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5345 ; accuracy: 0.7466666666666667; loss: 2.1648731231689453\n",
      "Training epoch 5346 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5346 ; accuracy: 0.7466666666666667; loss: 2.1649234294891357\n",
      "Training epoch 5347 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5347 ; accuracy: 0.7466666666666667; loss: 2.1649739742279053\n",
      "Training epoch 5348 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5348 ; accuracy: 0.7466666666666667; loss: 2.165020704269409\n",
      "Training epoch 5349 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5349 ; accuracy: 0.7466666666666667; loss: 2.1650612354278564\n",
      "Training epoch 5350 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5350 ; accuracy: 0.7466666666666667; loss: 2.165102481842041\n",
      "Training epoch 5351 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5351 ; accuracy: 0.7466666666666667; loss: 2.1651458740234375\n",
      "Training epoch 5352 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5352 ; accuracy: 0.7466666666666667; loss: 2.16518235206604\n",
      "Training epoch 5353 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5353 ; accuracy: 0.7466666666666667; loss: 2.165226936340332\n",
      "Training epoch 5354 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5354 ; accuracy: 0.7466666666666667; loss: 2.1652894020080566\n",
      "Training epoch 5355 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5355 ; accuracy: 0.7466666666666667; loss: 2.1653478145599365\n",
      "Training epoch 5356 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5356 ; accuracy: 0.7466666666666667; loss: 2.165412187576294\n",
      "Training epoch 5357 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5357 ; accuracy: 0.7466666666666667; loss: 2.165472984313965\n",
      "Training epoch 5358 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5358 ; accuracy: 0.7466666666666667; loss: 2.1655378341674805\n",
      "Training epoch 5359 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5359 ; accuracy: 0.7466666666666667; loss: 2.165595769882202\n",
      "Training epoch 5360 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5360 ; accuracy: 0.7466666666666667; loss: 2.1656506061553955\n",
      "Training epoch 5361 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5361 ; accuracy: 0.7466666666666667; loss: 2.1657090187072754\n",
      "Training epoch 5362 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5362 ; accuracy: 0.7466666666666667; loss: 2.1657698154449463\n",
      "Training epoch 5363 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5363 ; accuracy: 0.7466666666666667; loss: 2.1658287048339844\n",
      "Training epoch 5364 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5364 ; accuracy: 0.7466666666666667; loss: 2.1658849716186523\n",
      "Training epoch 5365 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5365 ; accuracy: 0.7466666666666667; loss: 2.1659414768218994\n",
      "Training epoch 5366 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5366 ; accuracy: 0.7466666666666667; loss: 2.165987968444824\n",
      "Training epoch 5367 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5367 ; accuracy: 0.7466666666666667; loss: 2.1660234928131104\n",
      "Training epoch 5368 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5368 ; accuracy: 0.7466666666666667; loss: 2.1660518646240234\n",
      "Training epoch 5369 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5369 ; accuracy: 0.7466666666666667; loss: 2.166074752807617\n",
      "Training epoch 5370 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5370 ; accuracy: 0.7466666666666667; loss: 2.1661007404327393\n",
      "Training epoch 5371 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5371 ; accuracy: 0.7466666666666667; loss: 2.166130781173706\n",
      "Training epoch 5372 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5372 ; accuracy: 0.7466666666666667; loss: 2.1661441326141357\n",
      "Training epoch 5373 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5373 ; accuracy: 0.7466666666666667; loss: 2.166144371032715\n",
      "Training epoch 5374 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5374 ; accuracy: 0.7466666666666667; loss: 2.166142225265503\n",
      "Training epoch 5375 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5375 ; accuracy: 0.7466666666666667; loss: 2.1661453247070312\n",
      "Training epoch 5376 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5376 ; accuracy: 0.7466666666666667; loss: 2.1662158966064453\n",
      "Training epoch 5377 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5377 ; accuracy: 0.7466666666666667; loss: 2.1662909984588623\n",
      "Training epoch 5378 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5378 ; accuracy: 0.7466666666666667; loss: 2.1663661003112793\n",
      "Training epoch 5379 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5379 ; accuracy: 0.7466666666666667; loss: 2.1664352416992188\n",
      "Training epoch 5380 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5380 ; accuracy: 0.7466666666666667; loss: 2.1665029525756836\n",
      "Training epoch 5381 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5381 ; accuracy: 0.7466666666666667; loss: 2.166569232940674\n",
      "Training epoch 5382 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5382 ; accuracy: 0.7466666666666667; loss: 2.166635036468506\n",
      "Training epoch 5383 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5383 ; accuracy: 0.7466666666666667; loss: 2.1666922569274902\n",
      "Training epoch 5384 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5384 ; accuracy: 0.7466666666666667; loss: 2.1667497158050537\n",
      "Training epoch 5385 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5385 ; accuracy: 0.7466666666666667; loss: 2.166806221008301\n",
      "Training epoch 5386 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5386 ; accuracy: 0.7466666666666667; loss: 2.166860342025757\n",
      "Training epoch 5387 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5387 ; accuracy: 0.7466666666666667; loss: 2.1669068336486816\n",
      "Training epoch 5388 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5388 ; accuracy: 0.7466666666666667; loss: 2.166963815689087\n",
      "Training epoch 5389 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5389 ; accuracy: 0.7466666666666667; loss: 2.1670262813568115\n",
      "Training epoch 5390 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5390 ; accuracy: 0.7466666666666667; loss: 2.167080879211426\n",
      "Training epoch 5391 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5391 ; accuracy: 0.7466666666666667; loss: 2.167149066925049\n",
      "Training epoch 5392 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 5392 ; accuracy: 0.7466666666666667; loss: 2.167229652404785\n",
      "Training epoch 5393 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5393 ; accuracy: 0.7433333333333333; loss: 2.167306661605835\n",
      "Training epoch 5394 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5394 ; accuracy: 0.7433333333333333; loss: 2.167379140853882\n",
      "Training epoch 5395 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5395 ; accuracy: 0.7433333333333333; loss: 2.1674535274505615\n",
      "Training epoch 5396 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5396 ; accuracy: 0.7433333333333333; loss: 2.1675214767456055\n",
      "Training epoch 5397 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5397 ; accuracy: 0.7433333333333333; loss: 2.1675922870635986\n",
      "Training epoch 5398 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5398 ; accuracy: 0.7433333333333333; loss: 2.16766357421875\n",
      "Training epoch 5399 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5399 ; accuracy: 0.7433333333333333; loss: 2.167736530303955\n",
      "Training epoch 5400 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5400 ; accuracy: 0.7433333333333333; loss: 2.1678178310394287\n",
      "Training epoch 5401 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5401 ; accuracy: 0.7433333333333333; loss: 2.1678876876831055\n",
      "Training epoch 5402 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5402 ; accuracy: 0.7433333333333333; loss: 2.1679515838623047\n",
      "Training epoch 5403 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5403 ; accuracy: 0.7433333333333333; loss: 2.168010950088501\n",
      "Training epoch 5404 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5404 ; accuracy: 0.7433333333333333; loss: 2.1680705547332764\n",
      "Training epoch 5405 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5405 ; accuracy: 0.7433333333333333; loss: 2.168121814727783\n",
      "Training epoch 5406 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5406 ; accuracy: 0.7433333333333333; loss: 2.1681761741638184\n",
      "Training epoch 5407 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5407 ; accuracy: 0.7433333333333333; loss: 2.168231725692749\n",
      "Training epoch 5408 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5408 ; accuracy: 0.7433333333333333; loss: 2.1683013439178467\n",
      "Training epoch 5409 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5409 ; accuracy: 0.7433333333333333; loss: 2.1683578491210938\n",
      "Training epoch 5410 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5410 ; accuracy: 0.7433333333333333; loss: 2.168410301208496\n",
      "Training epoch 5411 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5411 ; accuracy: 0.7433333333333333; loss: 2.168471097946167\n",
      "Training epoch 5412 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5412 ; accuracy: 0.7433333333333333; loss: 2.168530225753784\n",
      "Training epoch 5413 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5413 ; accuracy: 0.7433333333333333; loss: 2.1685802936553955\n",
      "Training epoch 5414 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5414 ; accuracy: 0.7433333333333333; loss: 2.1686174869537354\n",
      "Training epoch 5415 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5415 ; accuracy: 0.7433333333333333; loss: 2.168651819229126\n",
      "Training epoch 5416 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5416 ; accuracy: 0.7433333333333333; loss: 2.168689489364624\n",
      "Training epoch 5417 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5417 ; accuracy: 0.7433333333333333; loss: 2.1687228679656982\n",
      "Training epoch 5418 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5418 ; accuracy: 0.7433333333333333; loss: 2.1687633991241455\n",
      "Training epoch 5419 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5419 ; accuracy: 0.7433333333333333; loss: 2.1688156127929688\n",
      "Training epoch 5420 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5420 ; accuracy: 0.7433333333333333; loss: 2.168863534927368\n",
      "Training epoch 5421 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5421 ; accuracy: 0.7433333333333333; loss: 2.1689038276672363\n",
      "Training epoch 5422 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5422 ; accuracy: 0.7433333333333333; loss: 2.1689395904541016\n",
      "Training epoch 5423 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5423 ; accuracy: 0.7433333333333333; loss: 2.168973684310913\n",
      "Training epoch 5424 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5424 ; accuracy: 0.7433333333333333; loss: 2.1690053939819336\n",
      "Training epoch 5425 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5425 ; accuracy: 0.7433333333333333; loss: 2.169032573699951\n",
      "Training epoch 5426 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5426 ; accuracy: 0.7433333333333333; loss: 2.1690587997436523\n",
      "Training epoch 5427 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5427 ; accuracy: 0.7433333333333333; loss: 2.1691014766693115\n",
      "Training epoch 5428 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5428 ; accuracy: 0.7433333333333333; loss: 2.1691346168518066\n",
      "Training epoch 5429 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5429 ; accuracy: 0.7433333333333333; loss: 2.1691691875457764\n",
      "Training epoch 5430 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5430 ; accuracy: 0.7433333333333333; loss: 2.1692044734954834\n",
      "Training epoch 5431 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5431 ; accuracy: 0.7433333333333333; loss: 2.1692357063293457\n",
      "Training epoch 5432 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5432 ; accuracy: 0.7433333333333333; loss: 2.169261932373047\n",
      "Training epoch 5433 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5433 ; accuracy: 0.7433333333333333; loss: 2.1692874431610107\n",
      "Training epoch 5434 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5434 ; accuracy: 0.7433333333333333; loss: 2.1693084239959717\n",
      "Training epoch 5435 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5435 ; accuracy: 0.7433333333333333; loss: 2.169332981109619\n",
      "Training epoch 5436 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5436 ; accuracy: 0.7433333333333333; loss: 2.169353485107422\n",
      "Training epoch 5437 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5437 ; accuracy: 0.7433333333333333; loss: 2.169370412826538\n",
      "Training epoch 5438 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5438 ; accuracy: 0.7433333333333333; loss: 2.169391393661499\n",
      "Training epoch 5439 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5439 ; accuracy: 0.7433333333333333; loss: 2.169407844543457\n",
      "Training epoch 5440 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5440 ; accuracy: 0.7433333333333333; loss: 2.169426918029785\n",
      "Training epoch 5441 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5441 ; accuracy: 0.7433333333333333; loss: 2.169447898864746\n",
      "Training epoch 5442 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5442 ; accuracy: 0.7433333333333333; loss: 2.169473886489868\n",
      "Training epoch 5443 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5443 ; accuracy: 0.7433333333333333; loss: 2.169492483139038\n",
      "Training epoch 5444 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5444 ; accuracy: 0.7433333333333333; loss: 2.16951060295105\n",
      "Training epoch 5445 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5445 ; accuracy: 0.7433333333333333; loss: 2.1695291996002197\n",
      "Training epoch 5446 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5446 ; accuracy: 0.7433333333333333; loss: 2.169544219970703\n",
      "Training epoch 5447 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5447 ; accuracy: 0.7433333333333333; loss: 2.1695642471313477\n",
      "Training epoch 5448 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5448 ; accuracy: 0.7433333333333333; loss: 2.169586181640625\n",
      "Training epoch 5449 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5449 ; accuracy: 0.7433333333333333; loss: 2.169610023498535\n",
      "Training epoch 5450 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5450 ; accuracy: 0.7433333333333333; loss: 2.16963267326355\n",
      "Training epoch 5451 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5451 ; accuracy: 0.7433333333333333; loss: 2.169645071029663\n",
      "Training epoch 5452 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5452 ; accuracy: 0.7433333333333333; loss: 2.169665813446045\n",
      "Training epoch 5453 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5453 ; accuracy: 0.7433333333333333; loss: 2.169691562652588\n",
      "Training epoch 5454 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5454 ; accuracy: 0.7433333333333333; loss: 2.169724464416504\n",
      "Training epoch 5455 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5455 ; accuracy: 0.7433333333333333; loss: 2.1697518825531006\n",
      "Training epoch 5456 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5456 ; accuracy: 0.7433333333333333; loss: 2.1697847843170166\n",
      "Training epoch 5457 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5457 ; accuracy: 0.7433333333333333; loss: 2.169818878173828\n",
      "Training epoch 5458 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5458 ; accuracy: 0.7433333333333333; loss: 2.169867515563965\n",
      "Training epoch 5459 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5459 ; accuracy: 0.7433333333333333; loss: 2.169910192489624\n",
      "Training epoch 5460 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5460 ; accuracy: 0.7433333333333333; loss: 2.1699535846710205\n",
      "Training epoch 5461 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5461 ; accuracy: 0.7433333333333333; loss: 2.17000150680542\n",
      "Training epoch 5462 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5462 ; accuracy: 0.7433333333333333; loss: 2.1700525283813477\n",
      "Training epoch 5463 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5463 ; accuracy: 0.7433333333333333; loss: 2.1701014041900635\n",
      "Training epoch 5464 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5464 ; accuracy: 0.7433333333333333; loss: 2.1701598167419434\n",
      "Training epoch 5465 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5465 ; accuracy: 0.7433333333333333; loss: 2.1702141761779785\n",
      "Training epoch 5466 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5466 ; accuracy: 0.7433333333333333; loss: 2.170271396636963\n",
      "Training epoch 5467 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5467 ; accuracy: 0.7433333333333333; loss: 2.1703221797943115\n",
      "Training epoch 5468 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5468 ; accuracy: 0.7433333333333333; loss: 2.17036509513855\n",
      "Training epoch 5469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5469 ; accuracy: 0.7433333333333333; loss: 2.1704020500183105\n",
      "Training epoch 5470 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5470 ; accuracy: 0.7433333333333333; loss: 2.170436382293701\n",
      "Training epoch 5471 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5471 ; accuracy: 0.7433333333333333; loss: 2.1704697608947754\n",
      "Training epoch 5472 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5472 ; accuracy: 0.7433333333333333; loss: 2.170502185821533\n",
      "Training epoch 5473 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5473 ; accuracy: 0.7433333333333333; loss: 2.1705329418182373\n",
      "Training epoch 5474 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5474 ; accuracy: 0.7433333333333333; loss: 2.170565605163574\n",
      "Training epoch 5475 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5475 ; accuracy: 0.7433333333333333; loss: 2.170592784881592\n",
      "Training epoch 5476 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5476 ; accuracy: 0.7433333333333333; loss: 2.1706175804138184\n",
      "Training epoch 5477 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5477 ; accuracy: 0.7433333333333333; loss: 2.1706416606903076\n",
      "Training epoch 5478 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5478 ; accuracy: 0.7433333333333333; loss: 2.1706604957580566\n",
      "Training epoch 5479 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5479 ; accuracy: 0.7433333333333333; loss: 2.170675039291382\n",
      "Training epoch 5480 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5480 ; accuracy: 0.7433333333333333; loss: 2.1707255840301514\n",
      "Training epoch 5481 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5481 ; accuracy: 0.7433333333333333; loss: 2.1707923412323\n",
      "Training epoch 5482 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5482 ; accuracy: 0.7433333333333333; loss: 2.170849323272705\n",
      "Training epoch 5483 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5483 ; accuracy: 0.7433333333333333; loss: 2.1709046363830566\n",
      "Training epoch 5484 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5484 ; accuracy: 0.7433333333333333; loss: 2.170950174331665\n",
      "Training epoch 5485 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5485 ; accuracy: 0.7433333333333333; loss: 2.1709916591644287\n",
      "Training epoch 5486 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5486 ; accuracy: 0.7433333333333333; loss: 2.171032667160034\n",
      "Training epoch 5487 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5487 ; accuracy: 0.7433333333333333; loss: 2.171077013015747\n",
      "Training epoch 5488 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5488 ; accuracy: 0.7433333333333333; loss: 2.171116828918457\n",
      "Training epoch 5489 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5489 ; accuracy: 0.7433333333333333; loss: 2.1711556911468506\n",
      "Training epoch 5490 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5490 ; accuracy: 0.7433333333333333; loss: 2.1711905002593994\n",
      "Training epoch 5491 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5491 ; accuracy: 0.7433333333333333; loss: 2.1712279319763184\n",
      "Training epoch 5492 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5492 ; accuracy: 0.7433333333333333; loss: 2.1712639331817627\n",
      "Training epoch 5493 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5493 ; accuracy: 0.7433333333333333; loss: 2.171297073364258\n",
      "Training epoch 5494 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5494 ; accuracy: 0.7433333333333333; loss: 2.1713199615478516\n",
      "Training epoch 5495 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5495 ; accuracy: 0.7433333333333333; loss: 2.1713404655456543\n",
      "Training epoch 5496 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5496 ; accuracy: 0.7433333333333333; loss: 2.17136549949646\n",
      "Training epoch 5497 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5497 ; accuracy: 0.7433333333333333; loss: 2.1713876724243164\n",
      "Training epoch 5498 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5498 ; accuracy: 0.7433333333333333; loss: 2.171419382095337\n",
      "Training epoch 5499 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5499 ; accuracy: 0.7433333333333333; loss: 2.171457052230835\n",
      "Training epoch 5500 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5500 ; accuracy: 0.7433333333333333; loss: 2.1714906692504883\n",
      "Training epoch 5501 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5501 ; accuracy: 0.7433333333333333; loss: 2.1715240478515625\n",
      "Training epoch 5502 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5502 ; accuracy: 0.7433333333333333; loss: 2.1715588569641113\n",
      "Training epoch 5503 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5503 ; accuracy: 0.7433333333333333; loss: 2.171614170074463\n",
      "Training epoch 5504 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5504 ; accuracy: 0.7433333333333333; loss: 2.171665668487549\n",
      "Training epoch 5505 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5505 ; accuracy: 0.7433333333333333; loss: 2.1717188358306885\n",
      "Training epoch 5506 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5506 ; accuracy: 0.7433333333333333; loss: 2.171767473220825\n",
      "Training epoch 5507 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5507 ; accuracy: 0.7433333333333333; loss: 2.17179799079895\n",
      "Training epoch 5508 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5508 ; accuracy: 0.7433333333333333; loss: 2.171823263168335\n",
      "Training epoch 5509 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5509 ; accuracy: 0.7433333333333333; loss: 2.1718590259552\n",
      "Training epoch 5510 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5510 ; accuracy: 0.7433333333333333; loss: 2.171888589859009\n",
      "Training epoch 5511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5511 ; accuracy: 0.7433333333333333; loss: 2.171926736831665\n",
      "Training epoch 5512 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5512 ; accuracy: 0.7466666666666667; loss: 2.1719632148742676\n",
      "Training epoch 5513 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5513 ; accuracy: 0.7466666666666667; loss: 2.172001600265503\n",
      "Training epoch 5514 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5514 ; accuracy: 0.7466666666666667; loss: 2.1720404624938965\n",
      "Training epoch 5515 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5515 ; accuracy: 0.7466666666666667; loss: 2.1720824241638184\n",
      "Training epoch 5516 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5516 ; accuracy: 0.7466666666666667; loss: 2.172121286392212\n",
      "Training epoch 5517 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5517 ; accuracy: 0.7466666666666667; loss: 2.172154426574707\n",
      "Training epoch 5518 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5518 ; accuracy: 0.7466666666666667; loss: 2.1721901893615723\n",
      "Training epoch 5519 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5519 ; accuracy: 0.7466666666666667; loss: 2.1722211837768555\n",
      "Training epoch 5520 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5520 ; accuracy: 0.7466666666666667; loss: 2.172252655029297\n",
      "Training epoch 5521 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5521 ; accuracy: 0.7466666666666667; loss: 2.1722779273986816\n",
      "Training epoch 5522 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5522 ; accuracy: 0.7466666666666667; loss: 2.172299385070801\n",
      "Training epoch 5523 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5523 ; accuracy: 0.7466666666666667; loss: 2.1723227500915527\n",
      "Training epoch 5524 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5524 ; accuracy: 0.7466666666666667; loss: 2.1723482608795166\n",
      "Training epoch 5525 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5525 ; accuracy: 0.7466666666666667; loss: 2.1723716259002686\n",
      "Training epoch 5526 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5526 ; accuracy: 0.7466666666666667; loss: 2.172386646270752\n",
      "Training epoch 5527 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5527 ; accuracy: 0.7466666666666667; loss: 2.172410488128662\n",
      "Training epoch 5528 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5528 ; accuracy: 0.7466666666666667; loss: 2.172442674636841\n",
      "Training epoch 5529 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5529 ; accuracy: 0.7466666666666667; loss: 2.172478199005127\n",
      "Training epoch 5530 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5530 ; accuracy: 0.7466666666666667; loss: 2.1725118160247803\n",
      "Training epoch 5531 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5531 ; accuracy: 0.7466666666666667; loss: 2.1725411415100098\n",
      "Training epoch 5532 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5532 ; accuracy: 0.7466666666666667; loss: 2.172572135925293\n",
      "Training epoch 5533 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5533 ; accuracy: 0.7466666666666667; loss: 2.1726133823394775\n",
      "Training epoch 5534 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5534 ; accuracy: 0.7466666666666667; loss: 2.1726534366607666\n",
      "Training epoch 5535 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5535 ; accuracy: 0.7466666666666667; loss: 2.1726887226104736\n",
      "Training epoch 5536 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5536 ; accuracy: 0.7466666666666667; loss: 2.1727218627929688\n",
      "Training epoch 5537 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5537 ; accuracy: 0.7466666666666667; loss: 2.172757387161255\n",
      "Training epoch 5538 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5538 ; accuracy: 0.7466666666666667; loss: 2.1727921962738037\n",
      "Training epoch 5539 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5539 ; accuracy: 0.7466666666666667; loss: 2.172826051712036\n",
      "Training epoch 5540 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5540 ; accuracy: 0.7466666666666667; loss: 2.1728575229644775\n",
      "Training epoch 5541 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5541 ; accuracy: 0.7466666666666667; loss: 2.17288875579834\n",
      "Training epoch 5542 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5542 ; accuracy: 0.7466666666666667; loss: 2.1729373931884766\n",
      "Training epoch 5543 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5543 ; accuracy: 0.7466666666666667; loss: 2.1729812622070312\n",
      "Training epoch 5544 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5544 ; accuracy: 0.7466666666666667; loss: 2.17301607131958\n",
      "Training epoch 5545 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5545 ; accuracy: 0.7466666666666667; loss: 2.1730496883392334\n",
      "Training epoch 5546 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5546 ; accuracy: 0.7466666666666667; loss: 2.173078775405884\n",
      "Training epoch 5547 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5547 ; accuracy: 0.7466666666666667; loss: 2.173110008239746\n",
      "Training epoch 5548 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5548 ; accuracy: 0.7466666666666667; loss: 2.1731371879577637\n",
      "Training epoch 5549 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5549 ; accuracy: 0.7466666666666667; loss: 2.1731672286987305\n",
      "Training epoch 5550 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5550 ; accuracy: 0.7466666666666667; loss: 2.1731903553009033\n",
      "Training epoch 5551 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5551 ; accuracy: 0.7466666666666667; loss: 2.1732165813446045\n",
      "Training epoch 5552 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5552 ; accuracy: 0.7466666666666667; loss: 2.1732282638549805\n",
      "Training epoch 5553 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5553 ; accuracy: 0.7466666666666667; loss: 2.17323899269104\n",
      "Training epoch 5554 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5554 ; accuracy: 0.7466666666666667; loss: 2.1732521057128906\n",
      "Training epoch 5555 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5555 ; accuracy: 0.7466666666666667; loss: 2.1732633113861084\n",
      "Training epoch 5556 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5556 ; accuracy: 0.7466666666666667; loss: 2.1732723712921143\n",
      "Training epoch 5557 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5557 ; accuracy: 0.7466666666666667; loss: 2.1732850074768066\n",
      "Training epoch 5558 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5558 ; accuracy: 0.7466666666666667; loss: 2.173302412033081\n",
      "Training epoch 5559 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5559 ; accuracy: 0.7466666666666667; loss: 2.173321008682251\n",
      "Training epoch 5560 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5560 ; accuracy: 0.7466666666666667; loss: 2.1733360290527344\n",
      "Training epoch 5561 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5561 ; accuracy: 0.7466666666666667; loss: 2.1733601093292236\n",
      "Training epoch 5562 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5562 ; accuracy: 0.7466666666666667; loss: 2.1733882427215576\n",
      "Training epoch 5563 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5563 ; accuracy: 0.7466666666666667; loss: 2.173417806625366\n",
      "Training epoch 5564 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5564 ; accuracy: 0.7466666666666667; loss: 2.17344331741333\n",
      "Training epoch 5565 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5565 ; accuracy: 0.7466666666666667; loss: 2.1734704971313477\n",
      "Training epoch 5566 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5566 ; accuracy: 0.7466666666666667; loss: 2.1735005378723145\n",
      "Training epoch 5567 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5567 ; accuracy: 0.7466666666666667; loss: 2.1735262870788574\n",
      "Training epoch 5568 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5568 ; accuracy: 0.7466666666666667; loss: 2.1735498905181885\n",
      "Training epoch 5569 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5569 ; accuracy: 0.7466666666666667; loss: 2.173572063446045\n",
      "Training epoch 5570 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5570 ; accuracy: 0.7466666666666667; loss: 2.1735942363739014\n",
      "Training epoch 5571 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5571 ; accuracy: 0.7466666666666667; loss: 2.173619031906128\n",
      "Training epoch 5572 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5572 ; accuracy: 0.7466666666666667; loss: 2.1736128330230713\n",
      "Training epoch 5573 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5573 ; accuracy: 0.7466666666666667; loss: 2.173614501953125\n",
      "Training epoch 5574 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5574 ; accuracy: 0.7466666666666667; loss: 2.1736247539520264\n",
      "Training epoch 5575 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5575 ; accuracy: 0.7466666666666667; loss: 2.1736392974853516\n",
      "Training epoch 5576 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5576 ; accuracy: 0.7466666666666667; loss: 2.173661708831787\n",
      "Training epoch 5577 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5577 ; accuracy: 0.7466666666666667; loss: 2.1736741065979004\n",
      "Training epoch 5578 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5578 ; accuracy: 0.7466666666666667; loss: 2.1736879348754883\n",
      "Training epoch 5579 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5579 ; accuracy: 0.7466666666666667; loss: 2.1736958026885986\n",
      "Training epoch 5580 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5580 ; accuracy: 0.7466666666666667; loss: 2.1737067699432373\n",
      "Training epoch 5581 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5581 ; accuracy: 0.7466666666666667; loss: 2.1737215518951416\n",
      "Training epoch 5582 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5582 ; accuracy: 0.7466666666666667; loss: 2.173743963241577\n",
      "Training epoch 5583 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5583 ; accuracy: 0.7466666666666667; loss: 2.173769474029541\n",
      "Training epoch 5584 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5584 ; accuracy: 0.7466666666666667; loss: 2.1737964153289795\n",
      "Training epoch 5585 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5585 ; accuracy: 0.7466666666666667; loss: 2.1738219261169434\n",
      "Training epoch 5586 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5586 ; accuracy: 0.7466666666666667; loss: 2.173851728439331\n",
      "Training epoch 5587 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5587 ; accuracy: 0.7466666666666667; loss: 2.1738765239715576\n",
      "Training epoch 5588 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5588 ; accuracy: 0.7466666666666667; loss: 2.173898935317993\n",
      "Training epoch 5589 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5589 ; accuracy: 0.7466666666666667; loss: 2.173921585083008\n",
      "Training epoch 5590 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5590 ; accuracy: 0.7466666666666667; loss: 2.173949956893921\n",
      "Training epoch 5591 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5591 ; accuracy: 0.7466666666666667; loss: 2.173973321914673\n",
      "Training epoch 5592 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5592 ; accuracy: 0.7466666666666667; loss: 2.173999071121216\n",
      "Training epoch 5593 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5593 ; accuracy: 0.7466666666666667; loss: 2.1740283966064453\n",
      "Training epoch 5594 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5594 ; accuracy: 0.7466666666666667; loss: 2.174060344696045\n",
      "Training epoch 5595 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5595 ; accuracy: 0.7466666666666667; loss: 2.17409348487854\n",
      "Training epoch 5596 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5596 ; accuracy: 0.7466666666666667; loss: 2.1741161346435547\n",
      "Training epoch 5597 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5597 ; accuracy: 0.7466666666666667; loss: 2.1741397380828857\n",
      "Training epoch 5598 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5598 ; accuracy: 0.7466666666666667; loss: 2.1741695404052734\n",
      "Training epoch 5599 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5599 ; accuracy: 0.7466666666666667; loss: 2.174201726913452\n",
      "Training epoch 5600 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5600 ; accuracy: 0.7466666666666667; loss: 2.1742336750030518\n",
      "Training epoch 5601 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5601 ; accuracy: 0.7466666666666667; loss: 2.1742615699768066\n",
      "Training epoch 5602 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5602 ; accuracy: 0.7466666666666667; loss: 2.174290657043457\n",
      "Training epoch 5603 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5603 ; accuracy: 0.7466666666666667; loss: 2.174318552017212\n",
      "Training epoch 5604 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5604 ; accuracy: 0.7466666666666667; loss: 2.1743557453155518\n",
      "Training epoch 5605 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5605 ; accuracy: 0.7466666666666667; loss: 2.174386501312256\n",
      "Training epoch 5606 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5606 ; accuracy: 0.7466666666666667; loss: 2.1744139194488525\n",
      "Training epoch 5607 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5607 ; accuracy: 0.7466666666666667; loss: 2.174436330795288\n",
      "Training epoch 5608 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5608 ; accuracy: 0.7466666666666667; loss: 2.174457311630249\n",
      "Training epoch 5609 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5609 ; accuracy: 0.7466666666666667; loss: 2.174478054046631\n",
      "Training epoch 5610 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5610 ; accuracy: 0.7466666666666667; loss: 2.1745028495788574\n",
      "Training epoch 5611 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5611 ; accuracy: 0.7466666666666667; loss: 2.1745264530181885\n",
      "Training epoch 5612 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5612 ; accuracy: 0.7466666666666667; loss: 2.174548625946045\n",
      "Training epoch 5613 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5613 ; accuracy: 0.7466666666666667; loss: 2.1745681762695312\n",
      "Training epoch 5614 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5614 ; accuracy: 0.7466666666666667; loss: 2.174579381942749\n",
      "Training epoch 5615 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5615 ; accuracy: 0.7466666666666667; loss: 2.174590826034546\n",
      "Training epoch 5616 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5616 ; accuracy: 0.7466666666666667; loss: 2.174593925476074\n",
      "Training epoch 5617 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5617 ; accuracy: 0.7466666666666667; loss: 2.1745986938476562\n",
      "Training epoch 5618 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5618 ; accuracy: 0.7466666666666667; loss: 2.174600124359131\n",
      "Training epoch 5619 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5619 ; accuracy: 0.7466666666666667; loss: 2.1746041774749756\n",
      "Training epoch 5620 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5620 ; accuracy: 0.7466666666666667; loss: 2.174607992172241\n",
      "Training epoch 5621 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5621 ; accuracy: 0.7466666666666667; loss: 2.1746063232421875\n",
      "Training epoch 5622 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5622 ; accuracy: 0.7466666666666667; loss: 2.1745967864990234\n",
      "Training epoch 5623 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5623 ; accuracy: 0.7466666666666667; loss: 2.174593448638916\n",
      "Training epoch 5624 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5624 ; accuracy: 0.7466666666666667; loss: 2.1746060848236084\n",
      "Training epoch 5625 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5625 ; accuracy: 0.7466666666666667; loss: 2.1746299266815186\n",
      "Training epoch 5626 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5626 ; accuracy: 0.7466666666666667; loss: 2.1746528148651123\n",
      "Training epoch 5627 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5627 ; accuracy: 0.7466666666666667; loss: 2.1746771335601807\n",
      "Training epoch 5628 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5628 ; accuracy: 0.7466666666666667; loss: 2.1746985912323\n",
      "Training epoch 5629 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5629 ; accuracy: 0.7466666666666667; loss: 2.1747188568115234\n",
      "Training epoch 5630 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5630 ; accuracy: 0.7466666666666667; loss: 2.174746513366699\n",
      "Training epoch 5631 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5631 ; accuracy: 0.7466666666666667; loss: 2.1747689247131348\n",
      "Training epoch 5632 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5632 ; accuracy: 0.7466666666666667; loss: 2.174793243408203\n",
      "Training epoch 5633 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5633 ; accuracy: 0.7466666666666667; loss: 2.1748268604278564\n",
      "Training epoch 5634 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 5634 ; accuracy: 0.7466666666666667; loss: 2.1750097274780273\n",
      "Training epoch 5635 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5635 ; accuracy: 0.7466666666666667; loss: 2.1751790046691895\n",
      "Training epoch 5636 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5636 ; accuracy: 0.7466666666666667; loss: 2.175358295440674\n",
      "Training epoch 5637 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5637 ; accuracy: 0.7466666666666667; loss: 2.1755211353302\n",
      "Training epoch 5638 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5638 ; accuracy: 0.7466666666666667; loss: 2.175661087036133\n",
      "Training epoch 5639 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5639 ; accuracy: 0.7466666666666667; loss: 2.175784111022949\n",
      "Training epoch 5640 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5640 ; accuracy: 0.7433333333333333; loss: 2.175907850265503\n",
      "Training epoch 5641 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5641 ; accuracy: 0.7433333333333333; loss: 2.1760220527648926\n",
      "Training epoch 5642 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5642 ; accuracy: 0.7433333333333333; loss: 2.1761279106140137\n",
      "Training epoch 5643 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5643 ; accuracy: 0.7433333333333333; loss: 2.1762256622314453\n",
      "Training epoch 5644 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5644 ; accuracy: 0.7433333333333333; loss: 2.176316022872925\n",
      "Training epoch 5645 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5645 ; accuracy: 0.7433333333333333; loss: 2.1764047145843506\n",
      "Training epoch 5646 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5646 ; accuracy: 0.7433333333333333; loss: 2.1764883995056152\n",
      "Training epoch 5647 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5647 ; accuracy: 0.7433333333333333; loss: 2.1765711307525635\n",
      "Training epoch 5648 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5648 ; accuracy: 0.7433333333333333; loss: 2.176647424697876\n",
      "Training epoch 5649 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5649 ; accuracy: 0.7433333333333333; loss: 2.1767284870147705\n",
      "Training epoch 5650 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5650 ; accuracy: 0.7433333333333333; loss: 2.176811933517456\n",
      "Training epoch 5651 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5651 ; accuracy: 0.7433333333333333; loss: 2.1769185066223145\n",
      "Training epoch 5652 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5652 ; accuracy: 0.7433333333333333; loss: 2.1770215034484863\n",
      "Training epoch 5653 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5653 ; accuracy: 0.7433333333333333; loss: 2.1771206855773926\n",
      "Training epoch 5654 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5654 ; accuracy: 0.7433333333333333; loss: 2.1772162914276123\n",
      "Training epoch 5655 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5655 ; accuracy: 0.7433333333333333; loss: 2.177304983139038\n",
      "Training epoch 5656 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5656 ; accuracy: 0.7433333333333333; loss: 2.1773877143859863\n",
      "Training epoch 5657 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5657 ; accuracy: 0.7433333333333333; loss: 2.1774673461914062\n",
      "Training epoch 5658 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5658 ; accuracy: 0.7433333333333333; loss: 2.177542209625244\n",
      "Training epoch 5659 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5659 ; accuracy: 0.7433333333333333; loss: 2.1776113510131836\n",
      "Training epoch 5660 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5660 ; accuracy: 0.7433333333333333; loss: 2.1776726245880127\n",
      "Training epoch 5661 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5661 ; accuracy: 0.7433333333333333; loss: 2.1777267456054688\n",
      "Training epoch 5662 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5662 ; accuracy: 0.7433333333333333; loss: 2.1777796745300293\n",
      "Training epoch 5663 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5663 ; accuracy: 0.7433333333333333; loss: 2.1778295040130615\n",
      "Training epoch 5664 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5664 ; accuracy: 0.7433333333333333; loss: 2.177870512008667\n",
      "Training epoch 5665 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5665 ; accuracy: 0.7433333333333333; loss: 2.1779088973999023\n",
      "Training epoch 5666 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5666 ; accuracy: 0.7433333333333333; loss: 2.177943229675293\n",
      "Training epoch 5667 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5667 ; accuracy: 0.7433333333333333; loss: 2.1779754161834717\n",
      "Training epoch 5668 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5668 ; accuracy: 0.7433333333333333; loss: 2.1780030727386475\n",
      "Training epoch 5669 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5669 ; accuracy: 0.7433333333333333; loss: 2.1780316829681396\n",
      "Training epoch 5670 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5670 ; accuracy: 0.7433333333333333; loss: 2.1780667304992676\n",
      "Training epoch 5671 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5671 ; accuracy: 0.7433333333333333; loss: 2.1780993938446045\n",
      "Training epoch 5672 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5672 ; accuracy: 0.7433333333333333; loss: 2.1781342029571533\n",
      "Training epoch 5673 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5673 ; accuracy: 0.7433333333333333; loss: 2.178170919418335\n",
      "Training epoch 5674 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5674 ; accuracy: 0.7433333333333333; loss: 2.1782121658325195\n",
      "Training epoch 5675 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5675 ; accuracy: 0.7433333333333333; loss: 2.178255081176758\n",
      "Training epoch 5676 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5676 ; accuracy: 0.7433333333333333; loss: 2.1782987117767334\n",
      "Training epoch 5677 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5677 ; accuracy: 0.7433333333333333; loss: 2.178345203399658\n",
      "Training epoch 5678 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5678 ; accuracy: 0.7433333333333333; loss: 2.1783907413482666\n",
      "Training epoch 5679 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5679 ; accuracy: 0.7433333333333333; loss: 2.1784355640411377\n",
      "Training epoch 5680 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5680 ; accuracy: 0.7433333333333333; loss: 2.17848539352417\n",
      "Training epoch 5681 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5681 ; accuracy: 0.7433333333333333; loss: 2.1785335540771484\n",
      "Training epoch 5682 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5682 ; accuracy: 0.7433333333333333; loss: 2.178576946258545\n",
      "Training epoch 5683 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5683 ; accuracy: 0.7433333333333333; loss: 2.1786139011383057\n",
      "Training epoch 5684 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5684 ; accuracy: 0.7433333333333333; loss: 2.1786417961120605\n",
      "Training epoch 5685 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5685 ; accuracy: 0.7433333333333333; loss: 2.1786699295043945\n",
      "Training epoch 5686 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5686 ; accuracy: 0.7433333333333333; loss: 2.178697109222412\n",
      "Training epoch 5687 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5687 ; accuracy: 0.7433333333333333; loss: 2.178720712661743\n",
      "Training epoch 5688 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5688 ; accuracy: 0.7433333333333333; loss: 2.1787397861480713\n",
      "Training epoch 5689 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5689 ; accuracy: 0.7433333333333333; loss: 2.1787588596343994\n",
      "Training epoch 5690 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5690 ; accuracy: 0.7433333333333333; loss: 2.178781032562256\n",
      "Training epoch 5691 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5691 ; accuracy: 0.7433333333333333; loss: 2.178807258605957\n",
      "Training epoch 5692 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5692 ; accuracy: 0.7433333333333333; loss: 2.1788413524627686\n",
      "Training epoch 5693 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5693 ; accuracy: 0.7433333333333333; loss: 2.1788744926452637\n",
      "Training epoch 5694 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5694 ; accuracy: 0.7433333333333333; loss: 2.1789071559906006\n",
      "Training epoch 5695 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5695 ; accuracy: 0.7433333333333333; loss: 2.178926944732666\n",
      "Training epoch 5696 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5696 ; accuracy: 0.7433333333333333; loss: 2.1789391040802\n",
      "Training epoch 5697 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5697 ; accuracy: 0.7433333333333333; loss: 2.178924322128296\n",
      "Training epoch 5698 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5698 ; accuracy: 0.7433333333333333; loss: 2.178912401199341\n",
      "Training epoch 5699 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5699 ; accuracy: 0.7433333333333333; loss: 2.1789097785949707\n",
      "Training epoch 5700 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5700 ; accuracy: 0.7433333333333333; loss: 2.1789073944091797\n",
      "Training epoch 5701 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5701 ; accuracy: 0.7433333333333333; loss: 2.17891526222229\n",
      "Training epoch 5702 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5702 ; accuracy: 0.7433333333333333; loss: 2.178925037384033\n",
      "Training epoch 5703 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5703 ; accuracy: 0.7433333333333333; loss: 2.178934335708618\n",
      "Training epoch 5704 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5704 ; accuracy: 0.7433333333333333; loss: 2.1789591312408447\n",
      "Training epoch 5705 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5705 ; accuracy: 0.7433333333333333; loss: 2.178987979888916\n",
      "Training epoch 5706 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5706 ; accuracy: 0.7433333333333333; loss: 2.1790220737457275\n",
      "Training epoch 5707 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5707 ; accuracy: 0.7433333333333333; loss: 2.1790502071380615\n",
      "Training epoch 5708 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5708 ; accuracy: 0.7433333333333333; loss: 2.1790692806243896\n",
      "Training epoch 5709 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5709 ; accuracy: 0.7433333333333333; loss: 2.1790833473205566\n",
      "Training epoch 5710 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5710 ; accuracy: 0.7433333333333333; loss: 2.179097890853882\n",
      "Training epoch 5711 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5711 ; accuracy: 0.7433333333333333; loss: 2.1791152954101562\n",
      "Training epoch 5712 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5712 ; accuracy: 0.7433333333333333; loss: 2.1791350841522217\n",
      "Training epoch 5713 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5713 ; accuracy: 0.7433333333333333; loss: 2.179152488708496\n",
      "Training epoch 5714 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5714 ; accuracy: 0.7433333333333333; loss: 2.1791579723358154\n",
      "Training epoch 5715 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5715 ; accuracy: 0.7433333333333333; loss: 2.179159164428711\n",
      "Training epoch 5716 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5716 ; accuracy: 0.7433333333333333; loss: 2.1791651248931885\n",
      "Training epoch 5717 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5717 ; accuracy: 0.7433333333333333; loss: 2.179166555404663\n",
      "Training epoch 5718 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5718 ; accuracy: 0.7433333333333333; loss: 2.179170846939087\n",
      "Training epoch 5719 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5719 ; accuracy: 0.7433333333333333; loss: 2.1791775226593018\n",
      "Training epoch 5720 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5720 ; accuracy: 0.7433333333333333; loss: 2.179180860519409\n",
      "Training epoch 5721 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5721 ; accuracy: 0.7433333333333333; loss: 2.179187536239624\n",
      "Training epoch 5722 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5722 ; accuracy: 0.7433333333333333; loss: 2.179201126098633\n",
      "Training epoch 5723 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5723 ; accuracy: 0.7433333333333333; loss: 2.179216146469116\n",
      "Training epoch 5724 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5724 ; accuracy: 0.7433333333333333; loss: 2.1792333126068115\n",
      "Training epoch 5725 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5725 ; accuracy: 0.7433333333333333; loss: 2.179255485534668\n",
      "Training epoch 5726 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5726 ; accuracy: 0.7433333333333333; loss: 2.17927622795105\n",
      "Training epoch 5727 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5727 ; accuracy: 0.7433333333333333; loss: 2.179295539855957\n",
      "Training epoch 5728 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5728 ; accuracy: 0.74; loss: 2.179330587387085\n",
      "Training epoch 5729 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5729 ; accuracy: 0.74; loss: 2.179365396499634\n",
      "Training epoch 5730 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5730 ; accuracy: 0.74; loss: 2.1794066429138184\n",
      "Training epoch 5731 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5731 ; accuracy: 0.74; loss: 2.1794419288635254\n",
      "Training epoch 5732 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5732 ; accuracy: 0.74; loss: 2.1794815063476562\n",
      "Training epoch 5733 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5733 ; accuracy: 0.74; loss: 2.1795217990875244\n",
      "Training epoch 5734 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5734 ; accuracy: 0.74; loss: 2.1795637607574463\n",
      "Training epoch 5735 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5735 ; accuracy: 0.74; loss: 2.1796092987060547\n",
      "Training epoch 5736 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5736 ; accuracy: 0.74; loss: 2.1796460151672363\n",
      "Training epoch 5737 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5737 ; accuracy: 0.74; loss: 2.179680585861206\n",
      "Training epoch 5738 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5738 ; accuracy: 0.74; loss: 2.179715871810913\n",
      "Training epoch 5739 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5739 ; accuracy: 0.74; loss: 2.1797449588775635\n",
      "Training epoch 5740 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5740 ; accuracy: 0.74; loss: 2.17976975440979\n",
      "Training epoch 5741 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5741 ; accuracy: 0.74; loss: 2.179790735244751\n",
      "Training epoch 5742 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5742 ; accuracy: 0.74; loss: 2.179814100265503\n",
      "Training epoch 5743 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5743 ; accuracy: 0.74; loss: 2.1798365116119385\n",
      "Training epoch 5744 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5744 ; accuracy: 0.74; loss: 2.17987060546875\n",
      "Training epoch 5745 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5745 ; accuracy: 0.74; loss: 2.179896116256714\n",
      "Training epoch 5746 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5746 ; accuracy: 0.74; loss: 2.179920196533203\n",
      "Training epoch 5747 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5747 ; accuracy: 0.74; loss: 2.1799509525299072\n",
      "Training epoch 5748 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5748 ; accuracy: 0.74; loss: 2.179983139038086\n",
      "Training epoch 5749 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5749 ; accuracy: 0.74; loss: 2.1800105571746826\n",
      "Training epoch 5750 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5750 ; accuracy: 0.74; loss: 2.180032730102539\n",
      "Training epoch 5751 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5751 ; accuracy: 0.74; loss: 2.1800599098205566\n",
      "Training epoch 5752 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5752 ; accuracy: 0.74; loss: 2.1800570487976074\n",
      "Training epoch 5753 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5753 ; accuracy: 0.74; loss: 2.180049419403076\n",
      "Training epoch 5754 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5754 ; accuracy: 0.74; loss: 2.1800460815429688\n",
      "Training epoch 5755 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5755 ; accuracy: 0.74; loss: 2.1800475120544434\n",
      "Training epoch 5756 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5756 ; accuracy: 0.74; loss: 2.1800570487976074\n",
      "Training epoch 5757 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5757 ; accuracy: 0.74; loss: 2.1800665855407715\n",
      "Training epoch 5758 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5758 ; accuracy: 0.74; loss: 2.180079460144043\n",
      "Training epoch 5759 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5759 ; accuracy: 0.74; loss: 2.1800904273986816\n",
      "Training epoch 5760 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5760 ; accuracy: 0.74; loss: 2.180098056793213\n",
      "Training epoch 5761 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5761 ; accuracy: 0.74; loss: 2.1801114082336426\n",
      "Training epoch 5762 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5762 ; accuracy: 0.74; loss: 2.180126428604126\n",
      "Training epoch 5763 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5763 ; accuracy: 0.74; loss: 2.1801395416259766\n",
      "Training epoch 5764 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5764 ; accuracy: 0.74; loss: 2.1801388263702393\n",
      "Training epoch 5765 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5765 ; accuracy: 0.74; loss: 2.1801390647888184\n",
      "Training epoch 5766 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5766 ; accuracy: 0.74; loss: 2.1801490783691406\n",
      "Training epoch 5767 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5767 ; accuracy: 0.74; loss: 2.1801555156707764\n",
      "Training epoch 5768 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5768 ; accuracy: 0.74; loss: 2.1801652908325195\n",
      "Training epoch 5769 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5769 ; accuracy: 0.74; loss: 2.180166006088257\n",
      "Training epoch 5770 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5770 ; accuracy: 0.74; loss: 2.180164098739624\n",
      "Training epoch 5771 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5771 ; accuracy: 0.74; loss: 2.1801586151123047\n",
      "Training epoch 5772 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5772 ; accuracy: 0.74; loss: 2.1801607608795166\n",
      "Training epoch 5773 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5773 ; accuracy: 0.7433333333333333; loss: 2.180177927017212\n",
      "Training epoch 5774 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5774 ; accuracy: 0.7433333333333333; loss: 2.1801881790161133\n",
      "Training epoch 5775 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5775 ; accuracy: 0.7433333333333333; loss: 2.1801953315734863\n",
      "Training epoch 5776 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5776 ; accuracy: 0.7433333333333333; loss: 2.1802144050598145\n",
      "Training epoch 5777 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5777 ; accuracy: 0.7433333333333333; loss: 2.1802380084991455\n",
      "Training epoch 5778 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5778 ; accuracy: 0.7433333333333333; loss: 2.1802618503570557\n",
      "Training epoch 5779 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5779 ; accuracy: 0.7433333333333333; loss: 2.180284023284912\n",
      "Training epoch 5780 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5780 ; accuracy: 0.7433333333333333; loss: 2.1803083419799805\n",
      "Training epoch 5781 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5781 ; accuracy: 0.7433333333333333; loss: 2.180330991744995\n",
      "Training epoch 5782 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5782 ; accuracy: 0.7433333333333333; loss: 2.180356740951538\n",
      "Training epoch 5783 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5783 ; accuracy: 0.7433333333333333; loss: 2.180388927459717\n",
      "Training epoch 5784 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5784 ; accuracy: 0.7433333333333333; loss: 2.180419921875\n",
      "Training epoch 5785 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5785 ; accuracy: 0.7433333333333333; loss: 2.1804468631744385\n",
      "Training epoch 5786 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5786 ; accuracy: 0.7433333333333333; loss: 2.1804816722869873\n",
      "Training epoch 5787 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5787 ; accuracy: 0.7433333333333333; loss: 2.1805195808410645\n",
      "Training epoch 5788 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5788 ; accuracy: 0.7433333333333333; loss: 2.1805474758148193\n",
      "Training epoch 5789 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5789 ; accuracy: 0.7433333333333333; loss: 2.180572748184204\n",
      "Training epoch 5790 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5790 ; accuracy: 0.7433333333333333; loss: 2.1805975437164307\n",
      "Training epoch 5791 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5791 ; accuracy: 0.7433333333333333; loss: 2.1806204319000244\n",
      "Training epoch 5792 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5792 ; accuracy: 0.7433333333333333; loss: 2.1806464195251465\n",
      "Training epoch 5793 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5793 ; accuracy: 0.74; loss: 2.1806869506835938\n",
      "Training epoch 5794 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5794 ; accuracy: 0.74; loss: 2.180729866027832\n",
      "Training epoch 5795 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5795 ; accuracy: 0.74; loss: 2.180771827697754\n",
      "Training epoch 5796 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 5796 ; accuracy: 0.74; loss: 2.1808547973632812\n",
      "Training epoch 5797 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5797 ; accuracy: 0.74; loss: 2.1809401512145996\n",
      "Training epoch 5798 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5798 ; accuracy: 0.74; loss: 2.181021213531494\n",
      "Training epoch 5799 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5799 ; accuracy: 0.74; loss: 2.1810879707336426\n",
      "Training epoch 5800 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5800 ; accuracy: 0.74; loss: 2.1811466217041016\n",
      "Training epoch 5801 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5801 ; accuracy: 0.74; loss: 2.1812002658843994\n",
      "Training epoch 5802 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5802 ; accuracy: 0.74; loss: 2.181243658065796\n",
      "Training epoch 5803 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5803 ; accuracy: 0.74; loss: 2.181288957595825\n",
      "Training epoch 5804 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5804 ; accuracy: 0.74; loss: 2.181337594985962\n",
      "Training epoch 5805 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5805 ; accuracy: 0.74; loss: 2.1813833713531494\n",
      "Training epoch 5806 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5806 ; accuracy: 0.74; loss: 2.1814262866973877\n",
      "Training epoch 5807 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5807 ; accuracy: 0.74; loss: 2.181466579437256\n",
      "Training epoch 5808 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5808 ; accuracy: 0.74; loss: 2.181504726409912\n",
      "Training epoch 5809 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5809 ; accuracy: 0.74; loss: 2.1815507411956787\n",
      "Training epoch 5810 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5810 ; accuracy: 0.74; loss: 2.181588649749756\n",
      "Training epoch 5811 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5811 ; accuracy: 0.74; loss: 2.181626319885254\n",
      "Training epoch 5812 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5812 ; accuracy: 0.74; loss: 2.18166446685791\n",
      "Training epoch 5813 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5813 ; accuracy: 0.74; loss: 2.1816980838775635\n",
      "Training epoch 5814 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5814 ; accuracy: 0.74; loss: 2.181729793548584\n",
      "Training epoch 5815 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5815 ; accuracy: 0.74; loss: 2.1817522048950195\n",
      "Training epoch 5816 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5816 ; accuracy: 0.74; loss: 2.181769371032715\n",
      "Training epoch 5817 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5817 ; accuracy: 0.74; loss: 2.181781768798828\n",
      "Training epoch 5818 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5818 ; accuracy: 0.74; loss: 2.181795835494995\n",
      "Training epoch 5819 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5819 ; accuracy: 0.74; loss: 2.1818161010742188\n",
      "Training epoch 5820 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5820 ; accuracy: 0.74; loss: 2.1818394660949707\n",
      "Training epoch 5821 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5821 ; accuracy: 0.74; loss: 2.1818618774414062\n",
      "Training epoch 5822 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5822 ; accuracy: 0.74; loss: 2.1818888187408447\n",
      "Training epoch 5823 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5823 ; accuracy: 0.7433333333333333; loss: 2.1819090843200684\n",
      "Training epoch 5824 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5824 ; accuracy: 0.7433333333333333; loss: 2.1819257736206055\n",
      "Training epoch 5825 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5825 ; accuracy: 0.7433333333333333; loss: 2.1819586753845215\n",
      "Training epoch 5826 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5826 ; accuracy: 0.7433333333333333; loss: 2.1819887161254883\n",
      "Training epoch 5827 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5827 ; accuracy: 0.7433333333333333; loss: 2.1820194721221924\n",
      "Training epoch 5828 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5828 ; accuracy: 0.7433333333333333; loss: 2.1820521354675293\n",
      "Training epoch 5829 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5829 ; accuracy: 0.7433333333333333; loss: 2.1820755004882812\n",
      "Training epoch 5830 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5830 ; accuracy: 0.7433333333333333; loss: 2.1820948123931885\n",
      "Training epoch 5831 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5831 ; accuracy: 0.7433333333333333; loss: 2.1821024417877197\n",
      "Training epoch 5832 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5832 ; accuracy: 0.7433333333333333; loss: 2.1821067333221436\n",
      "Training epoch 5833 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5833 ; accuracy: 0.7433333333333333; loss: 2.1821088790893555\n",
      "Training epoch 5834 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5834 ; accuracy: 0.7433333333333333; loss: 2.182112693786621\n",
      "Training epoch 5835 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5835 ; accuracy: 0.7433333333333333; loss: 2.182126522064209\n",
      "Training epoch 5836 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5836 ; accuracy: 0.7433333333333333; loss: 2.1821351051330566\n",
      "Training epoch 5837 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5837 ; accuracy: 0.7433333333333333; loss: 2.182138442993164\n",
      "Training epoch 5838 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5838 ; accuracy: 0.7433333333333333; loss: 2.1821467876434326\n",
      "Training epoch 5839 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5839 ; accuracy: 0.7433333333333333; loss: 2.182154893875122\n",
      "Training epoch 5840 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5840 ; accuracy: 0.7433333333333333; loss: 2.1821656227111816\n",
      "Training epoch 5841 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5841 ; accuracy: 0.7433333333333333; loss: 2.182178020477295\n",
      "Training epoch 5842 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5842 ; accuracy: 0.7433333333333333; loss: 2.182191848754883\n",
      "Training epoch 5843 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5843 ; accuracy: 0.7433333333333333; loss: 2.182203769683838\n",
      "Training epoch 5844 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5844 ; accuracy: 0.7433333333333333; loss: 2.1822304725646973\n",
      "Training epoch 5845 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5845 ; accuracy: 0.7433333333333333; loss: 2.182255506515503\n",
      "Training epoch 5846 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5846 ; accuracy: 0.7433333333333333; loss: 2.1822779178619385\n",
      "Training epoch 5847 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5847 ; accuracy: 0.7433333333333333; loss: 2.1823058128356934\n",
      "Training epoch 5848 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5848 ; accuracy: 0.7433333333333333; loss: 2.182335615158081\n",
      "Training epoch 5849 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5849 ; accuracy: 0.7433333333333333; loss: 2.1823599338531494\n",
      "Training epoch 5850 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5850 ; accuracy: 0.7433333333333333; loss: 2.182384967803955\n",
      "Training epoch 5851 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5851 ; accuracy: 0.7433333333333333; loss: 2.1824145317077637\n",
      "Training epoch 5852 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5852 ; accuracy: 0.7433333333333333; loss: 2.182438850402832\n",
      "Training epoch 5853 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5853 ; accuracy: 0.7433333333333333; loss: 2.1824662685394287\n",
      "Training epoch 5854 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5854 ; accuracy: 0.7433333333333333; loss: 2.1824967861175537\n",
      "Training epoch 5855 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5855 ; accuracy: 0.7433333333333333; loss: 2.182525634765625\n",
      "Training epoch 5856 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5856 ; accuracy: 0.7433333333333333; loss: 2.182548761367798\n",
      "Training epoch 5857 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5857 ; accuracy: 0.7433333333333333; loss: 2.1825711727142334\n",
      "Training epoch 5858 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5858 ; accuracy: 0.7433333333333333; loss: 2.1825907230377197\n",
      "Training epoch 5859 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5859 ; accuracy: 0.7433333333333333; loss: 2.1826069355010986\n",
      "Training epoch 5860 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5860 ; accuracy: 0.7433333333333333; loss: 2.1826331615448\n",
      "Training epoch 5861 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5861 ; accuracy: 0.7433333333333333; loss: 2.1826624870300293\n",
      "Training epoch 5862 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5862 ; accuracy: 0.7433333333333333; loss: 2.1826913356781006\n",
      "Training epoch 5863 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5863 ; accuracy: 0.7433333333333333; loss: 2.1827192306518555\n",
      "Training epoch 5864 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5864 ; accuracy: 0.7433333333333333; loss: 2.1827456951141357\n",
      "Training epoch 5865 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5865 ; accuracy: 0.7433333333333333; loss: 2.1827704906463623\n",
      "Training epoch 5866 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5866 ; accuracy: 0.7433333333333333; loss: 2.182807445526123\n",
      "Training epoch 5867 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5867 ; accuracy: 0.7433333333333333; loss: 2.182827949523926\n",
      "Training epoch 5868 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 5868 ; accuracy: 0.7433333333333333; loss: 2.183012008666992\n",
      "Training epoch 5869 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5869 ; accuracy: 0.7433333333333333; loss: 2.183181047439575\n",
      "Training epoch 5870 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5870 ; accuracy: 0.7433333333333333; loss: 2.1833336353302\n",
      "Training epoch 5871 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5871 ; accuracy: 0.7433333333333333; loss: 2.183480739593506\n",
      "Training epoch 5872 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5872 ; accuracy: 0.7433333333333333; loss: 2.1836202144622803\n",
      "Training epoch 5873 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5873 ; accuracy: 0.7433333333333333; loss: 2.1837539672851562\n",
      "Training epoch 5874 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5874 ; accuracy: 0.7433333333333333; loss: 2.1838793754577637\n",
      "Training epoch 5875 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5875 ; accuracy: 0.7433333333333333; loss: 2.183997392654419\n",
      "Training epoch 5876 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5876 ; accuracy: 0.74; loss: 2.18410587310791\n",
      "Training epoch 5877 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5877 ; accuracy: 0.74; loss: 2.1842117309570312\n",
      "Training epoch 5878 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5878 ; accuracy: 0.74; loss: 2.1843090057373047\n",
      "Training epoch 5879 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5879 ; accuracy: 0.74; loss: 2.184380054473877\n",
      "Training epoch 5880 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5880 ; accuracy: 0.74; loss: 2.184453010559082\n",
      "Training epoch 5881 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5881 ; accuracy: 0.74; loss: 2.184528112411499\n",
      "Training epoch 5882 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5882 ; accuracy: 0.74; loss: 2.1845991611480713\n",
      "Training epoch 5883 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5883 ; accuracy: 0.74; loss: 2.184663772583008\n",
      "Training epoch 5884 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5884 ; accuracy: 0.74; loss: 2.1847305297851562\n",
      "Training epoch 5885 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5885 ; accuracy: 0.74; loss: 2.184788227081299\n",
      "Training epoch 5886 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5886 ; accuracy: 0.74; loss: 2.1848413944244385\n",
      "Training epoch 5887 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5887 ; accuracy: 0.74; loss: 2.184889316558838\n",
      "Training epoch 5888 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5888 ; accuracy: 0.74; loss: 2.1849400997161865\n",
      "Training epoch 5889 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5889 ; accuracy: 0.74; loss: 2.184990644454956\n",
      "Training epoch 5890 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5890 ; accuracy: 0.74; loss: 2.1850650310516357\n",
      "Training epoch 5891 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 5891 ; accuracy: 0.74; loss: 2.1851203441619873\n",
      "Training epoch 5892 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5892 ; accuracy: 0.74; loss: 2.1851654052734375\n",
      "Training epoch 5893 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5893 ; accuracy: 0.74; loss: 2.185211181640625\n",
      "Training epoch 5894 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5894 ; accuracy: 0.74; loss: 2.1852593421936035\n",
      "Training epoch 5895 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5895 ; accuracy: 0.74; loss: 2.185295820236206\n",
      "Training epoch 5896 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5896 ; accuracy: 0.74; loss: 2.1853318214416504\n",
      "Training epoch 5897 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5897 ; accuracy: 0.74; loss: 2.1853764057159424\n",
      "Training epoch 5898 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5898 ; accuracy: 0.74; loss: 2.185408592224121\n",
      "Training epoch 5899 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5899 ; accuracy: 0.74; loss: 2.185438632965088\n",
      "Training epoch 5900 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5900 ; accuracy: 0.74; loss: 2.185465097427368\n",
      "Training epoch 5901 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5901 ; accuracy: 0.74; loss: 2.185488224029541\n",
      "Training epoch 5902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5902 ; accuracy: 0.74; loss: 2.1855132579803467\n",
      "Training epoch 5903 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5903 ; accuracy: 0.74; loss: 2.1855263710021973\n",
      "Training epoch 5904 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5904 ; accuracy: 0.74; loss: 2.1855392456054688\n",
      "Training epoch 5905 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5905 ; accuracy: 0.74; loss: 2.1855545043945312\n",
      "Training epoch 5906 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5906 ; accuracy: 0.74; loss: 2.1855692863464355\n",
      "Training epoch 5907 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5907 ; accuracy: 0.74; loss: 2.1855878829956055\n",
      "Training epoch 5908 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5908 ; accuracy: 0.74; loss: 2.1856045722961426\n",
      "Training epoch 5909 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5909 ; accuracy: 0.74; loss: 2.1856284141540527\n",
      "Training epoch 5910 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5910 ; accuracy: 0.74; loss: 2.1856513023376465\n",
      "Training epoch 5911 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5911 ; accuracy: 0.74; loss: 2.1856746673583984\n",
      "Training epoch 5912 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5912 ; accuracy: 0.74; loss: 2.1856918334960938\n",
      "Training epoch 5913 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5913 ; accuracy: 0.74; loss: 2.185720920562744\n",
      "Training epoch 5914 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5914 ; accuracy: 0.74; loss: 2.1857547760009766\n",
      "Training epoch 5915 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5915 ; accuracy: 0.74; loss: 2.185791015625\n",
      "Training epoch 5916 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5916 ; accuracy: 0.74; loss: 2.1858301162719727\n",
      "Training epoch 5917 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5917 ; accuracy: 0.74; loss: 2.1858668327331543\n",
      "Training epoch 5918 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5918 ; accuracy: 0.74; loss: 2.1858930587768555\n",
      "Training epoch 5919 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5919 ; accuracy: 0.74; loss: 2.185917615890503\n",
      "Training epoch 5920 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5920 ; accuracy: 0.74; loss: 2.1859428882598877\n",
      "Training epoch 5921 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5921 ; accuracy: 0.74; loss: 2.185976028442383\n",
      "Training epoch 5922 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5922 ; accuracy: 0.74; loss: 2.1860151290893555\n",
      "Training epoch 5923 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5923 ; accuracy: 0.74; loss: 2.1860525608062744\n",
      "Training epoch 5924 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5924 ; accuracy: 0.74; loss: 2.1860969066619873\n",
      "Training epoch 5925 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5925 ; accuracy: 0.74; loss: 2.18613862991333\n",
      "Training epoch 5926 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5926 ; accuracy: 0.74; loss: 2.186182737350464\n",
      "Training epoch 5927 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5927 ; accuracy: 0.74; loss: 2.1862218379974365\n",
      "Training epoch 5928 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5928 ; accuracy: 0.74; loss: 2.1862614154815674\n",
      "Training epoch 5929 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5929 ; accuracy: 0.74; loss: 2.186298370361328\n",
      "Training epoch 5930 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5930 ; accuracy: 0.74; loss: 2.1863322257995605\n",
      "Training epoch 5931 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5931 ; accuracy: 0.74; loss: 2.186363458633423\n",
      "Training epoch 5932 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5932 ; accuracy: 0.74; loss: 2.186389446258545\n",
      "Training epoch 5933 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5933 ; accuracy: 0.74; loss: 2.1864194869995117\n",
      "Training epoch 5934 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5934 ; accuracy: 0.74; loss: 2.1864514350891113\n",
      "Training epoch 5935 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5935 ; accuracy: 0.74; loss: 2.1864771842956543\n",
      "Training epoch 5936 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5936 ; accuracy: 0.74; loss: 2.1865079402923584\n",
      "Training epoch 5937 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5937 ; accuracy: 0.74; loss: 2.186535120010376\n",
      "Training epoch 5938 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5938 ; accuracy: 0.74; loss: 2.1865615844726562\n",
      "Training epoch 5939 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5939 ; accuracy: 0.74; loss: 2.186586856842041\n",
      "Training epoch 5940 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5940 ; accuracy: 0.74; loss: 2.1866254806518555\n",
      "Training epoch 5941 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5941 ; accuracy: 0.74; loss: 2.1866865158081055\n",
      "Training epoch 5942 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5942 ; accuracy: 0.74; loss: 2.1867411136627197\n",
      "Training epoch 5943 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5943 ; accuracy: 0.74; loss: 2.1867852210998535\n",
      "Training epoch 5944 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5944 ; accuracy: 0.74; loss: 2.186829090118408\n",
      "Training epoch 5945 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5945 ; accuracy: 0.74; loss: 2.1868746280670166\n",
      "Training epoch 5946 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5946 ; accuracy: 0.74; loss: 2.1869137287139893\n",
      "Training epoch 5947 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5947 ; accuracy: 0.74; loss: 2.1869592666625977\n",
      "Training epoch 5948 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5948 ; accuracy: 0.74; loss: 2.1870012283325195\n",
      "Training epoch 5949 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5949 ; accuracy: 0.74; loss: 2.1870532035827637\n",
      "Training epoch 5950 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5950 ; accuracy: 0.74; loss: 2.1871018409729004\n",
      "Training epoch 5951 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5951 ; accuracy: 0.74; loss: 2.1871442794799805\n",
      "Training epoch 5952 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5952 ; accuracy: 0.74; loss: 2.187183380126953\n",
      "Training epoch 5953 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5953 ; accuracy: 0.74; loss: 2.1872265338897705\n",
      "Training epoch 5954 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5954 ; accuracy: 0.74; loss: 2.187258005142212\n",
      "Training epoch 5955 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5955 ; accuracy: 0.74; loss: 2.187283515930176\n",
      "Training epoch 5956 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5956 ; accuracy: 0.74; loss: 2.1873161792755127\n",
      "Training epoch 5957 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5957 ; accuracy: 0.74; loss: 2.187356472015381\n",
      "Training epoch 5958 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5958 ; accuracy: 0.74; loss: 2.187382936477661\n",
      "Training epoch 5959 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5959 ; accuracy: 0.74; loss: 2.187412738800049\n",
      "Training epoch 5960 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5960 ; accuracy: 0.74; loss: 2.1874406337738037\n",
      "Training epoch 5961 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5961 ; accuracy: 0.74; loss: 2.187467098236084\n",
      "Training epoch 5962 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5962 ; accuracy: 0.74; loss: 2.1874890327453613\n",
      "Training epoch 5963 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5963 ; accuracy: 0.74; loss: 2.1875205039978027\n",
      "Training epoch 5964 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5964 ; accuracy: 0.74; loss: 2.187551736831665\n",
      "Training epoch 5965 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5965 ; accuracy: 0.74; loss: 2.1875810623168945\n",
      "Training epoch 5966 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5966 ; accuracy: 0.74; loss: 2.1876027584075928\n",
      "Training epoch 5967 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5967 ; accuracy: 0.74; loss: 2.1876232624053955\n",
      "Training epoch 5968 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5968 ; accuracy: 0.74; loss: 2.1876394748687744\n",
      "Training epoch 5969 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5969 ; accuracy: 0.74; loss: 2.1876590251922607\n",
      "Training epoch 5970 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5970 ; accuracy: 0.74; loss: 2.1876773834228516\n",
      "Training epoch 5971 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5971 ; accuracy: 0.74; loss: 2.1876935958862305\n",
      "Training epoch 5972 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5972 ; accuracy: 0.74; loss: 2.187725305557251\n",
      "Training epoch 5973 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5973 ; accuracy: 0.74; loss: 2.1877596378326416\n",
      "Training epoch 5974 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5974 ; accuracy: 0.74; loss: 2.1877992153167725\n",
      "Training epoch 5975 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5975 ; accuracy: 0.74; loss: 2.1878442764282227\n",
      "Training epoch 5976 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5976 ; accuracy: 0.74; loss: 2.1878886222839355\n",
      "Training epoch 5977 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5977 ; accuracy: 0.74; loss: 2.187933921813965\n",
      "Training epoch 5978 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5978 ; accuracy: 0.74; loss: 2.1879723072052\n",
      "Training epoch 5979 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5979 ; accuracy: 0.74; loss: 2.1880152225494385\n",
      "Training epoch 5980 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5980 ; accuracy: 0.74; loss: 2.1880502700805664\n",
      "Training epoch 5981 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5981 ; accuracy: 0.74; loss: 2.188080310821533\n",
      "Training epoch 5982 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5982 ; accuracy: 0.74; loss: 2.1881144046783447\n",
      "Training epoch 5983 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5983 ; accuracy: 0.74; loss: 2.1881449222564697\n",
      "Training epoch 5984 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5984 ; accuracy: 0.74; loss: 2.18817138671875\n",
      "Training epoch 5985 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5985 ; accuracy: 0.74; loss: 2.1882073879241943\n",
      "Training epoch 5986 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5986 ; accuracy: 0.74; loss: 2.188242197036743\n",
      "Training epoch 5987 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5987 ; accuracy: 0.74; loss: 2.1882734298706055\n",
      "Training epoch 5988 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5988 ; accuracy: 0.74; loss: 2.1882803440093994\n",
      "Training epoch 5989 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 5989 ; accuracy: 0.74; loss: 2.1882903575897217\n",
      "Training epoch 5990 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5990 ; accuracy: 0.74; loss: 2.1883044242858887\n",
      "Training epoch 5991 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5991 ; accuracy: 0.74; loss: 2.188319206237793\n",
      "Training epoch 5992 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5992 ; accuracy: 0.74; loss: 2.188336133956909\n",
      "Training epoch 5993 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5993 ; accuracy: 0.74; loss: 2.1883370876312256\n",
      "Training epoch 5994 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5994 ; accuracy: 0.74; loss: 2.1883413791656494\n",
      "Training epoch 5995 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5995 ; accuracy: 0.74; loss: 2.1883511543273926\n",
      "Training epoch 5996 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5996 ; accuracy: 0.74; loss: 2.188361406326294\n",
      "Training epoch 5997 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5997 ; accuracy: 0.74; loss: 2.1883745193481445\n",
      "Training epoch 5998 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 5998 ; accuracy: 0.74; loss: 2.1883935928344727\n",
      "Training epoch 5999 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 5999 ; accuracy: 0.74; loss: 2.1884050369262695\n",
      "Training epoch 6000 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6000 ; accuracy: 0.74; loss: 2.188419818878174\n",
      "Training epoch 6001 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6001 ; accuracy: 0.74; loss: 2.188434362411499\n",
      "Training epoch 6002 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6002 ; accuracy: 0.74; loss: 2.1884403228759766\n",
      "Training epoch 6003 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6003 ; accuracy: 0.74; loss: 2.1884491443634033\n",
      "Training epoch 6004 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6004 ; accuracy: 0.74; loss: 2.188453435897827\n",
      "Training epoch 6005 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6005 ; accuracy: 0.74; loss: 2.1884615421295166\n",
      "Training epoch 6006 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6006 ; accuracy: 0.74; loss: 2.1884732246398926\n",
      "Training epoch 6007 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6007 ; accuracy: 0.74; loss: 2.1884870529174805\n",
      "Training epoch 6008 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6008 ; accuracy: 0.74; loss: 2.1884989738464355\n",
      "Training epoch 6009 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6009 ; accuracy: 0.74; loss: 2.1884984970092773\n",
      "Training epoch 6010 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6010 ; accuracy: 0.74; loss: 2.1885037422180176\n",
      "Training epoch 6011 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6011 ; accuracy: 0.74; loss: 2.1885128021240234\n",
      "Training epoch 6012 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6012 ; accuracy: 0.74; loss: 2.1885251998901367\n",
      "Training epoch 6013 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6013 ; accuracy: 0.74; loss: 2.188537359237671\n",
      "Training epoch 6014 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6014 ; accuracy: 0.74; loss: 2.1885464191436768\n",
      "Training epoch 6015 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6015 ; accuracy: 0.74; loss: 2.18855357170105\n",
      "Training epoch 6016 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6016 ; accuracy: 0.74; loss: 2.1885714530944824\n",
      "Training epoch 6017 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6017 ; accuracy: 0.74; loss: 2.188585042953491\n",
      "Training epoch 6018 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6018 ; accuracy: 0.74; loss: 2.1886062622070312\n",
      "Training epoch 6019 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6019 ; accuracy: 0.74; loss: 2.1886284351348877\n",
      "Training epoch 6020 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6020 ; accuracy: 0.74; loss: 2.1886520385742188\n",
      "Training epoch 6021 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6021 ; accuracy: 0.74; loss: 2.1886801719665527\n",
      "Training epoch 6022 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6022 ; accuracy: 0.74; loss: 2.1887073516845703\n",
      "Training epoch 6023 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6023 ; accuracy: 0.74; loss: 2.1887309551239014\n",
      "Training epoch 6024 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6024 ; accuracy: 0.74; loss: 2.1887619495391846\n",
      "Training epoch 6025 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6025 ; accuracy: 0.74; loss: 2.1887898445129395\n",
      "Training epoch 6026 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6026 ; accuracy: 0.74; loss: 2.1888227462768555\n",
      "Training epoch 6027 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6027 ; accuracy: 0.74; loss: 2.1888599395751953\n",
      "Training epoch 6028 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6028 ; accuracy: 0.74; loss: 2.1888933181762695\n",
      "Training epoch 6029 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6029 ; accuracy: 0.74; loss: 2.188927173614502\n",
      "Training epoch 6030 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6030 ; accuracy: 0.74; loss: 2.1889779567718506\n",
      "Training epoch 6031 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6031 ; accuracy: 0.74; loss: 2.1890299320220947\n",
      "Training epoch 6032 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6032 ; accuracy: 0.74; loss: 2.189082384109497\n",
      "Training epoch 6033 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6033 ; accuracy: 0.74; loss: 2.1891348361968994\n",
      "Training epoch 6034 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6034 ; accuracy: 0.74; loss: 2.189183235168457\n",
      "Training epoch 6035 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6035 ; accuracy: 0.74; loss: 2.1892333030700684\n",
      "Training epoch 6036 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6036 ; accuracy: 0.74; loss: 2.189284324645996\n",
      "Training epoch 6037 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6037 ; accuracy: 0.74; loss: 2.189319372177124\n",
      "Training epoch 6038 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6038 ; accuracy: 0.74; loss: 2.1893558502197266\n",
      "Training epoch 6039 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6039 ; accuracy: 0.74; loss: 2.1893882751464844\n",
      "Training epoch 6040 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6040 ; accuracy: 0.74; loss: 2.189427375793457\n",
      "Training epoch 6041 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6041 ; accuracy: 0.74; loss: 2.189464569091797\n",
      "Training epoch 6042 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6042 ; accuracy: 0.74; loss: 2.1894991397857666\n",
      "Training epoch 6043 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6043 ; accuracy: 0.74; loss: 2.1895337104797363\n",
      "Training epoch 6044 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6044 ; accuracy: 0.74; loss: 2.1895670890808105\n",
      "Training epoch 6045 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6045 ; accuracy: 0.74; loss: 2.1895997524261475\n",
      "Training epoch 6046 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6046 ; accuracy: 0.74; loss: 2.1896321773529053\n",
      "Training epoch 6047 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6047 ; accuracy: 0.74; loss: 2.1896607875823975\n",
      "Training epoch 6048 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6048 ; accuracy: 0.74; loss: 2.1896839141845703\n",
      "Training epoch 6049 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6049 ; accuracy: 0.74; loss: 2.189697265625\n",
      "Training epoch 6050 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6050 ; accuracy: 0.74; loss: 2.189706802368164\n",
      "Training epoch 6051 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6051 ; accuracy: 0.74; loss: 2.189714193344116\n",
      "Training epoch 6052 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6052 ; accuracy: 0.74; loss: 2.189722776412964\n",
      "Training epoch 6053 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6053 ; accuracy: 0.74; loss: 2.1897242069244385\n",
      "Training epoch 6054 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6054 ; accuracy: 0.74; loss: 2.1897268295288086\n",
      "Training epoch 6055 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6055 ; accuracy: 0.74; loss: 2.1897265911102295\n",
      "Training epoch 6056 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6056 ; accuracy: 0.74; loss: 2.189718723297119\n",
      "Training epoch 6057 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6057 ; accuracy: 0.74; loss: 2.1897172927856445\n",
      "Training epoch 6058 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6058 ; accuracy: 0.74; loss: 2.189720392227173\n",
      "Training epoch 6059 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6059 ; accuracy: 0.74; loss: 2.1897222995758057\n",
      "Training epoch 6060 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6060 ; accuracy: 0.74; loss: 2.1897149085998535\n",
      "Training epoch 6061 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6061 ; accuracy: 0.74; loss: 2.189706325531006\n",
      "Training epoch 6062 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6062 ; accuracy: 0.74; loss: 2.189687967300415\n",
      "Training epoch 6063 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6063 ; accuracy: 0.74; loss: 2.189668893814087\n",
      "Training epoch 6064 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6064 ; accuracy: 0.7433333333333333; loss: 2.189650774002075\n",
      "Training epoch 6065 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6065 ; accuracy: 0.7433333333333333; loss: 2.1896398067474365\n",
      "Training epoch 6066 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6066 ; accuracy: 0.7433333333333333; loss: 2.189635753631592\n",
      "Training epoch 6067 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6067 ; accuracy: 0.7433333333333333; loss: 2.189642906188965\n",
      "Training epoch 6068 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6068 ; accuracy: 0.7433333333333333; loss: 2.1896562576293945\n",
      "Training epoch 6069 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6069 ; accuracy: 0.7433333333333333; loss: 2.1896772384643555\n",
      "Training epoch 6070 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6070 ; accuracy: 0.7433333333333333; loss: 2.189699411392212\n",
      "Training epoch 6071 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6071 ; accuracy: 0.7433333333333333; loss: 2.1897215843200684\n",
      "Training epoch 6072 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6072 ; accuracy: 0.7433333333333333; loss: 2.1897356510162354\n",
      "Training epoch 6073 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6073 ; accuracy: 0.7433333333333333; loss: 2.189751148223877\n",
      "Training epoch 6074 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6074 ; accuracy: 0.7433333333333333; loss: 2.1897776126861572\n",
      "Training epoch 6075 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6075 ; accuracy: 0.7433333333333333; loss: 2.1898036003112793\n",
      "Training epoch 6076 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6076 ; accuracy: 0.7433333333333333; loss: 2.189831256866455\n",
      "Training epoch 6077 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6077 ; accuracy: 0.7433333333333333; loss: 2.1898601055145264\n",
      "Training epoch 6078 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6078 ; accuracy: 0.7433333333333333; loss: 2.189882516860962\n",
      "Training epoch 6079 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6079 ; accuracy: 0.7433333333333333; loss: 2.189901828765869\n",
      "Training epoch 6080 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6080 ; accuracy: 0.7433333333333333; loss: 2.189920663833618\n",
      "Training epoch 6081 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6081 ; accuracy: 0.7433333333333333; loss: 2.1899354457855225\n",
      "Training epoch 6082 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6082 ; accuracy: 0.7433333333333333; loss: 2.1899755001068115\n",
      "Training epoch 6083 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6083 ; accuracy: 0.7433333333333333; loss: 2.1900157928466797\n",
      "Training epoch 6084 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6084 ; accuracy: 0.7433333333333333; loss: 2.190053701400757\n",
      "Training epoch 6085 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6085 ; accuracy: 0.7433333333333333; loss: 2.1900932788848877\n",
      "Training epoch 6086 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6086 ; accuracy: 0.7433333333333333; loss: 2.1901259422302246\n",
      "Training epoch 6087 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6087 ; accuracy: 0.7433333333333333; loss: 2.190166473388672\n",
      "Training epoch 6088 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6088 ; accuracy: 0.7433333333333333; loss: 2.190214157104492\n",
      "Training epoch 6089 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6089 ; accuracy: 0.7433333333333333; loss: 2.1902542114257812\n",
      "Training epoch 6090 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6090 ; accuracy: 0.7433333333333333; loss: 2.1902925968170166\n",
      "Training epoch 6091 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6091 ; accuracy: 0.7433333333333333; loss: 2.1903302669525146\n",
      "Training epoch 6092 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6092 ; accuracy: 0.7433333333333333; loss: 2.1903698444366455\n",
      "Training epoch 6093 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6093 ; accuracy: 0.74; loss: 2.190403938293457\n",
      "Training epoch 6094 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6094 ; accuracy: 0.74; loss: 2.1904261112213135\n",
      "Training epoch 6095 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6095 ; accuracy: 0.74; loss: 2.1904468536376953\n",
      "Training epoch 6096 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6096 ; accuracy: 0.74; loss: 2.1904664039611816\n",
      "Training epoch 6097 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6097 ; accuracy: 0.74; loss: 2.1904845237731934\n",
      "Training epoch 6098 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6098 ; accuracy: 0.74; loss: 2.190502882003784\n",
      "Training epoch 6099 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6099 ; accuracy: 0.74; loss: 2.1905155181884766\n",
      "Training epoch 6100 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6100 ; accuracy: 0.74; loss: 2.1905248165130615\n",
      "Training epoch 6101 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 6101 ; accuracy: 0.74; loss: 2.1905484199523926\n",
      "Training epoch 6102 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6102 ; accuracy: 0.74; loss: 2.1905734539031982\n",
      "Training epoch 6103 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6103 ; accuracy: 0.74; loss: 2.1905980110168457\n",
      "Training epoch 6104 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6104 ; accuracy: 0.74; loss: 2.1906237602233887\n",
      "Training epoch 6105 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6105 ; accuracy: 0.74; loss: 2.1906545162200928\n",
      "Training epoch 6106 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6106 ; accuracy: 0.7433333333333333; loss: 2.190682888031006\n",
      "Training epoch 6107 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6107 ; accuracy: 0.7433333333333333; loss: 2.1907007694244385\n",
      "Training epoch 6108 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6108 ; accuracy: 0.7433333333333333; loss: 2.190723180770874\n",
      "Training epoch 6109 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6109 ; accuracy: 0.7433333333333333; loss: 2.190739154815674\n",
      "Training epoch 6110 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6110 ; accuracy: 0.7433333333333333; loss: 2.1907577514648438\n",
      "Training epoch 6111 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6111 ; accuracy: 0.7433333333333333; loss: 2.190767765045166\n",
      "Training epoch 6112 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6112 ; accuracy: 0.7433333333333333; loss: 2.1907765865325928\n",
      "Training epoch 6113 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6113 ; accuracy: 0.7433333333333333; loss: 2.1907825469970703\n",
      "Training epoch 6114 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6114 ; accuracy: 0.7433333333333333; loss: 2.190786123275757\n",
      "Training epoch 6115 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6115 ; accuracy: 0.7433333333333333; loss: 2.1907966136932373\n",
      "Training epoch 6116 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6116 ; accuracy: 0.7433333333333333; loss: 2.1908016204833984\n",
      "Training epoch 6117 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6117 ; accuracy: 0.7433333333333333; loss: 2.190807580947876\n",
      "Training epoch 6118 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6118 ; accuracy: 0.7433333333333333; loss: 2.1908156871795654\n",
      "Training epoch 6119 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6119 ; accuracy: 0.7433333333333333; loss: 2.190826416015625\n",
      "Training epoch 6120 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6120 ; accuracy: 0.7433333333333333; loss: 2.190842628479004\n",
      "Training epoch 6121 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6121 ; accuracy: 0.7433333333333333; loss: 2.190863847732544\n",
      "Training epoch 6122 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6122 ; accuracy: 0.7433333333333333; loss: 2.190899133682251\n",
      "Training epoch 6123 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6123 ; accuracy: 0.7433333333333333; loss: 2.1909265518188477\n",
      "Training epoch 6124 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6124 ; accuracy: 0.7433333333333333; loss: 2.1909542083740234\n",
      "Training epoch 6125 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6125 ; accuracy: 0.7433333333333333; loss: 2.190976142883301\n",
      "Training epoch 6126 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6126 ; accuracy: 0.7433333333333333; loss: 2.1910014152526855\n",
      "Training epoch 6127 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6127 ; accuracy: 0.7433333333333333; loss: 2.1910207271575928\n",
      "Training epoch 6128 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6128 ; accuracy: 0.7433333333333333; loss: 2.191032648086548\n",
      "Training epoch 6129 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6129 ; accuracy: 0.7433333333333333; loss: 2.1910440921783447\n",
      "Training epoch 6130 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6130 ; accuracy: 0.7433333333333333; loss: 2.1910600662231445\n",
      "Training epoch 6131 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6131 ; accuracy: 0.7433333333333333; loss: 2.1910881996154785\n",
      "Training epoch 6132 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6132 ; accuracy: 0.7433333333333333; loss: 2.191115140914917\n",
      "Training epoch 6133 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6133 ; accuracy: 0.7433333333333333; loss: 2.1911392211914062\n",
      "Training epoch 6134 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6134 ; accuracy: 0.7433333333333333; loss: 2.1911685466766357\n",
      "Training epoch 6135 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6135 ; accuracy: 0.7433333333333333; loss: 2.1911983489990234\n",
      "Training epoch 6136 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6136 ; accuracy: 0.7433333333333333; loss: 2.1912243366241455\n",
      "Training epoch 6137 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6137 ; accuracy: 0.7433333333333333; loss: 2.191251754760742\n",
      "Training epoch 6138 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6138 ; accuracy: 0.7433333333333333; loss: 2.191272497177124\n",
      "Training epoch 6139 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6139 ; accuracy: 0.7433333333333333; loss: 2.191301107406616\n",
      "Training epoch 6140 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6140 ; accuracy: 0.7433333333333333; loss: 2.191331624984741\n",
      "Training epoch 6141 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6141 ; accuracy: 0.7433333333333333; loss: 2.1913602352142334\n",
      "Training epoch 6142 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6142 ; accuracy: 0.74; loss: 2.191380500793457\n",
      "Training epoch 6143 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6143 ; accuracy: 0.74; loss: 2.191391706466675\n",
      "Training epoch 6144 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6144 ; accuracy: 0.74; loss: 2.1913998126983643\n",
      "Training epoch 6145 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6145 ; accuracy: 0.74; loss: 2.191408634185791\n",
      "Training epoch 6146 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6146 ; accuracy: 0.74; loss: 2.191417932510376\n",
      "Training epoch 6147 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6147 ; accuracy: 0.74; loss: 2.1914336681365967\n",
      "Training epoch 6148 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6148 ; accuracy: 0.74; loss: 2.1914498805999756\n",
      "Training epoch 6149 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6149 ; accuracy: 0.74; loss: 2.191467046737671\n",
      "Training epoch 6150 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6150 ; accuracy: 0.74; loss: 2.191483974456787\n",
      "Training epoch 6151 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6151 ; accuracy: 0.74; loss: 2.1915011405944824\n",
      "Training epoch 6152 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6152 ; accuracy: 0.74; loss: 2.1915242671966553\n",
      "Training epoch 6153 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6153 ; accuracy: 0.74; loss: 2.1915478706359863\n",
      "Training epoch 6154 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6154 ; accuracy: 0.74; loss: 2.191574811935425\n",
      "Training epoch 6155 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6155 ; accuracy: 0.74; loss: 2.191603899002075\n",
      "Training epoch 6156 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6156 ; accuracy: 0.74; loss: 2.191632032394409\n",
      "Training epoch 6157 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6157 ; accuracy: 0.74; loss: 2.1916587352752686\n",
      "Training epoch 6158 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6158 ; accuracy: 0.74; loss: 2.1916894912719727\n",
      "Training epoch 6159 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6159 ; accuracy: 0.74; loss: 2.191720724105835\n",
      "Training epoch 6160 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6160 ; accuracy: 0.74; loss: 2.191725730895996\n",
      "Training epoch 6161 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6161 ; accuracy: 0.74; loss: 2.191732168197632\n",
      "Training epoch 6162 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6162 ; accuracy: 0.74; loss: 2.191744089126587\n",
      "Training epoch 6163 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6163 ; accuracy: 0.74; loss: 2.191754102706909\n",
      "Training epoch 6164 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6164 ; accuracy: 0.74; loss: 2.1917593479156494\n",
      "Training epoch 6165 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6165 ; accuracy: 0.74; loss: 2.191768169403076\n",
      "Training epoch 6166 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6166 ; accuracy: 0.74; loss: 2.191770076751709\n",
      "Training epoch 6167 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6167 ; accuracy: 0.74; loss: 2.191774368286133\n",
      "Training epoch 6168 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6168 ; accuracy: 0.74; loss: 2.191782236099243\n",
      "Training epoch 6169 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6169 ; accuracy: 0.74; loss: 2.191784143447876\n",
      "Training epoch 6170 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6170 ; accuracy: 0.74; loss: 2.191774606704712\n",
      "Training epoch 6171 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6171 ; accuracy: 0.74; loss: 2.191767692565918\n",
      "Training epoch 6172 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6172 ; accuracy: 0.74; loss: 2.1917684078216553\n",
      "Training epoch 6173 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6173 ; accuracy: 0.74; loss: 2.1917786598205566\n",
      "Training epoch 6174 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6174 ; accuracy: 0.74; loss: 2.1918141841888428\n",
      "Training epoch 6175 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6175 ; accuracy: 0.74; loss: 2.191854953765869\n",
      "Training epoch 6176 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6176 ; accuracy: 0.74; loss: 2.191892385482788\n",
      "Training epoch 6177 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6177 ; accuracy: 0.74; loss: 2.1919262409210205\n",
      "Training epoch 6178 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6178 ; accuracy: 0.74; loss: 2.19195556640625\n",
      "Training epoch 6179 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6179 ; accuracy: 0.74; loss: 2.1919853687286377\n",
      "Training epoch 6180 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6180 ; accuracy: 0.74; loss: 2.1920180320739746\n",
      "Training epoch 6181 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6181 ; accuracy: 0.74; loss: 2.192042827606201\n",
      "Training epoch 6182 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6182 ; accuracy: 0.74; loss: 2.1920511722564697\n",
      "Training epoch 6183 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6183 ; accuracy: 0.74; loss: 2.1920547485351562\n",
      "Training epoch 6184 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6184 ; accuracy: 0.74; loss: 2.192067861557007\n",
      "Training epoch 6185 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6185 ; accuracy: 0.74; loss: 2.1920738220214844\n",
      "Training epoch 6186 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6186 ; accuracy: 0.74; loss: 2.192080020904541\n",
      "Training epoch 6187 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6187 ; accuracy: 0.74; loss: 2.1920769214630127\n",
      "Training epoch 6188 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6188 ; accuracy: 0.74; loss: 2.1920721530914307\n",
      "Training epoch 6189 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6189 ; accuracy: 0.74; loss: 2.1920723915100098\n",
      "Training epoch 6190 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6190 ; accuracy: 0.74; loss: 2.192082166671753\n",
      "Training epoch 6191 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6191 ; accuracy: 0.74; loss: 2.1921002864837646\n",
      "Training epoch 6192 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6192 ; accuracy: 0.74; loss: 2.192120313644409\n",
      "Training epoch 6193 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6193 ; accuracy: 0.74; loss: 2.192138671875\n",
      "Training epoch 6194 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6194 ; accuracy: 0.74; loss: 2.1921615600585938\n",
      "Training epoch 6195 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6195 ; accuracy: 0.74; loss: 2.192183017730713\n",
      "Training epoch 6196 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6196 ; accuracy: 0.74; loss: 2.1922013759613037\n",
      "Training epoch 6197 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6197 ; accuracy: 0.74; loss: 2.1922202110290527\n",
      "Training epoch 6198 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6198 ; accuracy: 0.74; loss: 2.192241907119751\n",
      "Training epoch 6199 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6199 ; accuracy: 0.74; loss: 2.1922571659088135\n",
      "Training epoch 6200 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6200 ; accuracy: 0.74; loss: 2.192274808883667\n",
      "Training epoch 6201 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6201 ; accuracy: 0.74; loss: 2.192286491394043\n",
      "Training epoch 6202 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6202 ; accuracy: 0.74; loss: 2.1922993659973145\n",
      "Training epoch 6203 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6203 ; accuracy: 0.74; loss: 2.1923253536224365\n",
      "Training epoch 6204 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6204 ; accuracy: 0.74; loss: 2.1923530101776123\n",
      "Training epoch 6205 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6205 ; accuracy: 0.74; loss: 2.1923835277557373\n",
      "Training epoch 6206 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6206 ; accuracy: 0.74; loss: 2.192412853240967\n",
      "Training epoch 6207 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6207 ; accuracy: 0.74; loss: 2.1924448013305664\n",
      "Training epoch 6208 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6208 ; accuracy: 0.74; loss: 2.1924755573272705\n",
      "Training epoch 6209 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6209 ; accuracy: 0.74; loss: 2.1925063133239746\n",
      "Training epoch 6210 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6210 ; accuracy: 0.74; loss: 2.192539930343628\n",
      "Training epoch 6211 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6211 ; accuracy: 0.74; loss: 2.192561626434326\n",
      "Training epoch 6212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6212 ; accuracy: 0.74; loss: 2.1925876140594482\n",
      "Training epoch 6213 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6213 ; accuracy: 0.74; loss: 2.1926143169403076\n",
      "Training epoch 6214 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6214 ; accuracy: 0.74; loss: 2.1926379203796387\n",
      "Training epoch 6215 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6215 ; accuracy: 0.74; loss: 2.192664384841919\n",
      "Training epoch 6216 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6216 ; accuracy: 0.74; loss: 2.1927294731140137\n",
      "Training epoch 6217 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6217 ; accuracy: 0.74; loss: 2.192788600921631\n",
      "Training epoch 6218 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6218 ; accuracy: 0.74; loss: 2.1928396224975586\n",
      "Training epoch 6219 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6219 ; accuracy: 0.74; loss: 2.192892551422119\n",
      "Training epoch 6220 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6220 ; accuracy: 0.74; loss: 2.1929430961608887\n",
      "Training epoch 6221 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6221 ; accuracy: 0.74; loss: 2.192995548248291\n",
      "Training epoch 6222 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6222 ; accuracy: 0.74; loss: 2.1930408477783203\n",
      "Training epoch 6223 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6223 ; accuracy: 0.74; loss: 2.1930859088897705\n",
      "Training epoch 6224 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6224 ; accuracy: 0.74; loss: 2.1931381225585938\n",
      "Training epoch 6225 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6225 ; accuracy: 0.74; loss: 2.193185329437256\n",
      "Training epoch 6226 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6226 ; accuracy: 0.74; loss: 2.1932287216186523\n",
      "Training epoch 6227 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6227 ; accuracy: 0.74; loss: 2.193267822265625\n",
      "Training epoch 6228 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6228 ; accuracy: 0.74; loss: 2.1933066844940186\n",
      "Training epoch 6229 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6229 ; accuracy: 0.74; loss: 2.193340539932251\n",
      "Training epoch 6230 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6230 ; accuracy: 0.74; loss: 2.1933646202087402\n",
      "Training epoch 6231 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6231 ; accuracy: 0.74; loss: 2.193382978439331\n",
      "Training epoch 6232 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6232 ; accuracy: 0.74; loss: 2.1934032440185547\n",
      "Training epoch 6233 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6233 ; accuracy: 0.74; loss: 2.1934285163879395\n",
      "Training epoch 6234 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6234 ; accuracy: 0.74; loss: 2.1934521198272705\n",
      "Training epoch 6235 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6235 ; accuracy: 0.74; loss: 2.1934776306152344\n",
      "Training epoch 6236 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6236 ; accuracy: 0.74; loss: 2.1935043334960938\n",
      "Training epoch 6237 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6237 ; accuracy: 0.74; loss: 2.1935360431671143\n",
      "Training epoch 6238 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6238 ; accuracy: 0.74; loss: 2.193568229675293\n",
      "Training epoch 6239 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6239 ; accuracy: 0.74; loss: 2.1936099529266357\n",
      "Training epoch 6240 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6240 ; accuracy: 0.74; loss: 2.19364595413208\n",
      "Training epoch 6241 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6241 ; accuracy: 0.74; loss: 2.1936824321746826\n",
      "Training epoch 6242 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6242 ; accuracy: 0.74; loss: 2.1937124729156494\n",
      "Training epoch 6243 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6243 ; accuracy: 0.74; loss: 2.193742275238037\n",
      "Training epoch 6244 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6244 ; accuracy: 0.74; loss: 2.1937708854675293\n",
      "Training epoch 6245 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6245 ; accuracy: 0.74; loss: 2.1937973499298096\n",
      "Training epoch 6246 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6246 ; accuracy: 0.74; loss: 2.1938226222991943\n",
      "Training epoch 6247 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6247 ; accuracy: 0.74; loss: 2.1938490867614746\n",
      "Training epoch 6248 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6248 ; accuracy: 0.74; loss: 2.193882703781128\n",
      "Training epoch 6249 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6249 ; accuracy: 0.74; loss: 2.1938745975494385\n",
      "Training epoch 6250 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6250 ; accuracy: 0.74; loss: 2.193873643875122\n",
      "Training epoch 6251 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6251 ; accuracy: 0.74; loss: 2.1938862800598145\n",
      "Training epoch 6252 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6252 ; accuracy: 0.74; loss: 2.193899393081665\n",
      "Training epoch 6253 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6253 ; accuracy: 0.74; loss: 2.1939172744750977\n",
      "Training epoch 6254 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6254 ; accuracy: 0.74; loss: 2.193937063217163\n",
      "Training epoch 6255 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6255 ; accuracy: 0.74; loss: 2.193955421447754\n",
      "Training epoch 6256 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6256 ; accuracy: 0.74; loss: 2.1939857006073\n",
      "Training epoch 6257 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6257 ; accuracy: 0.74; loss: 2.194014072418213\n",
      "Training epoch 6258 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6258 ; accuracy: 0.74; loss: 2.194044351577759\n",
      "Training epoch 6259 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6259 ; accuracy: 0.74; loss: 2.1940722465515137\n",
      "Training epoch 6260 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6260 ; accuracy: 0.74; loss: 2.1941187381744385\n",
      "Training epoch 6261 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6261 ; accuracy: 0.74; loss: 2.194164514541626\n",
      "Training epoch 6262 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6262 ; accuracy: 0.74; loss: 2.194209575653076\n",
      "Training epoch 6263 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6263 ; accuracy: 0.74; loss: 2.19425368309021\n",
      "Training epoch 6264 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6264 ; accuracy: 0.74; loss: 2.194296360015869\n",
      "Training epoch 6265 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6265 ; accuracy: 0.74; loss: 2.19433856010437\n",
      "Training epoch 6266 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6266 ; accuracy: 0.74; loss: 2.1943793296813965\n",
      "Training epoch 6267 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6267 ; accuracy: 0.74; loss: 2.1944193840026855\n",
      "Training epoch 6268 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6268 ; accuracy: 0.74; loss: 2.1944518089294434\n",
      "Training epoch 6269 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6269 ; accuracy: 0.74; loss: 2.1944878101348877\n",
      "Training epoch 6270 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6270 ; accuracy: 0.74; loss: 2.194516658782959\n",
      "Training epoch 6271 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6271 ; accuracy: 0.74; loss: 2.194546937942505\n",
      "Training epoch 6272 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6272 ; accuracy: 0.74; loss: 2.194582223892212\n",
      "Training epoch 6273 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6273 ; accuracy: 0.74; loss: 2.194624185562134\n",
      "Training epoch 6274 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6274 ; accuracy: 0.74; loss: 2.1946699619293213\n",
      "Training epoch 6275 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6275 ; accuracy: 0.74; loss: 2.1947145462036133\n",
      "Training epoch 6276 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6276 ; accuracy: 0.74; loss: 2.1947617530822754\n",
      "Training epoch 6277 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6277 ; accuracy: 0.74; loss: 2.194810628890991\n",
      "Training epoch 6278 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6278 ; accuracy: 0.74; loss: 2.194880485534668\n",
      "Training epoch 6279 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6279 ; accuracy: 0.74; loss: 2.194950819015503\n",
      "Training epoch 6280 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6280 ; accuracy: 0.74; loss: 2.1950175762176514\n",
      "Training epoch 6281 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6281 ; accuracy: 0.74; loss: 2.1950759887695312\n",
      "Training epoch 6282 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6282 ; accuracy: 0.74; loss: 2.195117235183716\n",
      "Training epoch 6283 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6283 ; accuracy: 0.74; loss: 2.1951560974121094\n",
      "Training epoch 6284 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6284 ; accuracy: 0.74; loss: 2.195193290710449\n",
      "Training epoch 6285 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6285 ; accuracy: 0.74; loss: 2.195230484008789\n",
      "Training epoch 6286 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6286 ; accuracy: 0.74; loss: 2.1953001022338867\n",
      "Training epoch 6287 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6287 ; accuracy: 0.74; loss: 2.1953680515289307\n",
      "Training epoch 6288 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6288 ; accuracy: 0.74; loss: 2.195427179336548\n",
      "Training epoch 6289 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6289 ; accuracy: 0.74; loss: 2.195488214492798\n",
      "Training epoch 6290 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6290 ; accuracy: 0.74; loss: 2.1955578327178955\n",
      "Training epoch 6291 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6291 ; accuracy: 0.74; loss: 2.1956231594085693\n",
      "Training epoch 6292 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6292 ; accuracy: 0.74; loss: 2.195680618286133\n",
      "Training epoch 6293 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6293 ; accuracy: 0.74; loss: 2.1957364082336426\n",
      "Training epoch 6294 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6294 ; accuracy: 0.74; loss: 2.1957895755767822\n",
      "Training epoch 6295 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6295 ; accuracy: 0.74; loss: 2.195842981338501\n",
      "Training epoch 6296 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6296 ; accuracy: 0.74; loss: 2.195887565612793\n",
      "Training epoch 6297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6297 ; accuracy: 0.74; loss: 2.1959354877471924\n",
      "Training epoch 6298 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6298 ; accuracy: 0.74; loss: 2.1959691047668457\n",
      "Training epoch 6299 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6299 ; accuracy: 0.74; loss: 2.1960067749023438\n",
      "Training epoch 6300 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6300 ; accuracy: 0.74; loss: 2.196038007736206\n",
      "Training epoch 6301 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6301 ; accuracy: 0.74; loss: 2.196075677871704\n",
      "Training epoch 6302 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6302 ; accuracy: 0.74; loss: 2.1961145401000977\n",
      "Training epoch 6303 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6303 ; accuracy: 0.74; loss: 2.196159601211548\n",
      "Training epoch 6304 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6304 ; accuracy: 0.74; loss: 2.196201801300049\n",
      "Training epoch 6305 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6305 ; accuracy: 0.74; loss: 2.1962428092956543\n",
      "Training epoch 6306 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6306 ; accuracy: 0.74; loss: 2.1962809562683105\n",
      "Training epoch 6307 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6307 ; accuracy: 0.74; loss: 2.1963109970092773\n",
      "Training epoch 6308 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6308 ; accuracy: 0.74; loss: 2.1963422298431396\n",
      "Training epoch 6309 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6309 ; accuracy: 0.74; loss: 2.196373462677002\n",
      "Training epoch 6310 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6310 ; accuracy: 0.74; loss: 2.1964035034179688\n",
      "Training epoch 6311 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6311 ; accuracy: 0.74; loss: 2.196429491043091\n",
      "Training epoch 6312 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6312 ; accuracy: 0.74; loss: 2.196476936340332\n",
      "Training epoch 6313 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6313 ; accuracy: 0.74; loss: 2.1965274810791016\n",
      "Training epoch 6314 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6314 ; accuracy: 0.74; loss: 2.1965816020965576\n",
      "Training epoch 6315 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6315 ; accuracy: 0.74; loss: 2.196643829345703\n",
      "Training epoch 6316 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6316 ; accuracy: 0.74; loss: 2.196708917617798\n",
      "Training epoch 6317 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6317 ; accuracy: 0.74; loss: 2.196772813796997\n",
      "Training epoch 6318 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6318 ; accuracy: 0.74; loss: 2.1968326568603516\n",
      "Training epoch 6319 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6319 ; accuracy: 0.74; loss: 2.196890354156494\n",
      "Training epoch 6320 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6320 ; accuracy: 0.74; loss: 2.1969528198242188\n",
      "Training epoch 6321 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6321 ; accuracy: 0.74; loss: 2.197009801864624\n",
      "Training epoch 6322 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6322 ; accuracy: 0.74; loss: 2.197063446044922\n",
      "Training epoch 6323 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6323 ; accuracy: 0.74; loss: 2.1971185207366943\n",
      "Training epoch 6324 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6324 ; accuracy: 0.74; loss: 2.1971685886383057\n",
      "Training epoch 6325 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6325 ; accuracy: 0.7366666666666667; loss: 2.1972239017486572\n",
      "Training epoch 6326 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6326 ; accuracy: 0.7366666666666667; loss: 2.197277069091797\n",
      "Training epoch 6327 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6327 ; accuracy: 0.7366666666666667; loss: 2.197307586669922\n",
      "Training epoch 6328 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6328 ; accuracy: 0.7366666666666667; loss: 2.1973323822021484\n",
      "Training epoch 6329 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6329 ; accuracy: 0.7366666666666667; loss: 2.1973559856414795\n",
      "Training epoch 6330 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6330 ; accuracy: 0.7366666666666667; loss: 2.1973705291748047\n",
      "Training epoch 6331 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6331 ; accuracy: 0.7366666666666667; loss: 2.1973941326141357\n",
      "Training epoch 6332 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6332 ; accuracy: 0.7366666666666667; loss: 2.1974174976348877\n",
      "Training epoch 6333 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6333 ; accuracy: 0.7366666666666667; loss: 2.1974399089813232\n",
      "Training epoch 6334 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6334 ; accuracy: 0.7366666666666667; loss: 2.197456121444702\n",
      "Training epoch 6335 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 6335 ; accuracy: 0.7366666666666667; loss: 2.197633981704712\n",
      "Training epoch 6336 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6336 ; accuracy: 0.7366666666666667; loss: 2.1977970600128174\n",
      "Training epoch 6337 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6337 ; accuracy: 0.7366666666666667; loss: 2.1979377269744873\n",
      "Training epoch 6338 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6338 ; accuracy: 0.7366666666666667; loss: 2.198070764541626\n",
      "Training epoch 6339 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6339 ; accuracy: 0.7366666666666667; loss: 2.198202610015869\n",
      "Training epoch 6340 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6340 ; accuracy: 0.7366666666666667; loss: 2.1983234882354736\n",
      "Training epoch 6341 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6341 ; accuracy: 0.7366666666666667; loss: 2.198438882827759\n",
      "Training epoch 6342 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6342 ; accuracy: 0.7366666666666667; loss: 2.1985466480255127\n",
      "Training epoch 6343 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6343 ; accuracy: 0.7366666666666667; loss: 2.1986441612243652\n",
      "Training epoch 6344 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6344 ; accuracy: 0.7366666666666667; loss: 2.198734760284424\n",
      "Training epoch 6345 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6345 ; accuracy: 0.7366666666666667; loss: 2.1988346576690674\n",
      "Training epoch 6346 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6346 ; accuracy: 0.7366666666666667; loss: 2.1989316940307617\n",
      "Training epoch 6347 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6347 ; accuracy: 0.7366666666666667; loss: 2.1990225315093994\n",
      "Training epoch 6348 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6348 ; accuracy: 0.7366666666666667; loss: 2.199106454849243\n",
      "Training epoch 6349 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6349 ; accuracy: 0.7366666666666667; loss: 2.199176073074341\n",
      "Training epoch 6350 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6350 ; accuracy: 0.7366666666666667; loss: 2.1991913318634033\n",
      "Training epoch 6351 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6351 ; accuracy: 0.7366666666666667; loss: 2.1992077827453613\n",
      "Training epoch 6352 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6352 ; accuracy: 0.7366666666666667; loss: 2.19920015335083\n",
      "Training epoch 6353 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6353 ; accuracy: 0.7366666666666667; loss: 2.199190616607666\n",
      "Training epoch 6354 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6354 ; accuracy: 0.7366666666666667; loss: 2.199183464050293\n",
      "Training epoch 6355 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6355 ; accuracy: 0.7366666666666667; loss: 2.1991779804229736\n",
      "Training epoch 6356 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6356 ; accuracy: 0.7366666666666667; loss: 2.1991803646087646\n",
      "Training epoch 6357 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6357 ; accuracy: 0.7366666666666667; loss: 2.199185848236084\n",
      "Training epoch 6358 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6358 ; accuracy: 0.7366666666666667; loss: 2.1991994380950928\n",
      "Training epoch 6359 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6359 ; accuracy: 0.7366666666666667; loss: 2.1992146968841553\n",
      "Training epoch 6360 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6360 ; accuracy: 0.7366666666666667; loss: 2.199234962463379\n",
      "Training epoch 6361 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6361 ; accuracy: 0.7366666666666667; loss: 2.199247121810913\n",
      "Training epoch 6362 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6362 ; accuracy: 0.7366666666666667; loss: 2.199254035949707\n",
      "Training epoch 6363 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6363 ; accuracy: 0.7366666666666667; loss: 2.1992568969726562\n",
      "Training epoch 6364 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6364 ; accuracy: 0.7366666666666667; loss: 2.1992602348327637\n",
      "Training epoch 6365 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6365 ; accuracy: 0.7366666666666667; loss: 2.199258327484131\n",
      "Training epoch 6366 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6366 ; accuracy: 0.7366666666666667; loss: 2.1992597579956055\n",
      "Training epoch 6367 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6367 ; accuracy: 0.7366666666666667; loss: 2.1992666721343994\n",
      "Training epoch 6368 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6368 ; accuracy: 0.7366666666666667; loss: 2.199273109436035\n",
      "Training epoch 6369 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6369 ; accuracy: 0.7366666666666667; loss: 2.1992874145507812\n",
      "Training epoch 6370 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6370 ; accuracy: 0.7366666666666667; loss: 2.19929575920105\n",
      "Training epoch 6371 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6371 ; accuracy: 0.7366666666666667; loss: 2.199298620223999\n",
      "Training epoch 6372 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6372 ; accuracy: 0.7366666666666667; loss: 2.199307680130005\n",
      "Training epoch 6373 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6373 ; accuracy: 0.7366666666666667; loss: 2.199314832687378\n",
      "Training epoch 6374 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6374 ; accuracy: 0.7366666666666667; loss: 2.1993138790130615\n",
      "Training epoch 6375 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6375 ; accuracy: 0.7366666666666667; loss: 2.1993167400360107\n",
      "Training epoch 6376 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6376 ; accuracy: 0.7366666666666667; loss: 2.199322462081909\n",
      "Training epoch 6377 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6377 ; accuracy: 0.7366666666666667; loss: 2.1993277072906494\n",
      "Training epoch 6378 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6378 ; accuracy: 0.7366666666666667; loss: 2.1993513107299805\n",
      "Training epoch 6379 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6379 ; accuracy: 0.7366666666666667; loss: 2.199376106262207\n",
      "Training epoch 6380 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6380 ; accuracy: 0.7366666666666667; loss: 2.1993930339813232\n",
      "Training epoch 6381 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6381 ; accuracy: 0.7366666666666667; loss: 2.199409008026123\n",
      "Training epoch 6382 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6382 ; accuracy: 0.7366666666666667; loss: 2.199425458908081\n",
      "Training epoch 6383 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6383 ; accuracy: 0.7366666666666667; loss: 2.1994469165802\n",
      "Training epoch 6384 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6384 ; accuracy: 0.7366666666666667; loss: 2.1994714736938477\n",
      "Training epoch 6385 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6385 ; accuracy: 0.7366666666666667; loss: 2.199490547180176\n",
      "Training epoch 6386 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6386 ; accuracy: 0.7366666666666667; loss: 2.19950795173645\n",
      "Training epoch 6387 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6387 ; accuracy: 0.7366666666666667; loss: 2.199504852294922\n",
      "Training epoch 6388 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6388 ; accuracy: 0.7366666666666667; loss: 2.1995015144348145\n",
      "Training epoch 6389 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6389 ; accuracy: 0.7366666666666667; loss: 2.1995058059692383\n",
      "Training epoch 6390 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6390 ; accuracy: 0.7366666666666667; loss: 2.1995131969451904\n",
      "Training epoch 6391 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6391 ; accuracy: 0.7366666666666667; loss: 2.199510335922241\n",
      "Training epoch 6392 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6392 ; accuracy: 0.7366666666666667; loss: 2.199512243270874\n",
      "Training epoch 6393 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6393 ; accuracy: 0.7366666666666667; loss: 2.19952654838562\n",
      "Training epoch 6394 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6394 ; accuracy: 0.7366666666666667; loss: 2.1995363235473633\n",
      "Training epoch 6395 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6395 ; accuracy: 0.7366666666666667; loss: 2.1995551586151123\n",
      "Training epoch 6396 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6396 ; accuracy: 0.7366666666666667; loss: 2.199572801589966\n",
      "Training epoch 6397 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6397 ; accuracy: 0.7366666666666667; loss: 2.1995856761932373\n",
      "Training epoch 6398 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6398 ; accuracy: 0.7366666666666667; loss: 2.199599504470825\n",
      "Training epoch 6399 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6399 ; accuracy: 0.7366666666666667; loss: 2.199615240097046\n",
      "Training epoch 6400 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6400 ; accuracy: 0.7366666666666667; loss: 2.1996335983276367\n",
      "Training epoch 6401 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6401 ; accuracy: 0.7366666666666667; loss: 2.199655055999756\n",
      "Training epoch 6402 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6402 ; accuracy: 0.7366666666666667; loss: 2.19966983795166\n",
      "Training epoch 6403 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6403 ; accuracy: 0.7366666666666667; loss: 2.1996943950653076\n",
      "Training epoch 6404 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6404 ; accuracy: 0.7366666666666667; loss: 2.199705123901367\n",
      "Training epoch 6405 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6405 ; accuracy: 0.7366666666666667; loss: 2.1997106075286865\n",
      "Training epoch 6406 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6406 ; accuracy: 0.7366666666666667; loss: 2.1997318267822266\n",
      "Training epoch 6407 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6407 ; accuracy: 0.7366666666666667; loss: 2.199749231338501\n",
      "Training epoch 6408 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6408 ; accuracy: 0.7366666666666667; loss: 2.1997687816619873\n",
      "Training epoch 6409 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6409 ; accuracy: 0.7366666666666667; loss: 2.199784755706787\n",
      "Training epoch 6410 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6410 ; accuracy: 0.7366666666666667; loss: 2.1998038291931152\n",
      "Training epoch 6411 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 6411 ; accuracy: 0.7366666666666667; loss: 2.1999971866607666\n",
      "Training epoch 6412 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6412 ; accuracy: 0.7366666666666667; loss: 2.2001683712005615\n",
      "Training epoch 6413 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6413 ; accuracy: 0.7366666666666667; loss: 2.200326919555664\n",
      "Training epoch 6414 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6414 ; accuracy: 0.7366666666666667; loss: 2.2004733085632324\n",
      "Training epoch 6415 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6415 ; accuracy: 0.7366666666666667; loss: 2.2006094455718994\n",
      "Training epoch 6416 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6416 ; accuracy: 0.7366666666666667; loss: 2.200732946395874\n",
      "Training epoch 6417 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6417 ; accuracy: 0.7366666666666667; loss: 2.2008557319641113\n",
      "Training epoch 6418 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6418 ; accuracy: 0.7366666666666667; loss: 2.2009618282318115\n",
      "Training epoch 6419 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6419 ; accuracy: 0.7366666666666667; loss: 2.2010719776153564\n",
      "Training epoch 6420 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6420 ; accuracy: 0.7366666666666667; loss: 2.201155185699463\n",
      "Training epoch 6421 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6421 ; accuracy: 0.7366666666666667; loss: 2.2012336254119873\n",
      "Training epoch 6422 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6422 ; accuracy: 0.7366666666666667; loss: 2.2013001441955566\n",
      "Training epoch 6423 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6423 ; accuracy: 0.7366666666666667; loss: 2.2013590335845947\n",
      "Training epoch 6424 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6424 ; accuracy: 0.7366666666666667; loss: 2.2014102935791016\n",
      "Training epoch 6425 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6425 ; accuracy: 0.7366666666666667; loss: 2.201460599899292\n",
      "Training epoch 6426 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6426 ; accuracy: 0.7366666666666667; loss: 2.201514482498169\n",
      "Training epoch 6427 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6427 ; accuracy: 0.7366666666666667; loss: 2.2015624046325684\n",
      "Training epoch 6428 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6428 ; accuracy: 0.7366666666666667; loss: 2.201610803604126\n",
      "Training epoch 6429 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6429 ; accuracy: 0.7366666666666667; loss: 2.201652765274048\n",
      "Training epoch 6430 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6430 ; accuracy: 0.7366666666666667; loss: 2.201699733734131\n",
      "Training epoch 6431 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6431 ; accuracy: 0.7366666666666667; loss: 2.201746702194214\n",
      "Training epoch 6432 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6432 ; accuracy: 0.7366666666666667; loss: 2.2017922401428223\n",
      "Training epoch 6433 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6433 ; accuracy: 0.7366666666666667; loss: 2.2018284797668457\n",
      "Training epoch 6434 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6434 ; accuracy: 0.7366666666666667; loss: 2.201869249343872\n",
      "Training epoch 6435 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6435 ; accuracy: 0.7366666666666667; loss: 2.2019312381744385\n",
      "Training epoch 6436 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 6436 ; accuracy: 0.7366666666666667; loss: 2.2020108699798584\n",
      "Training epoch 6437 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6437 ; accuracy: 0.7366666666666667; loss: 2.2020938396453857\n",
      "Training epoch 6438 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6438 ; accuracy: 0.7366666666666667; loss: 2.2021565437316895\n",
      "Training epoch 6439 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6439 ; accuracy: 0.7366666666666667; loss: 2.2022154331207275\n",
      "Training epoch 6440 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6440 ; accuracy: 0.7366666666666667; loss: 2.2022693157196045\n",
      "Training epoch 6441 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6441 ; accuracy: 0.7366666666666667; loss: 2.202322483062744\n",
      "Training epoch 6442 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6442 ; accuracy: 0.7366666666666667; loss: 2.202378749847412\n",
      "Training epoch 6443 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6443 ; accuracy: 0.7366666666666667; loss: 2.202430486679077\n",
      "Training epoch 6444 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6444 ; accuracy: 0.7366666666666667; loss: 2.202455520629883\n",
      "Training epoch 6445 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6445 ; accuracy: 0.7366666666666667; loss: 2.2024757862091064\n",
      "Training epoch 6446 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6446 ; accuracy: 0.7366666666666667; loss: 2.202509641647339\n",
      "Training epoch 6447 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6447 ; accuracy: 0.7366666666666667; loss: 2.202547311782837\n",
      "Training epoch 6448 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6448 ; accuracy: 0.7366666666666667; loss: 2.202591896057129\n",
      "Training epoch 6449 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6449 ; accuracy: 0.7366666666666667; loss: 2.202643394470215\n",
      "Training epoch 6450 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6450 ; accuracy: 0.7366666666666667; loss: 2.2026872634887695\n",
      "Training epoch 6451 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6451 ; accuracy: 0.7366666666666667; loss: 2.202730894088745\n",
      "Training epoch 6452 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6452 ; accuracy: 0.7366666666666667; loss: 2.202799081802368\n",
      "Training epoch 6453 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6453 ; accuracy: 0.7366666666666667; loss: 2.202871322631836\n",
      "Training epoch 6454 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6454 ; accuracy: 0.7366666666666667; loss: 2.202937126159668\n",
      "Training epoch 6455 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6455 ; accuracy: 0.7366666666666667; loss: 2.202998161315918\n",
      "Training epoch 6456 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6456 ; accuracy: 0.7366666666666667; loss: 2.203051805496216\n",
      "Training epoch 6457 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6457 ; accuracy: 0.7366666666666667; loss: 2.203099012374878\n",
      "Training epoch 6458 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6458 ; accuracy: 0.7366666666666667; loss: 2.203139543533325\n",
      "Training epoch 6459 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6459 ; accuracy: 0.7366666666666667; loss: 2.2031774520874023\n",
      "Training epoch 6460 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6460 ; accuracy: 0.7366666666666667; loss: 2.2031972408294678\n",
      "Training epoch 6461 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6461 ; accuracy: 0.7366666666666667; loss: 2.2032008171081543\n",
      "Training epoch 6462 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6462 ; accuracy: 0.7366666666666667; loss: 2.2032134532928467\n",
      "Training epoch 6463 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6463 ; accuracy: 0.7366666666666667; loss: 2.2032227516174316\n",
      "Training epoch 6464 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6464 ; accuracy: 0.7366666666666667; loss: 2.2031965255737305\n",
      "Training epoch 6465 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6465 ; accuracy: 0.7366666666666667; loss: 2.203171730041504\n",
      "Training epoch 6466 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6466 ; accuracy: 0.7366666666666667; loss: 2.203144073486328\n",
      "Training epoch 6467 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6467 ; accuracy: 0.7366666666666667; loss: 2.203120708465576\n",
      "Training epoch 6468 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6468 ; accuracy: 0.7366666666666667; loss: 2.2031006813049316\n",
      "Training epoch 6469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6469 ; accuracy: 0.7366666666666667; loss: 2.2030892372131348\n",
      "Training epoch 6470 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6470 ; accuracy: 0.7366666666666667; loss: 2.2030766010284424\n",
      "Training epoch 6471 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6471 ; accuracy: 0.7366666666666667; loss: 2.2030720710754395\n",
      "Training epoch 6472 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6472 ; accuracy: 0.7366666666666667; loss: 2.2030675411224365\n",
      "Training epoch 6473 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6473 ; accuracy: 0.7366666666666667; loss: 2.2030680179595947\n",
      "Training epoch 6474 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6474 ; accuracy: 0.7366666666666667; loss: 2.2030646800994873\n",
      "Training epoch 6475 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6475 ; accuracy: 0.7366666666666667; loss: 2.2030599117279053\n",
      "Training epoch 6476 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6476 ; accuracy: 0.7366666666666667; loss: 2.2030584812164307\n",
      "Training epoch 6477 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6477 ; accuracy: 0.7366666666666667; loss: 2.2030563354492188\n",
      "Training epoch 6478 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6478 ; accuracy: 0.7366666666666667; loss: 2.203054904937744\n",
      "Training epoch 6479 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6479 ; accuracy: 0.7366666666666667; loss: 2.2030506134033203\n",
      "Training epoch 6480 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6480 ; accuracy: 0.7366666666666667; loss: 2.2030599117279053\n",
      "Training epoch 6481 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6481 ; accuracy: 0.7366666666666667; loss: 2.203073263168335\n",
      "Training epoch 6482 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6482 ; accuracy: 0.7366666666666667; loss: 2.203090190887451\n",
      "Training epoch 6483 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6483 ; accuracy: 0.7366666666666667; loss: 2.2031052112579346\n",
      "Training epoch 6484 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6484 ; accuracy: 0.7366666666666667; loss: 2.203129768371582\n",
      "Training epoch 6485 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6485 ; accuracy: 0.7366666666666667; loss: 2.2031586170196533\n",
      "Training epoch 6486 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6486 ; accuracy: 0.7366666666666667; loss: 2.2031869888305664\n",
      "Training epoch 6487 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6487 ; accuracy: 0.7366666666666667; loss: 2.203216552734375\n",
      "Training epoch 6488 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6488 ; accuracy: 0.7366666666666667; loss: 2.2032523155212402\n",
      "Training epoch 6489 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6489 ; accuracy: 0.7366666666666667; loss: 2.2032883167266846\n",
      "Training epoch 6490 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6490 ; accuracy: 0.7366666666666667; loss: 2.203324794769287\n",
      "Training epoch 6491 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6491 ; accuracy: 0.7366666666666667; loss: 2.2033636569976807\n",
      "Training epoch 6492 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6492 ; accuracy: 0.7366666666666667; loss: 2.203406810760498\n",
      "Training epoch 6493 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6493 ; accuracy: 0.7366666666666667; loss: 2.203441858291626\n",
      "Training epoch 6494 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6494 ; accuracy: 0.7366666666666667; loss: 2.203481435775757\n",
      "Training epoch 6495 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6495 ; accuracy: 0.7366666666666667; loss: 2.203521490097046\n",
      "Training epoch 6496 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6496 ; accuracy: 0.7366666666666667; loss: 2.2035558223724365\n",
      "Training epoch 6497 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6497 ; accuracy: 0.7366666666666667; loss: 2.2035837173461914\n",
      "Training epoch 6498 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6498 ; accuracy: 0.7366666666666667; loss: 2.2036094665527344\n",
      "Training epoch 6499 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6499 ; accuracy: 0.7366666666666667; loss: 2.2036259174346924\n",
      "Training epoch 6500 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6500 ; accuracy: 0.7366666666666667; loss: 2.203648567199707\n",
      "Training epoch 6501 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6501 ; accuracy: 0.7366666666666667; loss: 2.203690767288208\n",
      "Training epoch 6502 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6502 ; accuracy: 0.7366666666666667; loss: 2.203728437423706\n",
      "Training epoch 6503 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6503 ; accuracy: 0.7366666666666667; loss: 2.203758716583252\n",
      "Training epoch 6504 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6504 ; accuracy: 0.7366666666666667; loss: 2.203784227371216\n",
      "Training epoch 6505 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6505 ; accuracy: 0.7366666666666667; loss: 2.2038075923919678\n",
      "Training epoch 6506 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6506 ; accuracy: 0.7366666666666667; loss: 2.2038233280181885\n",
      "Training epoch 6507 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6507 ; accuracy: 0.7366666666666667; loss: 2.2038509845733643\n",
      "Training epoch 6508 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6508 ; accuracy: 0.7366666666666667; loss: 2.2038795948028564\n",
      "Training epoch 6509 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6509 ; accuracy: 0.7366666666666667; loss: 2.203897714614868\n",
      "Training epoch 6510 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6510 ; accuracy: 0.7366666666666667; loss: 2.203918218612671\n",
      "Training epoch 6511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6511 ; accuracy: 0.7366666666666667; loss: 2.203939437866211\n",
      "Training epoch 6512 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6512 ; accuracy: 0.7366666666666667; loss: 2.20396089553833\n",
      "Training epoch 6513 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6513 ; accuracy: 0.7366666666666667; loss: 2.203983783721924\n",
      "Training epoch 6514 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6514 ; accuracy: 0.7366666666666667; loss: 2.2040185928344727\n",
      "Training epoch 6515 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6515 ; accuracy: 0.7366666666666667; loss: 2.204052448272705\n",
      "Training epoch 6516 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6516 ; accuracy: 0.7366666666666667; loss: 2.204080104827881\n",
      "Training epoch 6517 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6517 ; accuracy: 0.7366666666666667; loss: 2.2041027545928955\n",
      "Training epoch 6518 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6518 ; accuracy: 0.7366666666666667; loss: 2.204118490219116\n",
      "Training epoch 6519 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6519 ; accuracy: 0.7366666666666667; loss: 2.204132080078125\n",
      "Training epoch 6520 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6520 ; accuracy: 0.7366666666666667; loss: 2.2041497230529785\n",
      "Training epoch 6521 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6521 ; accuracy: 0.7366666666666667; loss: 2.2041664123535156\n",
      "Training epoch 6522 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6522 ; accuracy: 0.7366666666666667; loss: 2.2041854858398438\n",
      "Training epoch 6523 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6523 ; accuracy: 0.7366666666666667; loss: 2.20420241355896\n",
      "Training epoch 6524 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6524 ; accuracy: 0.7366666666666667; loss: 2.2042236328125\n",
      "Training epoch 6525 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6525 ; accuracy: 0.7366666666666667; loss: 2.2042500972747803\n",
      "Training epoch 6526 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6526 ; accuracy: 0.7366666666666667; loss: 2.2042675018310547\n",
      "Training epoch 6527 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6527 ; accuracy: 0.7366666666666667; loss: 2.204284429550171\n",
      "Training epoch 6528 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6528 ; accuracy: 0.7366666666666667; loss: 2.2042930126190186\n",
      "Training epoch 6529 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6529 ; accuracy: 0.7366666666666667; loss: 2.2043046951293945\n",
      "Training epoch 6530 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6530 ; accuracy: 0.7366666666666667; loss: 2.2043168544769287\n",
      "Training epoch 6531 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6531 ; accuracy: 0.7366666666666667; loss: 2.204324960708618\n",
      "Training epoch 6532 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6532 ; accuracy: 0.7366666666666667; loss: 2.2043325901031494\n",
      "Training epoch 6533 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6533 ; accuracy: 0.7366666666666667; loss: 2.204340934753418\n",
      "Training epoch 6534 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6534 ; accuracy: 0.7366666666666667; loss: 2.204350709915161\n",
      "Training epoch 6535 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6535 ; accuracy: 0.7366666666666667; loss: 2.204354763031006\n",
      "Training epoch 6536 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6536 ; accuracy: 0.7366666666666667; loss: 2.2043609619140625\n",
      "Training epoch 6537 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6537 ; accuracy: 0.7366666666666667; loss: 2.2043726444244385\n",
      "Training epoch 6538 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6538 ; accuracy: 0.7366666666666667; loss: 2.204374313354492\n",
      "Training epoch 6539 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6539 ; accuracy: 0.7366666666666667; loss: 2.204378843307495\n",
      "Training epoch 6540 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6540 ; accuracy: 0.7366666666666667; loss: 2.2043800354003906\n",
      "Training epoch 6541 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6541 ; accuracy: 0.7366666666666667; loss: 2.2043745517730713\n",
      "Training epoch 6542 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6542 ; accuracy: 0.7366666666666667; loss: 2.2043657302856445\n",
      "Training epoch 6543 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6543 ; accuracy: 0.7366666666666667; loss: 2.2043631076812744\n",
      "Training epoch 6544 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6544 ; accuracy: 0.7366666666666667; loss: 2.2043635845184326\n",
      "Training epoch 6545 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6545 ; accuracy: 0.7366666666666667; loss: 2.204364061355591\n",
      "Training epoch 6546 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6546 ; accuracy: 0.7366666666666667; loss: 2.2043662071228027\n",
      "Training epoch 6547 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6547 ; accuracy: 0.7366666666666667; loss: 2.204380989074707\n",
      "Training epoch 6548 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6548 ; accuracy: 0.7366666666666667; loss: 2.204399824142456\n",
      "Training epoch 6549 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6549 ; accuracy: 0.7366666666666667; loss: 2.2044167518615723\n",
      "Training epoch 6550 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6550 ; accuracy: 0.7366666666666667; loss: 2.2044360637664795\n",
      "Training epoch 6551 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6551 ; accuracy: 0.7366666666666667; loss: 2.2044546604156494\n",
      "Training epoch 6552 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6552 ; accuracy: 0.7366666666666667; loss: 2.2044599056243896\n",
      "Training epoch 6553 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6553 ; accuracy: 0.7366666666666667; loss: 2.2044618129730225\n",
      "Training epoch 6554 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6554 ; accuracy: 0.7366666666666667; loss: 2.2044711112976074\n",
      "Training epoch 6555 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6555 ; accuracy: 0.7366666666666667; loss: 2.204479932785034\n",
      "Training epoch 6556 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6556 ; accuracy: 0.7366666666666667; loss: 2.20448637008667\n",
      "Training epoch 6557 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6557 ; accuracy: 0.7366666666666667; loss: 2.2044966220855713\n",
      "Training epoch 6558 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6558 ; accuracy: 0.7366666666666667; loss: 2.2045066356658936\n",
      "Training epoch 6559 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6559 ; accuracy: 0.7366666666666667; loss: 2.204517126083374\n",
      "Training epoch 6560 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6560 ; accuracy: 0.7366666666666667; loss: 2.204529285430908\n",
      "Training epoch 6561 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6561 ; accuracy: 0.7366666666666667; loss: 2.2045369148254395\n",
      "Training epoch 6562 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6562 ; accuracy: 0.7366666666666667; loss: 2.204559564590454\n",
      "Training epoch 6563 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6563 ; accuracy: 0.7366666666666667; loss: 2.2045884132385254\n",
      "Training epoch 6564 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6564 ; accuracy: 0.7366666666666667; loss: 2.2046139240264893\n",
      "Training epoch 6565 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6565 ; accuracy: 0.7366666666666667; loss: 2.204637050628662\n",
      "Training epoch 6566 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6566 ; accuracy: 0.7366666666666667; loss: 2.2046585083007812\n",
      "Training epoch 6567 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6567 ; accuracy: 0.7366666666666667; loss: 2.204680919647217\n",
      "Training epoch 6568 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6568 ; accuracy: 0.7366666666666667; loss: 2.2047119140625\n",
      "Training epoch 6569 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6569 ; accuracy: 0.7366666666666667; loss: 2.204739570617676\n",
      "Training epoch 6570 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6570 ; accuracy: 0.7366666666666667; loss: 2.2047669887542725\n",
      "Training epoch 6571 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6571 ; accuracy: 0.7366666666666667; loss: 2.2047948837280273\n",
      "Training epoch 6572 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6572 ; accuracy: 0.7366666666666667; loss: 2.2048227787017822\n",
      "Training epoch 6573 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6573 ; accuracy: 0.7366666666666667; loss: 2.204848527908325\n",
      "Training epoch 6574 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6574 ; accuracy: 0.7366666666666667; loss: 2.204875946044922\n",
      "Training epoch 6575 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6575 ; accuracy: 0.7366666666666667; loss: 2.204899311065674\n",
      "Training epoch 6576 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6576 ; accuracy: 0.7366666666666667; loss: 2.2049217224121094\n",
      "Training epoch 6577 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6577 ; accuracy: 0.7366666666666667; loss: 2.2049436569213867\n",
      "Training epoch 6578 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6578 ; accuracy: 0.7366666666666667; loss: 2.2049596309661865\n",
      "Training epoch 6579 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6579 ; accuracy: 0.7366666666666667; loss: 2.20497989654541\n",
      "Training epoch 6580 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6580 ; accuracy: 0.7366666666666667; loss: 2.2049922943115234\n",
      "Training epoch 6581 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6581 ; accuracy: 0.7366666666666667; loss: 2.2049996852874756\n",
      "Training epoch 6582 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6582 ; accuracy: 0.7366666666666667; loss: 2.2050065994262695\n",
      "Training epoch 6583 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6583 ; accuracy: 0.7366666666666667; loss: 2.2050037384033203\n",
      "Training epoch 6584 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6584 ; accuracy: 0.7366666666666667; loss: 2.205002546310425\n",
      "Training epoch 6585 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6585 ; accuracy: 0.7366666666666667; loss: 2.2050058841705322\n",
      "Training epoch 6586 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6586 ; accuracy: 0.7366666666666667; loss: 2.2050232887268066\n",
      "Training epoch 6587 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6587 ; accuracy: 0.7366666666666667; loss: 2.205026865005493\n",
      "Training epoch 6588 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6588 ; accuracy: 0.7366666666666667; loss: 2.205028772354126\n",
      "Training epoch 6589 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6589 ; accuracy: 0.7366666666666667; loss: 2.2050278186798096\n",
      "Training epoch 6590 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6590 ; accuracy: 0.7366666666666667; loss: 2.2050259113311768\n",
      "Training epoch 6591 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6591 ; accuracy: 0.7366666666666667; loss: 2.205030918121338\n",
      "Training epoch 6592 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6592 ; accuracy: 0.7366666666666667; loss: 2.205035924911499\n",
      "Training epoch 6593 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6593 ; accuracy: 0.7366666666666667; loss: 2.205040216445923\n",
      "Training epoch 6594 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6594 ; accuracy: 0.7366666666666667; loss: 2.2050528526306152\n",
      "Training epoch 6595 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6595 ; accuracy: 0.7366666666666667; loss: 2.2050623893737793\n",
      "Training epoch 6596 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6596 ; accuracy: 0.7366666666666667; loss: 2.2050743103027344\n",
      "Training epoch 6597 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6597 ; accuracy: 0.7366666666666667; loss: 2.205080509185791\n",
      "Training epoch 6598 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6598 ; accuracy: 0.7366666666666667; loss: 2.205085277557373\n",
      "Training epoch 6599 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6599 ; accuracy: 0.7366666666666667; loss: 2.2050986289978027\n",
      "Training epoch 6600 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6600 ; accuracy: 0.7366666666666667; loss: 2.2051239013671875\n",
      "Training epoch 6601 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6601 ; accuracy: 0.7366666666666667; loss: 2.2051424980163574\n",
      "Training epoch 6602 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6602 ; accuracy: 0.7366666666666667; loss: 2.2051639556884766\n",
      "Training epoch 6603 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6603 ; accuracy: 0.7366666666666667; loss: 2.2051925659179688\n",
      "Training epoch 6604 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6604 ; accuracy: 0.7366666666666667; loss: 2.205216884613037\n",
      "Training epoch 6605 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6605 ; accuracy: 0.7366666666666667; loss: 2.2052414417266846\n",
      "Training epoch 6606 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6606 ; accuracy: 0.7366666666666667; loss: 2.2052810192108154\n",
      "Training epoch 6607 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6607 ; accuracy: 0.7366666666666667; loss: 2.205315351486206\n",
      "Training epoch 6608 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6608 ; accuracy: 0.7366666666666667; loss: 2.2053475379943848\n",
      "Training epoch 6609 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6609 ; accuracy: 0.7366666666666667; loss: 2.2054052352905273\n",
      "Training epoch 6610 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6610 ; accuracy: 0.7366666666666667; loss: 2.205458879470825\n",
      "Training epoch 6611 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6611 ; accuracy: 0.7366666666666667; loss: 2.205505847930908\n",
      "Training epoch 6612 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6612 ; accuracy: 0.7366666666666667; loss: 2.205544948577881\n",
      "Training epoch 6613 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6613 ; accuracy: 0.7366666666666667; loss: 2.2055795192718506\n",
      "Training epoch 6614 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6614 ; accuracy: 0.7366666666666667; loss: 2.2056190967559814\n",
      "Training epoch 6615 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6615 ; accuracy: 0.7366666666666667; loss: 2.2056493759155273\n",
      "Training epoch 6616 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6616 ; accuracy: 0.7366666666666667; loss: 2.2056851387023926\n",
      "Training epoch 6617 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6617 ; accuracy: 0.7366666666666667; loss: 2.205712080001831\n",
      "Training epoch 6618 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6618 ; accuracy: 0.7366666666666667; loss: 2.2057507038116455\n",
      "Training epoch 6619 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6619 ; accuracy: 0.7366666666666667; loss: 2.205789089202881\n",
      "Training epoch 6620 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6620 ; accuracy: 0.7366666666666667; loss: 2.205824136734009\n",
      "Training epoch 6621 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6621 ; accuracy: 0.7366666666666667; loss: 2.2058606147766113\n",
      "Training epoch 6622 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6622 ; accuracy: 0.7366666666666667; loss: 2.2058918476104736\n",
      "Training epoch 6623 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6623 ; accuracy: 0.7366666666666667; loss: 2.2059240341186523\n",
      "Training epoch 6624 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6624 ; accuracy: 0.7366666666666667; loss: 2.2059507369995117\n",
      "Training epoch 6625 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6625 ; accuracy: 0.7366666666666667; loss: 2.2059831619262695\n",
      "Training epoch 6626 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6626 ; accuracy: 0.7366666666666667; loss: 2.2060022354125977\n",
      "Training epoch 6627 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6627 ; accuracy: 0.7366666666666667; loss: 2.206022024154663\n",
      "Training epoch 6628 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6628 ; accuracy: 0.7366666666666667; loss: 2.2060444355010986\n",
      "Training epoch 6629 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6629 ; accuracy: 0.7366666666666667; loss: 2.2060699462890625\n",
      "Training epoch 6630 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6630 ; accuracy: 0.7366666666666667; loss: 2.2060956954956055\n",
      "Training epoch 6631 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6631 ; accuracy: 0.7366666666666667; loss: 2.206132173538208\n",
      "Training epoch 6632 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6632 ; accuracy: 0.7366666666666667; loss: 2.206162691116333\n",
      "Training epoch 6633 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6633 ; accuracy: 0.7366666666666667; loss: 2.206167221069336\n",
      "Training epoch 6634 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6634 ; accuracy: 0.7366666666666667; loss: 2.206183910369873\n",
      "Training epoch 6635 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 6635 ; accuracy: 0.7366666666666667; loss: 2.2062458992004395\n",
      "Training epoch 6636 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6636 ; accuracy: 0.7366666666666667; loss: 2.206305503845215\n",
      "Training epoch 6637 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6637 ; accuracy: 0.7366666666666667; loss: 2.2063591480255127\n",
      "Training epoch 6638 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6638 ; accuracy: 0.7366666666666667; loss: 2.206439256668091\n",
      "Training epoch 6639 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6639 ; accuracy: 0.7366666666666667; loss: 2.206510543823242\n",
      "Training epoch 6640 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6640 ; accuracy: 0.7366666666666667; loss: 2.2065751552581787\n",
      "Training epoch 6641 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6641 ; accuracy: 0.7366666666666667; loss: 2.2066378593444824\n",
      "Training epoch 6642 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6642 ; accuracy: 0.7366666666666667; loss: 2.206681251525879\n",
      "Training epoch 6643 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6643 ; accuracy: 0.7366666666666667; loss: 2.2067296504974365\n",
      "Training epoch 6644 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6644 ; accuracy: 0.7366666666666667; loss: 2.206780433654785\n",
      "Training epoch 6645 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6645 ; accuracy: 0.7366666666666667; loss: 2.206833600997925\n",
      "Training epoch 6646 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6646 ; accuracy: 0.7366666666666667; loss: 2.2068846225738525\n",
      "Training epoch 6647 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6647 ; accuracy: 0.7366666666666667; loss: 2.2069315910339355\n",
      "Training epoch 6648 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6648 ; accuracy: 0.7366666666666667; loss: 2.2069833278656006\n",
      "Training epoch 6649 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6649 ; accuracy: 0.7366666666666667; loss: 2.2070374488830566\n",
      "Training epoch 6650 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6650 ; accuracy: 0.7366666666666667; loss: 2.207084894180298\n",
      "Training epoch 6651 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6651 ; accuracy: 0.7366666666666667; loss: 2.2071311473846436\n",
      "Training epoch 6652 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6652 ; accuracy: 0.7366666666666667; loss: 2.207176685333252\n",
      "Training epoch 6653 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6653 ; accuracy: 0.7366666666666667; loss: 2.20723032951355\n",
      "Training epoch 6654 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6654 ; accuracy: 0.7366666666666667; loss: 2.2072792053222656\n",
      "Training epoch 6655 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6655 ; accuracy: 0.7366666666666667; loss: 2.2073309421539307\n",
      "Training epoch 6656 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6656 ; accuracy: 0.7366666666666667; loss: 2.2073774337768555\n",
      "Training epoch 6657 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6657 ; accuracy: 0.7366666666666667; loss: 2.2074224948883057\n",
      "Training epoch 6658 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6658 ; accuracy: 0.7366666666666667; loss: 2.2074828147888184\n",
      "Training epoch 6659 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6659 ; accuracy: 0.7366666666666667; loss: 2.207540988922119\n",
      "Training epoch 6660 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6660 ; accuracy: 0.7366666666666667; loss: 2.2075955867767334\n",
      "Training epoch 6661 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6661 ; accuracy: 0.7366666666666667; loss: 2.207638740539551\n",
      "Training epoch 6662 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6662 ; accuracy: 0.7366666666666667; loss: 2.2076845169067383\n",
      "Training epoch 6663 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6663 ; accuracy: 0.7366666666666667; loss: 2.207726001739502\n",
      "Training epoch 6664 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6664 ; accuracy: 0.7366666666666667; loss: 2.207763195037842\n",
      "Training epoch 6665 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6665 ; accuracy: 0.7366666666666667; loss: 2.2077977657318115\n",
      "Training epoch 6666 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6666 ; accuracy: 0.7366666666666667; loss: 2.2078351974487305\n",
      "Training epoch 6667 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6667 ; accuracy: 0.7366666666666667; loss: 2.2078638076782227\n",
      "Training epoch 6668 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6668 ; accuracy: 0.7366666666666667; loss: 2.2078821659088135\n",
      "Training epoch 6669 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6669 ; accuracy: 0.7366666666666667; loss: 2.2079060077667236\n",
      "Training epoch 6670 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6670 ; accuracy: 0.7366666666666667; loss: 2.2079315185546875\n",
      "Training epoch 6671 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6671 ; accuracy: 0.7366666666666667; loss: 2.2079575061798096\n",
      "Training epoch 6672 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6672 ; accuracy: 0.7366666666666667; loss: 2.207969903945923\n",
      "Training epoch 6673 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6673 ; accuracy: 0.7366666666666667; loss: 2.207989454269409\n",
      "Training epoch 6674 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6674 ; accuracy: 0.7366666666666667; loss: 2.208010673522949\n",
      "Training epoch 6675 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6675 ; accuracy: 0.7366666666666667; loss: 2.2079973220825195\n",
      "Training epoch 6676 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6676 ; accuracy: 0.7366666666666667; loss: 2.2079885005950928\n",
      "Training epoch 6677 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6677 ; accuracy: 0.7366666666666667; loss: 2.2079861164093018\n",
      "Training epoch 6678 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6678 ; accuracy: 0.7366666666666667; loss: 2.2079763412475586\n",
      "Training epoch 6679 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6679 ; accuracy: 0.7366666666666667; loss: 2.207955837249756\n",
      "Training epoch 6680 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6680 ; accuracy: 0.7366666666666667; loss: 2.2079274654388428\n",
      "Training epoch 6681 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6681 ; accuracy: 0.7366666666666667; loss: 2.207902669906616\n",
      "Training epoch 6682 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6682 ; accuracy: 0.7366666666666667; loss: 2.2078747749328613\n",
      "Training epoch 6683 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6683 ; accuracy: 0.7366666666666667; loss: 2.2078537940979004\n",
      "Training epoch 6684 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6684 ; accuracy: 0.7366666666666667; loss: 2.207839012145996\n",
      "Training epoch 6685 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6685 ; accuracy: 0.7366666666666667; loss: 2.2078285217285156\n",
      "Training epoch 6686 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6686 ; accuracy: 0.7366666666666667; loss: 2.207819938659668\n",
      "Training epoch 6687 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6687 ; accuracy: 0.7366666666666667; loss: 2.2078239917755127\n",
      "Training epoch 6688 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6688 ; accuracy: 0.7366666666666667; loss: 2.207825183868408\n",
      "Training epoch 6689 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6689 ; accuracy: 0.7366666666666667; loss: 2.2078449726104736\n",
      "Training epoch 6690 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6690 ; accuracy: 0.7366666666666667; loss: 2.2078680992126465\n",
      "Training epoch 6691 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6691 ; accuracy: 0.7366666666666667; loss: 2.2078874111175537\n",
      "Training epoch 6692 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6692 ; accuracy: 0.7366666666666667; loss: 2.207914113998413\n",
      "Training epoch 6693 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6693 ; accuracy: 0.7366666666666667; loss: 2.2079410552978516\n",
      "Training epoch 6694 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6694 ; accuracy: 0.7366666666666667; loss: 2.2079555988311768\n",
      "Training epoch 6695 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6695 ; accuracy: 0.7366666666666667; loss: 2.207972526550293\n",
      "Training epoch 6696 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6696 ; accuracy: 0.7366666666666667; loss: 2.207986831665039\n",
      "Training epoch 6697 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6697 ; accuracy: 0.7366666666666667; loss: 2.208012580871582\n",
      "Training epoch 6698 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6698 ; accuracy: 0.7366666666666667; loss: 2.208043098449707\n",
      "Training epoch 6699 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6699 ; accuracy: 0.7366666666666667; loss: 2.20807147026062\n",
      "Training epoch 6700 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6700 ; accuracy: 0.7366666666666667; loss: 2.2080941200256348\n",
      "Training epoch 6701 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6701 ; accuracy: 0.7366666666666667; loss: 2.2081174850463867\n",
      "Training epoch 6702 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6702 ; accuracy: 0.7366666666666667; loss: 2.2081339359283447\n",
      "Training epoch 6703 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6703 ; accuracy: 0.7366666666666667; loss: 2.2081515789031982\n",
      "Training epoch 6704 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6704 ; accuracy: 0.7366666666666667; loss: 2.2081665992736816\n",
      "Training epoch 6705 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6705 ; accuracy: 0.7366666666666667; loss: 2.208179473876953\n",
      "Training epoch 6706 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6706 ; accuracy: 0.7366666666666667; loss: 2.2081966400146484\n",
      "Training epoch 6707 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6707 ; accuracy: 0.7366666666666667; loss: 2.208220958709717\n",
      "Training epoch 6708 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6708 ; accuracy: 0.7366666666666667; loss: 2.2082440853118896\n",
      "Training epoch 6709 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6709 ; accuracy: 0.7366666666666667; loss: 2.2082760334014893\n",
      "Training epoch 6710 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6710 ; accuracy: 0.7366666666666667; loss: 2.2083065509796143\n",
      "Training epoch 6711 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6711 ; accuracy: 0.7366666666666667; loss: 2.208334445953369\n",
      "Training epoch 6712 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6712 ; accuracy: 0.7366666666666667; loss: 2.2083606719970703\n",
      "Training epoch 6713 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6713 ; accuracy: 0.7366666666666667; loss: 2.208388566970825\n",
      "Training epoch 6714 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6714 ; accuracy: 0.7366666666666667; loss: 2.2084147930145264\n",
      "Training epoch 6715 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6715 ; accuracy: 0.7366666666666667; loss: 2.2084357738494873\n",
      "Training epoch 6716 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6716 ; accuracy: 0.7366666666666667; loss: 2.2084543704986572\n",
      "Training epoch 6717 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6717 ; accuracy: 0.7366666666666667; loss: 2.2084712982177734\n",
      "Training epoch 6718 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6718 ; accuracy: 0.7366666666666667; loss: 2.208481550216675\n",
      "Training epoch 6719 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6719 ; accuracy: 0.7366666666666667; loss: 2.2085001468658447\n",
      "Training epoch 6720 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6720 ; accuracy: 0.7366666666666667; loss: 2.208519458770752\n",
      "Training epoch 6721 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6721 ; accuracy: 0.7366666666666667; loss: 2.208545684814453\n",
      "Training epoch 6722 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6722 ; accuracy: 0.7366666666666667; loss: 2.2085776329040527\n",
      "Training epoch 6723 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6723 ; accuracy: 0.7366666666666667; loss: 2.2086026668548584\n",
      "Training epoch 6724 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6724 ; accuracy: 0.7366666666666667; loss: 2.2086308002471924\n",
      "Training epoch 6725 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6725 ; accuracy: 0.7366666666666667; loss: 2.2086546421051025\n",
      "Training epoch 6726 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6726 ; accuracy: 0.7366666666666667; loss: 2.2086663246154785\n",
      "Training epoch 6727 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6727 ; accuracy: 0.7366666666666667; loss: 2.2086644172668457\n",
      "Training epoch 6728 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6728 ; accuracy: 0.7366666666666667; loss: 2.2086665630340576\n",
      "Training epoch 6729 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6729 ; accuracy: 0.7366666666666667; loss: 2.208658218383789\n",
      "Training epoch 6730 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6730 ; accuracy: 0.7366666666666667; loss: 2.208653450012207\n",
      "Training epoch 6731 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6731 ; accuracy: 0.7366666666666667; loss: 2.2086517810821533\n",
      "Training epoch 6732 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6732 ; accuracy: 0.7366666666666667; loss: 2.2086598873138428\n",
      "Training epoch 6733 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6733 ; accuracy: 0.7366666666666667; loss: 2.2086689472198486\n",
      "Training epoch 6734 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6734 ; accuracy: 0.7366666666666667; loss: 2.208676815032959\n",
      "Training epoch 6735 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6735 ; accuracy: 0.7366666666666667; loss: 2.208693027496338\n",
      "Training epoch 6736 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6736 ; accuracy: 0.7366666666666667; loss: 2.208717107772827\n",
      "Training epoch 6737 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6737 ; accuracy: 0.7366666666666667; loss: 2.2087297439575195\n",
      "Training epoch 6738 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6738 ; accuracy: 0.7366666666666667; loss: 2.2087507247924805\n",
      "Training epoch 6739 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6739 ; accuracy: 0.7366666666666667; loss: 2.2087745666503906\n",
      "Training epoch 6740 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6740 ; accuracy: 0.7366666666666667; loss: 2.208787441253662\n",
      "Training epoch 6741 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6741 ; accuracy: 0.74; loss: 2.208799362182617\n",
      "Training epoch 6742 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6742 ; accuracy: 0.74; loss: 2.208812713623047\n",
      "Training epoch 6743 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6743 ; accuracy: 0.74; loss: 2.2088241577148438\n",
      "Training epoch 6744 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6744 ; accuracy: 0.74; loss: 2.2088403701782227\n",
      "Training epoch 6745 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6745 ; accuracy: 0.74; loss: 2.208862781524658\n",
      "Training epoch 6746 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6746 ; accuracy: 0.74; loss: 2.208889961242676\n",
      "Training epoch 6747 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6747 ; accuracy: 0.74; loss: 2.2089171409606934\n",
      "Training epoch 6748 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6748 ; accuracy: 0.74; loss: 2.2089428901672363\n",
      "Training epoch 6749 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6749 ; accuracy: 0.74; loss: 2.208972692489624\n",
      "Training epoch 6750 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6750 ; accuracy: 0.74; loss: 2.20900559425354\n",
      "Training epoch 6751 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 6751 ; accuracy: 0.74; loss: 2.2091376781463623\n",
      "Training epoch 6752 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6752 ; accuracy: 0.7366666666666667; loss: 2.209261894226074\n",
      "Training epoch 6753 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6753 ; accuracy: 0.7366666666666667; loss: 2.2093818187713623\n",
      "Training epoch 6754 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6754 ; accuracy: 0.7366666666666667; loss: 2.2094900608062744\n",
      "Training epoch 6755 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6755 ; accuracy: 0.7366666666666667; loss: 2.209589958190918\n",
      "Training epoch 6756 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6756 ; accuracy: 0.7366666666666667; loss: 2.2096822261810303\n",
      "Training epoch 6757 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6757 ; accuracy: 0.7366666666666667; loss: 2.2097628116607666\n",
      "Training epoch 6758 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6758 ; accuracy: 0.7366666666666667; loss: 2.209829807281494\n",
      "Training epoch 6759 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6759 ; accuracy: 0.7366666666666667; loss: 2.2098946571350098\n",
      "Training epoch 6760 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6760 ; accuracy: 0.7366666666666667; loss: 2.2099640369415283\n",
      "Training epoch 6761 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6761 ; accuracy: 0.7366666666666667; loss: 2.2100517749786377\n",
      "Training epoch 6762 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6762 ; accuracy: 0.7366666666666667; loss: 2.2101306915283203\n",
      "Training epoch 6763 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6763 ; accuracy: 0.7366666666666667; loss: 2.210210084915161\n",
      "Training epoch 6764 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6764 ; accuracy: 0.7366666666666667; loss: 2.2102856636047363\n",
      "Training epoch 6765 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6765 ; accuracy: 0.7366666666666667; loss: 2.210360288619995\n",
      "Training epoch 6766 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6766 ; accuracy: 0.7366666666666667; loss: 2.21042799949646\n",
      "Training epoch 6767 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6767 ; accuracy: 0.7366666666666667; loss: 2.210479736328125\n",
      "Training epoch 6768 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6768 ; accuracy: 0.7366666666666667; loss: 2.21052622795105\n",
      "Training epoch 6769 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6769 ; accuracy: 0.7366666666666667; loss: 2.210562229156494\n",
      "Training epoch 6770 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6770 ; accuracy: 0.7366666666666667; loss: 2.210598945617676\n",
      "Training epoch 6771 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6771 ; accuracy: 0.7366666666666667; loss: 2.2106316089630127\n",
      "Training epoch 6772 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6772 ; accuracy: 0.7366666666666667; loss: 2.210657835006714\n",
      "Training epoch 6773 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6773 ; accuracy: 0.7366666666666667; loss: 2.2106857299804688\n",
      "Training epoch 6774 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6774 ; accuracy: 0.7366666666666667; loss: 2.2107067108154297\n",
      "Training epoch 6775 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6775 ; accuracy: 0.7366666666666667; loss: 2.2107179164886475\n",
      "Training epoch 6776 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6776 ; accuracy: 0.7366666666666667; loss: 2.2107279300689697\n",
      "Training epoch 6777 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6777 ; accuracy: 0.7366666666666667; loss: 2.210756778717041\n",
      "Training epoch 6778 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6778 ; accuracy: 0.7366666666666667; loss: 2.210775136947632\n",
      "Training epoch 6779 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6779 ; accuracy: 0.7366666666666667; loss: 2.210793972015381\n",
      "Training epoch 6780 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6780 ; accuracy: 0.7366666666666667; loss: 2.2108254432678223\n",
      "Training epoch 6781 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6781 ; accuracy: 0.7366666666666667; loss: 2.210850715637207\n",
      "Training epoch 6782 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6782 ; accuracy: 0.7366666666666667; loss: 2.2108755111694336\n",
      "Training epoch 6783 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6783 ; accuracy: 0.7366666666666667; loss: 2.210886240005493\n",
      "Training epoch 6784 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6784 ; accuracy: 0.7366666666666667; loss: 2.2109031677246094\n",
      "Training epoch 6785 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6785 ; accuracy: 0.7366666666666667; loss: 2.2109153270721436\n",
      "Training epoch 6786 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6786 ; accuracy: 0.7366666666666667; loss: 2.210922956466675\n",
      "Training epoch 6787 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6787 ; accuracy: 0.7366666666666667; loss: 2.210935354232788\n",
      "Training epoch 6788 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6788 ; accuracy: 0.7366666666666667; loss: 2.2109451293945312\n",
      "Training epoch 6789 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6789 ; accuracy: 0.7366666666666667; loss: 2.210954189300537\n",
      "Training epoch 6790 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6790 ; accuracy: 0.7366666666666667; loss: 2.2109618186950684\n",
      "Training epoch 6791 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6791 ; accuracy: 0.7366666666666667; loss: 2.210967540740967\n",
      "Training epoch 6792 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6792 ; accuracy: 0.7366666666666667; loss: 2.2109761238098145\n",
      "Training epoch 6793 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6793 ; accuracy: 0.7366666666666667; loss: 2.210987091064453\n",
      "Training epoch 6794 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6794 ; accuracy: 0.7366666666666667; loss: 2.2109951972961426\n",
      "Training epoch 6795 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6795 ; accuracy: 0.7366666666666667; loss: 2.211012125015259\n",
      "Training epoch 6796 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6796 ; accuracy: 0.7366666666666667; loss: 2.211030960083008\n",
      "Training epoch 6797 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6797 ; accuracy: 0.7366666666666667; loss: 2.211036443710327\n",
      "Training epoch 6798 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6798 ; accuracy: 0.7366666666666667; loss: 2.2110462188720703\n",
      "Training epoch 6799 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6799 ; accuracy: 0.7366666666666667; loss: 2.2110583782196045\n",
      "Training epoch 6800 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6800 ; accuracy: 0.7366666666666667; loss: 2.211059331893921\n",
      "Training epoch 6801 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6801 ; accuracy: 0.7366666666666667; loss: 2.2110445499420166\n",
      "Training epoch 6802 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6802 ; accuracy: 0.7366666666666667; loss: 2.2110185623168945\n",
      "Training epoch 6803 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6803 ; accuracy: 0.7366666666666667; loss: 2.2110092639923096\n",
      "Training epoch 6804 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6804 ; accuracy: 0.7366666666666667; loss: 2.2109949588775635\n",
      "Training epoch 6805 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6805 ; accuracy: 0.7366666666666667; loss: 2.21097993850708\n",
      "Training epoch 6806 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6806 ; accuracy: 0.7366666666666667; loss: 2.2110111713409424\n",
      "Training epoch 6807 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6807 ; accuracy: 0.7366666666666667; loss: 2.2110393047332764\n",
      "Training epoch 6808 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6808 ; accuracy: 0.7366666666666667; loss: 2.211073875427246\n",
      "Training epoch 6809 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6809 ; accuracy: 0.7366666666666667; loss: 2.211108922958374\n",
      "Training epoch 6810 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6810 ; accuracy: 0.7366666666666667; loss: 2.211151361465454\n",
      "Training epoch 6811 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6811 ; accuracy: 0.7366666666666667; loss: 2.211216926574707\n",
      "Training epoch 6812 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6812 ; accuracy: 0.7366666666666667; loss: 2.2112767696380615\n",
      "Training epoch 6813 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6813 ; accuracy: 0.7366666666666667; loss: 2.211332082748413\n",
      "Training epoch 6814 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6814 ; accuracy: 0.7366666666666667; loss: 2.211379289627075\n",
      "Training epoch 6815 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6815 ; accuracy: 0.7366666666666667; loss: 2.211421012878418\n",
      "Training epoch 6816 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6816 ; accuracy: 0.7366666666666667; loss: 2.2114505767822266\n",
      "Training epoch 6817 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6817 ; accuracy: 0.7366666666666667; loss: 2.211479902267456\n",
      "Training epoch 6818 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6818 ; accuracy: 0.7366666666666667; loss: 2.211507558822632\n",
      "Training epoch 6819 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6819 ; accuracy: 0.7366666666666667; loss: 2.2115321159362793\n",
      "Training epoch 6820 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6820 ; accuracy: 0.7366666666666667; loss: 2.2115535736083984\n",
      "Training epoch 6821 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6821 ; accuracy: 0.7366666666666667; loss: 2.2115771770477295\n",
      "Training epoch 6822 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6822 ; accuracy: 0.7366666666666667; loss: 2.211599826812744\n",
      "Training epoch 6823 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6823 ; accuracy: 0.7366666666666667; loss: 2.211620569229126\n",
      "Training epoch 6824 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6824 ; accuracy: 0.7366666666666667; loss: 2.211648464202881\n",
      "Training epoch 6825 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6825 ; accuracy: 0.7366666666666667; loss: 2.211676836013794\n",
      "Training epoch 6826 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6826 ; accuracy: 0.7366666666666667; loss: 2.2117106914520264\n",
      "Training epoch 6827 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6827 ; accuracy: 0.7366666666666667; loss: 2.211737871170044\n",
      "Training epoch 6828 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6828 ; accuracy: 0.7366666666666667; loss: 2.211773157119751\n",
      "Training epoch 6829 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6829 ; accuracy: 0.7366666666666667; loss: 2.2118067741394043\n",
      "Training epoch 6830 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6830 ; accuracy: 0.7366666666666667; loss: 2.211822509765625\n",
      "Training epoch 6831 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6831 ; accuracy: 0.7366666666666667; loss: 2.2118427753448486\n",
      "Training epoch 6832 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6832 ; accuracy: 0.7366666666666667; loss: 2.2118618488311768\n",
      "Training epoch 6833 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6833 ; accuracy: 0.7366666666666667; loss: 2.2118799686431885\n",
      "Training epoch 6834 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6834 ; accuracy: 0.7366666666666667; loss: 2.2118923664093018\n",
      "Training epoch 6835 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6835 ; accuracy: 0.7366666666666667; loss: 2.2119147777557373\n",
      "Training epoch 6836 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6836 ; accuracy: 0.7366666666666667; loss: 2.2119405269622803\n",
      "Training epoch 6837 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6837 ; accuracy: 0.7366666666666667; loss: 2.2119736671447754\n",
      "Training epoch 6838 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6838 ; accuracy: 0.7366666666666667; loss: 2.2120063304901123\n",
      "Training epoch 6839 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6839 ; accuracy: 0.7366666666666667; loss: 2.2120423316955566\n",
      "Training epoch 6840 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6840 ; accuracy: 0.7366666666666667; loss: 2.212080478668213\n",
      "Training epoch 6841 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6841 ; accuracy: 0.7366666666666667; loss: 2.2120959758758545\n",
      "Training epoch 6842 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6842 ; accuracy: 0.7366666666666667; loss: 2.212111234664917\n",
      "Training epoch 6843 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6843 ; accuracy: 0.7366666666666667; loss: 2.212130069732666\n",
      "Training epoch 6844 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6844 ; accuracy: 0.74; loss: 2.2121481895446777\n",
      "Training epoch 6845 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6845 ; accuracy: 0.74; loss: 2.2121646404266357\n",
      "Training epoch 6846 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6846 ; accuracy: 0.74; loss: 2.2121782302856445\n",
      "Training epoch 6847 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6847 ; accuracy: 0.74; loss: 2.2121949195861816\n",
      "Training epoch 6848 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6848 ; accuracy: 0.74; loss: 2.2122113704681396\n",
      "Training epoch 6849 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6849 ; accuracy: 0.74; loss: 2.2122323513031006\n",
      "Training epoch 6850 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6850 ; accuracy: 0.74; loss: 2.2122583389282227\n",
      "Training epoch 6851 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6851 ; accuracy: 0.74; loss: 2.212279796600342\n",
      "Training epoch 6852 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6852 ; accuracy: 0.74; loss: 2.2123072147369385\n",
      "Training epoch 6853 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6853 ; accuracy: 0.74; loss: 2.2123372554779053\n",
      "Training epoch 6854 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6854 ; accuracy: 0.74; loss: 2.212364912033081\n",
      "Training epoch 6855 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 6855 ; accuracy: 0.74; loss: 2.2123937606811523\n",
      "Training epoch 6856 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6856 ; accuracy: 0.74; loss: 2.2124271392822266\n",
      "Training epoch 6857 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6857 ; accuracy: 0.74; loss: 2.212456226348877\n",
      "Training epoch 6858 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6858 ; accuracy: 0.74; loss: 2.212486505508423\n",
      "Training epoch 6859 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6859 ; accuracy: 0.74; loss: 2.2125139236450195\n",
      "Training epoch 6860 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6860 ; accuracy: 0.74; loss: 2.2125418186187744\n",
      "Training epoch 6861 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6861 ; accuracy: 0.7366666666666667; loss: 2.2125720977783203\n",
      "Training epoch 6862 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6862 ; accuracy: 0.7366666666666667; loss: 2.212599754333496\n",
      "Training epoch 6863 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6863 ; accuracy: 0.7366666666666667; loss: 2.2126259803771973\n",
      "Training epoch 6864 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6864 ; accuracy: 0.7366666666666667; loss: 2.212651491165161\n",
      "Training epoch 6865 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6865 ; accuracy: 0.7366666666666667; loss: 2.2126708030700684\n",
      "Training epoch 6866 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6866 ; accuracy: 0.7366666666666667; loss: 2.2126970291137695\n",
      "Training epoch 6867 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6867 ; accuracy: 0.7366666666666667; loss: 2.21274471282959\n",
      "Training epoch 6868 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6868 ; accuracy: 0.7366666666666667; loss: 2.212783098220825\n",
      "Training epoch 6869 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6869 ; accuracy: 0.7366666666666667; loss: 2.2128193378448486\n",
      "Training epoch 6870 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6870 ; accuracy: 0.7366666666666667; loss: 2.212869167327881\n",
      "Training epoch 6871 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6871 ; accuracy: 0.7366666666666667; loss: 2.212918281555176\n",
      "Training epoch 6872 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6872 ; accuracy: 0.7366666666666667; loss: 2.2129640579223633\n",
      "Training epoch 6873 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6873 ; accuracy: 0.7366666666666667; loss: 2.2130050659179688\n",
      "Training epoch 6874 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6874 ; accuracy: 0.7366666666666667; loss: 2.213041067123413\n",
      "Training epoch 6875 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6875 ; accuracy: 0.7366666666666667; loss: 2.213076591491699\n",
      "Training epoch 6876 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6876 ; accuracy: 0.7366666666666667; loss: 2.213111400604248\n",
      "Training epoch 6877 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6877 ; accuracy: 0.7366666666666667; loss: 2.2131519317626953\n",
      "Training epoch 6878 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6878 ; accuracy: 0.7366666666666667; loss: 2.2131950855255127\n",
      "Training epoch 6879 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6879 ; accuracy: 0.7366666666666667; loss: 2.2132363319396973\n",
      "Training epoch 6880 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6880 ; accuracy: 0.7366666666666667; loss: 2.213280439376831\n",
      "Training epoch 6881 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6881 ; accuracy: 0.7366666666666667; loss: 2.2133240699768066\n",
      "Training epoch 6882 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6882 ; accuracy: 0.7366666666666667; loss: 2.213367462158203\n",
      "Training epoch 6883 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6883 ; accuracy: 0.7366666666666667; loss: 2.2134053707122803\n",
      "Training epoch 6884 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6884 ; accuracy: 0.7366666666666667; loss: 2.213442325592041\n",
      "Training epoch 6885 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6885 ; accuracy: 0.7366666666666667; loss: 2.2134745121002197\n",
      "Training epoch 6886 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6886 ; accuracy: 0.7366666666666667; loss: 2.2135026454925537\n",
      "Training epoch 6887 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6887 ; accuracy: 0.7366666666666667; loss: 2.213526725769043\n",
      "Training epoch 6888 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6888 ; accuracy: 0.7366666666666667; loss: 2.21355938911438\n",
      "Training epoch 6889 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6889 ; accuracy: 0.7366666666666667; loss: 2.213588237762451\n",
      "Training epoch 6890 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6890 ; accuracy: 0.7366666666666667; loss: 2.213625907897949\n",
      "Training epoch 6891 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6891 ; accuracy: 0.7366666666666667; loss: 2.2136588096618652\n",
      "Training epoch 6892 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6892 ; accuracy: 0.7366666666666667; loss: 2.2136855125427246\n",
      "Training epoch 6893 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6893 ; accuracy: 0.7366666666666667; loss: 2.2137160301208496\n",
      "Training epoch 6894 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6894 ; accuracy: 0.7366666666666667; loss: 2.2137463092803955\n",
      "Training epoch 6895 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6895 ; accuracy: 0.7366666666666667; loss: 2.21376895904541\n",
      "Training epoch 6896 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6896 ; accuracy: 0.7366666666666667; loss: 2.213793992996216\n",
      "Training epoch 6897 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6897 ; accuracy: 0.7366666666666667; loss: 2.2138173580169678\n",
      "Training epoch 6898 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6898 ; accuracy: 0.7366666666666667; loss: 2.2138512134552\n",
      "Training epoch 6899 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6899 ; accuracy: 0.7366666666666667; loss: 2.213829755783081\n",
      "Training epoch 6900 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6900 ; accuracy: 0.7366666666666667; loss: 2.213819980621338\n",
      "Training epoch 6901 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6901 ; accuracy: 0.7366666666666667; loss: 2.213815927505493\n",
      "Training epoch 6902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6902 ; accuracy: 0.7366666666666667; loss: 2.2138187885284424\n",
      "Training epoch 6903 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6903 ; accuracy: 0.7366666666666667; loss: 2.2138161659240723\n",
      "Training epoch 6904 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6904 ; accuracy: 0.7366666666666667; loss: 2.2138164043426514\n",
      "Training epoch 6905 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6905 ; accuracy: 0.7366666666666667; loss: 2.2138137817382812\n",
      "Training epoch 6906 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6906 ; accuracy: 0.7366666666666667; loss: 2.2138137817382812\n",
      "Training epoch 6907 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6907 ; accuracy: 0.7366666666666667; loss: 2.2138137817382812\n",
      "Training epoch 6908 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6908 ; accuracy: 0.7366666666666667; loss: 2.2138168811798096\n",
      "Training epoch 6909 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6909 ; accuracy: 0.7366666666666667; loss: 2.213815450668335\n",
      "Training epoch 6910 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6910 ; accuracy: 0.7366666666666667; loss: 2.213819980621338\n",
      "Training epoch 6911 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6911 ; accuracy: 0.7366666666666667; loss: 2.2138354778289795\n",
      "Training epoch 6912 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6912 ; accuracy: 0.7366666666666667; loss: 2.2138993740081787\n",
      "Training epoch 6913 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6913 ; accuracy: 0.7366666666666667; loss: 2.2139554023742676\n",
      "Training epoch 6914 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6914 ; accuracy: 0.7366666666666667; loss: 2.2140090465545654\n",
      "Training epoch 6915 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6915 ; accuracy: 0.7366666666666667; loss: 2.2140555381774902\n",
      "Training epoch 6916 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6916 ; accuracy: 0.7366666666666667; loss: 2.2140958309173584\n",
      "Training epoch 6917 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6917 ; accuracy: 0.7366666666666667; loss: 2.2141354084014893\n",
      "Training epoch 6918 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6918 ; accuracy: 0.7366666666666667; loss: 2.2141735553741455\n",
      "Training epoch 6919 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6919 ; accuracy: 0.7366666666666667; loss: 2.2142174243927\n",
      "Training epoch 6920 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6920 ; accuracy: 0.7366666666666667; loss: 2.2142550945281982\n",
      "Training epoch 6921 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6921 ; accuracy: 0.7366666666666667; loss: 2.214299201965332\n",
      "Training epoch 6922 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6922 ; accuracy: 0.7366666666666667; loss: 2.2143449783325195\n",
      "Training epoch 6923 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6923 ; accuracy: 0.7366666666666667; loss: 2.2143874168395996\n",
      "Training epoch 6924 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6924 ; accuracy: 0.7366666666666667; loss: 2.2144241333007812\n",
      "Training epoch 6925 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6925 ; accuracy: 0.7366666666666667; loss: 2.214463233947754\n",
      "Training epoch 6926 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6926 ; accuracy: 0.7366666666666667; loss: 2.2144925594329834\n",
      "Training epoch 6927 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6927 ; accuracy: 0.7366666666666667; loss: 2.2145185470581055\n",
      "Training epoch 6928 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6928 ; accuracy: 0.7366666666666667; loss: 2.214542865753174\n",
      "Training epoch 6929 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6929 ; accuracy: 0.7366666666666667; loss: 2.214561700820923\n",
      "Training epoch 6930 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6930 ; accuracy: 0.7366666666666667; loss: 2.214583158493042\n",
      "Training epoch 6931 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6931 ; accuracy: 0.7366666666666667; loss: 2.2146124839782715\n",
      "Training epoch 6932 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6932 ; accuracy: 0.7366666666666667; loss: 2.214639902114868\n",
      "Training epoch 6933 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6933 ; accuracy: 0.7366666666666667; loss: 2.214667320251465\n",
      "Training epoch 6934 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6934 ; accuracy: 0.7366666666666667; loss: 2.214690923690796\n",
      "Training epoch 6935 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6935 ; accuracy: 0.7366666666666667; loss: 2.2147107124328613\n",
      "Training epoch 6936 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6936 ; accuracy: 0.7366666666666667; loss: 2.2147328853607178\n",
      "Training epoch 6937 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6937 ; accuracy: 0.7366666666666667; loss: 2.2147562503814697\n",
      "Training epoch 6938 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6938 ; accuracy: 0.7366666666666667; loss: 2.214784622192383\n",
      "Training epoch 6939 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6939 ; accuracy: 0.7366666666666667; loss: 2.2148079872131348\n",
      "Training epoch 6940 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6940 ; accuracy: 0.7366666666666667; loss: 2.2148280143737793\n",
      "Training epoch 6941 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6941 ; accuracy: 0.7366666666666667; loss: 2.2148470878601074\n",
      "Training epoch 6942 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6942 ; accuracy: 0.7366666666666667; loss: 2.214876651763916\n",
      "Training epoch 6943 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6943 ; accuracy: 0.7366666666666667; loss: 2.2149040699005127\n",
      "Training epoch 6944 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6944 ; accuracy: 0.7366666666666667; loss: 2.214923143386841\n",
      "Training epoch 6945 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6945 ; accuracy: 0.7366666666666667; loss: 2.214937448501587\n",
      "Training epoch 6946 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6946 ; accuracy: 0.7366666666666667; loss: 2.2149603366851807\n",
      "Training epoch 6947 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6947 ; accuracy: 0.7366666666666667; loss: 2.214989423751831\n",
      "Training epoch 6948 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6948 ; accuracy: 0.7366666666666667; loss: 2.215013027191162\n",
      "Training epoch 6949 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6949 ; accuracy: 0.7366666666666667; loss: 2.215038537979126\n",
      "Training epoch 6950 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6950 ; accuracy: 0.7366666666666667; loss: 2.215069055557251\n",
      "Training epoch 6951 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6951 ; accuracy: 0.7366666666666667; loss: 2.2150943279266357\n",
      "Training epoch 6952 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6952 ; accuracy: 0.7366666666666667; loss: 2.215120553970337\n",
      "Training epoch 6953 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6953 ; accuracy: 0.7366666666666667; loss: 2.2151541709899902\n",
      "Training epoch 6954 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6954 ; accuracy: 0.7366666666666667; loss: 2.215184211730957\n",
      "Training epoch 6955 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6955 ; accuracy: 0.7366666666666667; loss: 2.2152161598205566\n",
      "Training epoch 6956 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6956 ; accuracy: 0.7366666666666667; loss: 2.215238571166992\n",
      "Training epoch 6957 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6957 ; accuracy: 0.7366666666666667; loss: 2.215254306793213\n",
      "Training epoch 6958 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6958 ; accuracy: 0.7366666666666667; loss: 2.215273857116699\n",
      "Training epoch 6959 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6959 ; accuracy: 0.7366666666666667; loss: 2.2153942584991455\n",
      "Training epoch 6960 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6960 ; accuracy: 0.7366666666666667; loss: 2.2154998779296875\n",
      "Training epoch 6961 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6961 ; accuracy: 0.7366666666666667; loss: 2.2155961990356445\n",
      "Training epoch 6962 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6962 ; accuracy: 0.7366666666666667; loss: 2.2156760692596436\n",
      "Training epoch 6963 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6963 ; accuracy: 0.7366666666666667; loss: 2.215744733810425\n",
      "Training epoch 6964 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6964 ; accuracy: 0.7366666666666667; loss: 2.215803861618042\n",
      "Training epoch 6965 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6965 ; accuracy: 0.7366666666666667; loss: 2.215846061706543\n",
      "Training epoch 6966 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6966 ; accuracy: 0.7366666666666667; loss: 2.2158918380737305\n",
      "Training epoch 6967 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6967 ; accuracy: 0.7366666666666667; loss: 2.2159321308135986\n",
      "Training epoch 6968 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6968 ; accuracy: 0.7366666666666667; loss: 2.2159664630889893\n",
      "Training epoch 6969 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6969 ; accuracy: 0.7366666666666667; loss: 2.2160353660583496\n",
      "Training epoch 6970 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6970 ; accuracy: 0.7366666666666667; loss: 2.2161006927490234\n",
      "Training epoch 6971 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6971 ; accuracy: 0.7366666666666667; loss: 2.216163158416748\n",
      "Training epoch 6972 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6972 ; accuracy: 0.7366666666666667; loss: 2.216228723526001\n",
      "Training epoch 6973 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6973 ; accuracy: 0.7366666666666667; loss: 2.216280221939087\n",
      "Training epoch 6974 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6974 ; accuracy: 0.7366666666666667; loss: 2.2163355350494385\n",
      "Training epoch 6975 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6975 ; accuracy: 0.7366666666666667; loss: 2.2163851261138916\n",
      "Training epoch 6976 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6976 ; accuracy: 0.7366666666666667; loss: 2.216439962387085\n",
      "Training epoch 6977 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6977 ; accuracy: 0.7366666666666667; loss: 2.216482639312744\n",
      "Training epoch 6978 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6978 ; accuracy: 0.7366666666666667; loss: 2.2165133953094482\n",
      "Training epoch 6979 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6979 ; accuracy: 0.7366666666666667; loss: 2.216543436050415\n",
      "Training epoch 6980 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6980 ; accuracy: 0.7366666666666667; loss: 2.21657133102417\n",
      "Training epoch 6981 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6981 ; accuracy: 0.7366666666666667; loss: 2.216597080230713\n",
      "Training epoch 6982 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6982 ; accuracy: 0.7366666666666667; loss: 2.216613292694092\n",
      "Training epoch 6983 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6983 ; accuracy: 0.7366666666666667; loss: 2.2166309356689453\n",
      "Training epoch 6984 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6984 ; accuracy: 0.7366666666666667; loss: 2.2166483402252197\n",
      "Training epoch 6985 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6985 ; accuracy: 0.7366666666666667; loss: 2.2166686058044434\n",
      "Training epoch 6986 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6986 ; accuracy: 0.7366666666666667; loss: 2.2166824340820312\n",
      "Training epoch 6987 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6987 ; accuracy: 0.7366666666666667; loss: 2.216717004776001\n",
      "Training epoch 6988 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6988 ; accuracy: 0.7366666666666667; loss: 2.2167413234710693\n",
      "Training epoch 6989 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6989 ; accuracy: 0.7366666666666667; loss: 2.216768503189087\n",
      "Training epoch 6990 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6990 ; accuracy: 0.7366666666666667; loss: 2.2167837619781494\n",
      "Training epoch 6991 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6991 ; accuracy: 0.7366666666666667; loss: 2.216794729232788\n",
      "Training epoch 6992 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6992 ; accuracy: 0.7366666666666667; loss: 2.2168099880218506\n",
      "Training epoch 6993 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6993 ; accuracy: 0.7366666666666667; loss: 2.2168262004852295\n",
      "Training epoch 6994 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6994 ; accuracy: 0.7366666666666667; loss: 2.2168517112731934\n",
      "Training epoch 6995 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 6995 ; accuracy: 0.7366666666666667; loss: 2.2168805599212646\n",
      "Training epoch 6996 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 6996 ; accuracy: 0.7366666666666667; loss: 2.216914176940918\n",
      "Training epoch 6997 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6997 ; accuracy: 0.7366666666666667; loss: 2.2169392108917236\n",
      "Training epoch 6998 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6998 ; accuracy: 0.7366666666666667; loss: 2.2169532775878906\n",
      "Training epoch 6999 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 6999 ; accuracy: 0.7366666666666667; loss: 2.216968536376953\n",
      "Training epoch 7000 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7000 ; accuracy: 0.7366666666666667; loss: 2.216977596282959\n",
      "Training epoch 7001 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7001 ; accuracy: 0.7366666666666667; loss: 2.2170026302337646\n",
      "Training epoch 7002 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7002 ; accuracy: 0.7366666666666667; loss: 2.217022180557251\n",
      "Training epoch 7003 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7003 ; accuracy: 0.7366666666666667; loss: 2.2170448303222656\n",
      "Training epoch 7004 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7004 ; accuracy: 0.7366666666666667; loss: 2.2170636653900146\n",
      "Training epoch 7005 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7005 ; accuracy: 0.7366666666666667; loss: 2.217083215713501\n",
      "Training epoch 7006 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7006 ; accuracy: 0.7366666666666667; loss: 2.2170894145965576\n",
      "Training epoch 7007 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7007 ; accuracy: 0.7366666666666667; loss: 2.2170979976654053\n",
      "Training epoch 7008 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7008 ; accuracy: 0.7366666666666667; loss: 2.2171263694763184\n",
      "Training epoch 7009 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7009 ; accuracy: 0.7366666666666667; loss: 2.2171576023101807\n",
      "Training epoch 7010 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7010 ; accuracy: 0.7366666666666667; loss: 2.2171874046325684\n",
      "Training epoch 7011 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7011 ; accuracy: 0.7366666666666667; loss: 2.217222213745117\n",
      "Training epoch 7012 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7012 ; accuracy: 0.7366666666666667; loss: 2.2172818183898926\n",
      "Training epoch 7013 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7013 ; accuracy: 0.7366666666666667; loss: 2.217338800430298\n",
      "Training epoch 7014 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7014 ; accuracy: 0.7366666666666667; loss: 2.2173988819122314\n",
      "Training epoch 7015 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7015 ; accuracy: 0.7366666666666667; loss: 2.217475414276123\n",
      "Training epoch 7016 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7016 ; accuracy: 0.7366666666666667; loss: 2.217545986175537\n",
      "Training epoch 7017 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7017 ; accuracy: 0.7366666666666667; loss: 2.21761417388916\n",
      "Training epoch 7018 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7018 ; accuracy: 0.7366666666666667; loss: 2.2176809310913086\n",
      "Training epoch 7019 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7019 ; accuracy: 0.7366666666666667; loss: 2.217742919921875\n",
      "Training epoch 7020 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7020 ; accuracy: 0.7366666666666667; loss: 2.2177956104278564\n",
      "Training epoch 7021 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7021 ; accuracy: 0.7366666666666667; loss: 2.217834711074829\n",
      "Training epoch 7022 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7022 ; accuracy: 0.7366666666666667; loss: 2.2178633213043213\n",
      "Training epoch 7023 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7023 ; accuracy: 0.7366666666666667; loss: 2.2178854942321777\n",
      "Training epoch 7024 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7024 ; accuracy: 0.7366666666666667; loss: 2.2179064750671387\n",
      "Training epoch 7025 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7025 ; accuracy: 0.7366666666666667; loss: 2.217937707901001\n",
      "Training epoch 7026 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7026 ; accuracy: 0.7366666666666667; loss: 2.2179601192474365\n",
      "Training epoch 7027 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7027 ; accuracy: 0.7366666666666667; loss: 2.217973232269287\n",
      "Training epoch 7028 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7028 ; accuracy: 0.7366666666666667; loss: 2.217991352081299\n",
      "Training epoch 7029 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7029 ; accuracy: 0.7366666666666667; loss: 2.2180113792419434\n",
      "Training epoch 7030 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7030 ; accuracy: 0.7366666666666667; loss: 2.218022108078003\n",
      "Training epoch 7031 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7031 ; accuracy: 0.7366666666666667; loss: 2.218031644821167\n",
      "Training epoch 7032 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7032 ; accuracy: 0.7366666666666667; loss: 2.218043565750122\n",
      "Training epoch 7033 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7033 ; accuracy: 0.7366666666666667; loss: 2.2180497646331787\n",
      "Training epoch 7034 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7034 ; accuracy: 0.7366666666666667; loss: 2.218055009841919\n",
      "Training epoch 7035 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7035 ; accuracy: 0.7366666666666667; loss: 2.2180681228637695\n",
      "Training epoch 7036 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7036 ; accuracy: 0.7366666666666667; loss: 2.2180840969085693\n",
      "Training epoch 7037 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7037 ; accuracy: 0.7366666666666667; loss: 2.218104124069214\n",
      "Training epoch 7038 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7038 ; accuracy: 0.7366666666666667; loss: 2.2181236743927\n",
      "Training epoch 7039 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7039 ; accuracy: 0.7366666666666667; loss: 2.2181458473205566\n",
      "Training epoch 7040 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7040 ; accuracy: 0.7366666666666667; loss: 2.2181787490844727\n",
      "Training epoch 7041 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7041 ; accuracy: 0.7366666666666667; loss: 2.218208074569702\n",
      "Training epoch 7042 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7042 ; accuracy: 0.7366666666666667; loss: 2.218230724334717\n",
      "Training epoch 7043 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7043 ; accuracy: 0.7366666666666667; loss: 2.218247175216675\n",
      "Training epoch 7044 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7044 ; accuracy: 0.7366666666666667; loss: 2.218266487121582\n",
      "Training epoch 7045 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7045 ; accuracy: 0.7366666666666667; loss: 2.218290090560913\n",
      "Training epoch 7046 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7046 ; accuracy: 0.7366666666666667; loss: 2.218320369720459\n",
      "Training epoch 7047 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7047 ; accuracy: 0.7366666666666667; loss: 2.2183303833007812\n",
      "Training epoch 7048 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7048 ; accuracy: 0.7366666666666667; loss: 2.2183401584625244\n",
      "Training epoch 7049 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7049 ; accuracy: 0.7366666666666667; loss: 2.2183399200439453\n",
      "Training epoch 7050 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7050 ; accuracy: 0.7366666666666667; loss: 2.218343734741211\n",
      "Training epoch 7051 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7051 ; accuracy: 0.7366666666666667; loss: 2.2183549404144287\n",
      "Training epoch 7052 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7052 ; accuracy: 0.7366666666666667; loss: 2.218369245529175\n",
      "Training epoch 7053 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7053 ; accuracy: 0.7366666666666667; loss: 2.2183914184570312\n",
      "Training epoch 7054 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7054 ; accuracy: 0.7366666666666667; loss: 2.2184078693389893\n",
      "Training epoch 7055 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7055 ; accuracy: 0.7366666666666667; loss: 2.2184202671051025\n",
      "Training epoch 7056 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7056 ; accuracy: 0.7366666666666667; loss: 2.2184183597564697\n",
      "Training epoch 7057 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7057 ; accuracy: 0.7366666666666667; loss: 2.2184157371520996\n",
      "Training epoch 7058 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7058 ; accuracy: 0.7366666666666667; loss: 2.218409776687622\n",
      "Training epoch 7059 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7059 ; accuracy: 0.7366666666666667; loss: 2.2184057235717773\n",
      "Training epoch 7060 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7060 ; accuracy: 0.7366666666666667; loss: 2.2183878421783447\n",
      "Training epoch 7061 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7061 ; accuracy: 0.7366666666666667; loss: 2.218367099761963\n",
      "Training epoch 7062 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7062 ; accuracy: 0.7366666666666667; loss: 2.218362808227539\n",
      "Training epoch 7063 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7063 ; accuracy: 0.7366666666666667; loss: 2.218367576599121\n",
      "Training epoch 7064 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7064 ; accuracy: 0.7366666666666667; loss: 2.218376398086548\n",
      "Training epoch 7065 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7065 ; accuracy: 0.7366666666666667; loss: 2.2183852195739746\n",
      "Training epoch 7066 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7066 ; accuracy: 0.7366666666666667; loss: 2.2183990478515625\n",
      "Training epoch 7067 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7067 ; accuracy: 0.7366666666666667; loss: 2.218414306640625\n",
      "Training epoch 7068 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7068 ; accuracy: 0.7366666666666667; loss: 2.2184247970581055\n",
      "Training epoch 7069 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7069 ; accuracy: 0.7366666666666667; loss: 2.2184388637542725\n",
      "Training epoch 7070 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7070 ; accuracy: 0.7366666666666667; loss: 2.218451738357544\n",
      "Training epoch 7071 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7071 ; accuracy: 0.7366666666666667; loss: 2.21846079826355\n",
      "Training epoch 7072 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7072 ; accuracy: 0.7366666666666667; loss: 2.2184770107269287\n",
      "Training epoch 7073 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7073 ; accuracy: 0.7366666666666667; loss: 2.2184948921203613\n",
      "Training epoch 7074 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7074 ; accuracy: 0.7366666666666667; loss: 2.218518018722534\n",
      "Training epoch 7075 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7075 ; accuracy: 0.7366666666666667; loss: 2.2185301780700684\n",
      "Training epoch 7076 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7076 ; accuracy: 0.7366666666666667; loss: 2.2185657024383545\n",
      "Training epoch 7077 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7077 ; accuracy: 0.7366666666666667; loss: 2.2186124324798584\n",
      "Training epoch 7078 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7078 ; accuracy: 0.7366666666666667; loss: 2.2186243534088135\n",
      "Training epoch 7079 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7079 ; accuracy: 0.7366666666666667; loss: 2.2186243534088135\n",
      "Training epoch 7080 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7080 ; accuracy: 0.7366666666666667; loss: 2.218616247177124\n",
      "Training epoch 7081 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7081 ; accuracy: 0.7366666666666667; loss: 2.2186150550842285\n",
      "Training epoch 7082 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7082 ; accuracy: 0.7366666666666667; loss: 2.218608856201172\n",
      "Training epoch 7083 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7083 ; accuracy: 0.7366666666666667; loss: 2.21860933303833\n",
      "Training epoch 7084 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7084 ; accuracy: 0.7366666666666667; loss: 2.218609094619751\n",
      "Training epoch 7085 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7085 ; accuracy: 0.7366666666666667; loss: 2.2186079025268555\n",
      "Training epoch 7086 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7086 ; accuracy: 0.7366666666666667; loss: 2.2186121940612793\n",
      "Training epoch 7087 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7087 ; accuracy: 0.7366666666666667; loss: 2.2186357975006104\n",
      "Training epoch 7088 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7088 ; accuracy: 0.7366666666666667; loss: 2.2186572551727295\n",
      "Training epoch 7089 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7089 ; accuracy: 0.7366666666666667; loss: 2.2186760902404785\n",
      "Training epoch 7090 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7090 ; accuracy: 0.7366666666666667; loss: 2.218693494796753\n",
      "Training epoch 7091 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7091 ; accuracy: 0.7366666666666667; loss: 2.2187132835388184\n",
      "Training epoch 7092 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7092 ; accuracy: 0.7366666666666667; loss: 2.218731641769409\n",
      "Training epoch 7093 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7093 ; accuracy: 0.7366666666666667; loss: 2.218701124191284\n",
      "Training epoch 7094 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7094 ; accuracy: 0.7366666666666667; loss: 2.2186832427978516\n",
      "Training epoch 7095 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7095 ; accuracy: 0.7366666666666667; loss: 2.218656063079834\n",
      "Training epoch 7096 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7096 ; accuracy: 0.7366666666666667; loss: 2.2186312675476074\n",
      "Training epoch 7097 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7097 ; accuracy: 0.7366666666666667; loss: 2.218604803085327\n",
      "Training epoch 7098 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7098 ; accuracy: 0.7366666666666667; loss: 2.218580484390259\n",
      "Training epoch 7099 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7099 ; accuracy: 0.7366666666666667; loss: 2.218559741973877\n",
      "Training epoch 7100 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7100 ; accuracy: 0.7366666666666667; loss: 2.2185370922088623\n",
      "Training epoch 7101 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 7101 ; accuracy: 0.7366666666666667; loss: 2.2186200618743896\n",
      "Training epoch 7102 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7102 ; accuracy: 0.7366666666666667; loss: 2.2186944484710693\n",
      "Training epoch 7103 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7103 ; accuracy: 0.7366666666666667; loss: 2.2187652587890625\n",
      "Training epoch 7104 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7104 ; accuracy: 0.7366666666666667; loss: 2.2188220024108887\n",
      "Training epoch 7105 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7105 ; accuracy: 0.7366666666666667; loss: 2.2188682556152344\n",
      "Training epoch 7106 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7106 ; accuracy: 0.7366666666666667; loss: 2.218918800354004\n",
      "Training epoch 7107 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7107 ; accuracy: 0.7366666666666667; loss: 2.218965768814087\n",
      "Training epoch 7108 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7108 ; accuracy: 0.7366666666666667; loss: 2.219005823135376\n",
      "Training epoch 7109 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7109 ; accuracy: 0.7366666666666667; loss: 2.2190401554107666\n",
      "Training epoch 7110 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7110 ; accuracy: 0.7366666666666667; loss: 2.2190706729888916\n",
      "Training epoch 7111 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7111 ; accuracy: 0.7366666666666667; loss: 2.219104528427124\n",
      "Training epoch 7112 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7112 ; accuracy: 0.7366666666666667; loss: 2.2191319465637207\n",
      "Training epoch 7113 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7113 ; accuracy: 0.7366666666666667; loss: 2.2191569805145264\n",
      "Training epoch 7114 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7114 ; accuracy: 0.7366666666666667; loss: 2.2191805839538574\n",
      "Training epoch 7115 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7115 ; accuracy: 0.7366666666666667; loss: 2.2192044258117676\n",
      "Training epoch 7116 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7116 ; accuracy: 0.7366666666666667; loss: 2.2192366123199463\n",
      "Training epoch 7117 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7117 ; accuracy: 0.7366666666666667; loss: 2.219268798828125\n",
      "Training epoch 7118 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7118 ; accuracy: 0.7366666666666667; loss: 2.219306230545044\n",
      "Training epoch 7119 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7119 ; accuracy: 0.7366666666666667; loss: 2.2193400859832764\n",
      "Training epoch 7120 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7120 ; accuracy: 0.7366666666666667; loss: 2.219372272491455\n",
      "Training epoch 7121 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7121 ; accuracy: 0.7366666666666667; loss: 2.2193961143493652\n",
      "Training epoch 7122 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7122 ; accuracy: 0.7366666666666667; loss: 2.219430685043335\n",
      "Training epoch 7123 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7123 ; accuracy: 0.7366666666666667; loss: 2.219463348388672\n",
      "Training epoch 7124 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7124 ; accuracy: 0.7366666666666667; loss: 2.2194783687591553\n",
      "Training epoch 7125 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7125 ; accuracy: 0.7366666666666667; loss: 2.219534158706665\n",
      "Training epoch 7126 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7126 ; accuracy: 0.7366666666666667; loss: 2.2195959091186523\n",
      "Training epoch 7127 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7127 ; accuracy: 0.7366666666666667; loss: 2.219641923904419\n",
      "Training epoch 7128 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7128 ; accuracy: 0.7366666666666667; loss: 2.2196836471557617\n",
      "Training epoch 7129 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7129 ; accuracy: 0.7366666666666667; loss: 2.2197282314300537\n",
      "Training epoch 7130 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7130 ; accuracy: 0.7366666666666667; loss: 2.2197623252868652\n",
      "Training epoch 7131 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7131 ; accuracy: 0.7366666666666667; loss: 2.2197916507720947\n",
      "Training epoch 7132 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7132 ; accuracy: 0.7366666666666667; loss: 2.219820261001587\n",
      "Training epoch 7133 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7133 ; accuracy: 0.7366666666666667; loss: 2.219848394393921\n",
      "Training epoch 7134 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7134 ; accuracy: 0.7366666666666667; loss: 2.219877004623413\n",
      "Training epoch 7135 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7135 ; accuracy: 0.7366666666666667; loss: 2.2199034690856934\n",
      "Training epoch 7136 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7136 ; accuracy: 0.7366666666666667; loss: 2.2199316024780273\n",
      "Training epoch 7137 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7137 ; accuracy: 0.7366666666666667; loss: 2.2199625968933105\n",
      "Training epoch 7138 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7138 ; accuracy: 0.7366666666666667; loss: 2.219985008239746\n",
      "Training epoch 7139 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7139 ; accuracy: 0.7366666666666667; loss: 2.220010280609131\n",
      "Training epoch 7140 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7140 ; accuracy: 0.7366666666666667; loss: 2.2200334072113037\n",
      "Training epoch 7141 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7141 ; accuracy: 0.7366666666666667; loss: 2.2200515270233154\n",
      "Training epoch 7142 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7142 ; accuracy: 0.7366666666666667; loss: 2.220071315765381\n",
      "Training epoch 7143 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7143 ; accuracy: 0.7366666666666667; loss: 2.220085382461548\n",
      "Training epoch 7144 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7144 ; accuracy: 0.7366666666666667; loss: 2.2201015949249268\n",
      "Training epoch 7145 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7145 ; accuracy: 0.7366666666666667; loss: 2.2201180458068848\n",
      "Training epoch 7146 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7146 ; accuracy: 0.7366666666666667; loss: 2.2201383113861084\n",
      "Training epoch 7147 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7147 ; accuracy: 0.7366666666666667; loss: 2.220160484313965\n",
      "Training epoch 7148 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7148 ; accuracy: 0.7366666666666667; loss: 2.220181941986084\n",
      "Training epoch 7149 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7149 ; accuracy: 0.7366666666666667; loss: 2.2202038764953613\n",
      "Training epoch 7150 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7150 ; accuracy: 0.7366666666666667; loss: 2.220223903656006\n",
      "Training epoch 7151 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7151 ; accuracy: 0.7366666666666667; loss: 2.2202417850494385\n",
      "Training epoch 7152 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7152 ; accuracy: 0.7366666666666667; loss: 2.2202577590942383\n",
      "Training epoch 7153 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7153 ; accuracy: 0.7366666666666667; loss: 2.220280647277832\n",
      "Training epoch 7154 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7154 ; accuracy: 0.7366666666666667; loss: 2.220301866531372\n",
      "Training epoch 7155 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7155 ; accuracy: 0.7366666666666667; loss: 2.220323085784912\n",
      "Training epoch 7156 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 7156 ; accuracy: 0.7366666666666667; loss: 2.220353126525879\n",
      "Training epoch 7157 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7157 ; accuracy: 0.7366666666666667; loss: 2.2203869819641113\n",
      "Training epoch 7158 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7158 ; accuracy: 0.7366666666666667; loss: 2.2204296588897705\n",
      "Training epoch 7159 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7159 ; accuracy: 0.7366666666666667; loss: 2.2204675674438477\n",
      "Training epoch 7160 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7160 ; accuracy: 0.7366666666666667; loss: 2.2205002307891846\n",
      "Training epoch 7161 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7161 ; accuracy: 0.7366666666666667; loss: 2.220527410507202\n",
      "Training epoch 7162 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7162 ; accuracy: 0.7366666666666667; loss: 2.2205398082733154\n",
      "Training epoch 7163 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7163 ; accuracy: 0.7366666666666667; loss: 2.220543146133423\n",
      "Training epoch 7164 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7164 ; accuracy: 0.7366666666666667; loss: 2.2205429077148438\n",
      "Training epoch 7165 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7165 ; accuracy: 0.7366666666666667; loss: 2.220550775527954\n",
      "Training epoch 7166 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7166 ; accuracy: 0.7366666666666667; loss: 2.2205567359924316\n",
      "Training epoch 7167 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7167 ; accuracy: 0.7366666666666667; loss: 2.2205612659454346\n",
      "Training epoch 7168 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7168 ; accuracy: 0.7366666666666667; loss: 2.220573663711548\n",
      "Training epoch 7169 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7169 ; accuracy: 0.7366666666666667; loss: 2.2205798625946045\n",
      "Training epoch 7170 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7170 ; accuracy: 0.7366666666666667; loss: 2.220581531524658\n",
      "Training epoch 7171 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7171 ; accuracy: 0.7366666666666667; loss: 2.2205891609191895\n",
      "Training epoch 7172 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7172 ; accuracy: 0.7366666666666667; loss: 2.2205982208251953\n",
      "Training epoch 7173 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7173 ; accuracy: 0.7366666666666667; loss: 2.220609426498413\n",
      "Training epoch 7174 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7174 ; accuracy: 0.7366666666666667; loss: 2.220613479614258\n",
      "Training epoch 7175 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7175 ; accuracy: 0.7366666666666667; loss: 2.2206177711486816\n",
      "Training epoch 7176 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7176 ; accuracy: 0.7366666666666667; loss: 2.2206220626831055\n",
      "Training epoch 7177 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7177 ; accuracy: 0.7366666666666667; loss: 2.2206273078918457\n",
      "Training epoch 7178 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7178 ; accuracy: 0.7366666666666667; loss: 2.220634698867798\n",
      "Training epoch 7179 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7179 ; accuracy: 0.7366666666666667; loss: 2.22064208984375\n",
      "Training epoch 7180 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7180 ; accuracy: 0.7366666666666667; loss: 2.2206523418426514\n",
      "Training epoch 7181 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7181 ; accuracy: 0.7366666666666667; loss: 2.2206623554229736\n",
      "Training epoch 7182 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7182 ; accuracy: 0.7366666666666667; loss: 2.2206757068634033\n",
      "Training epoch 7183 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7183 ; accuracy: 0.7366666666666667; loss: 2.220690965652466\n",
      "Training epoch 7184 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7184 ; accuracy: 0.7366666666666667; loss: 2.220705032348633\n",
      "Training epoch 7185 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7185 ; accuracy: 0.7366666666666667; loss: 2.2207188606262207\n",
      "Training epoch 7186 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7186 ; accuracy: 0.7366666666666667; loss: 2.2206969261169434\n",
      "Training epoch 7187 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7187 ; accuracy: 0.7366666666666667; loss: 2.2206525802612305\n",
      "Training epoch 7188 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7188 ; accuracy: 0.7366666666666667; loss: 2.2206060886383057\n",
      "Training epoch 7189 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7189 ; accuracy: 0.7366666666666667; loss: 2.2205653190612793\n",
      "Training epoch 7190 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7190 ; accuracy: 0.7366666666666667; loss: 2.220532178878784\n",
      "Training epoch 7191 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7191 ; accuracy: 0.7366666666666667; loss: 2.2204959392547607\n",
      "Training epoch 7192 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7192 ; accuracy: 0.7366666666666667; loss: 2.220472812652588\n",
      "Training epoch 7193 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7193 ; accuracy: 0.7366666666666667; loss: 2.2204606533050537\n",
      "Training epoch 7194 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7194 ; accuracy: 0.7366666666666667; loss: 2.2204418182373047\n",
      "Training epoch 7195 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7195 ; accuracy: 0.7366666666666667; loss: 2.220409631729126\n",
      "Training epoch 7196 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7196 ; accuracy: 0.7366666666666667; loss: 2.2203845977783203\n",
      "Training epoch 7197 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7197 ; accuracy: 0.7366666666666667; loss: 2.2203664779663086\n",
      "Training epoch 7198 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7198 ; accuracy: 0.7366666666666667; loss: 2.220345973968506\n",
      "Training epoch 7199 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7199 ; accuracy: 0.7366666666666667; loss: 2.2203288078308105\n",
      "Training epoch 7200 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7200 ; accuracy: 0.7366666666666667; loss: 2.2203128337860107\n",
      "Training epoch 7201 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7201 ; accuracy: 0.7366666666666667; loss: 2.22029447555542\n",
      "Training epoch 7202 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7202 ; accuracy: 0.7366666666666667; loss: 2.2202930450439453\n",
      "Training epoch 7203 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7203 ; accuracy: 0.7366666666666667; loss: 2.2202937602996826\n",
      "Training epoch 7204 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7204 ; accuracy: 0.7366666666666667; loss: 2.22029709815979\n",
      "Training epoch 7205 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7205 ; accuracy: 0.7366666666666667; loss: 2.2203121185302734\n",
      "Training epoch 7206 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7206 ; accuracy: 0.7366666666666667; loss: 2.22033429145813\n",
      "Training epoch 7207 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7207 ; accuracy: 0.7366666666666667; loss: 2.22035813331604\n",
      "Training epoch 7208 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7208 ; accuracy: 0.7366666666666667; loss: 2.220386266708374\n",
      "Training epoch 7209 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7209 ; accuracy: 0.7366666666666667; loss: 2.220409393310547\n",
      "Training epoch 7210 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7210 ; accuracy: 0.7366666666666667; loss: 2.2204370498657227\n",
      "Training epoch 7211 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7211 ; accuracy: 0.7366666666666667; loss: 2.2204642295837402\n",
      "Training epoch 7212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7212 ; accuracy: 0.7366666666666667; loss: 2.2204949855804443\n",
      "Training epoch 7213 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7213 ; accuracy: 0.7366666666666667; loss: 2.220527410507202\n",
      "Training epoch 7214 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7214 ; accuracy: 0.7366666666666667; loss: 2.2205569744110107\n",
      "Training epoch 7215 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7215 ; accuracy: 0.7366666666666667; loss: 2.220585584640503\n",
      "Training epoch 7216 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7216 ; accuracy: 0.7366666666666667; loss: 2.220614194869995\n",
      "Training epoch 7217 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7217 ; accuracy: 0.7366666666666667; loss: 2.2206456661224365\n",
      "Training epoch 7218 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7218 ; accuracy: 0.7366666666666667; loss: 2.220669746398926\n",
      "Training epoch 7219 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7219 ; accuracy: 0.7366666666666667; loss: 2.22070574760437\n",
      "Training epoch 7220 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7220 ; accuracy: 0.7366666666666667; loss: 2.2207367420196533\n",
      "Training epoch 7221 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7221 ; accuracy: 0.7366666666666667; loss: 2.2207846641540527\n",
      "Training epoch 7222 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7222 ; accuracy: 0.7366666666666667; loss: 2.220823049545288\n",
      "Training epoch 7223 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7223 ; accuracy: 0.7366666666666667; loss: 2.2208476066589355\n",
      "Training epoch 7224 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7224 ; accuracy: 0.7366666666666667; loss: 2.2208704948425293\n",
      "Training epoch 7225 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7225 ; accuracy: 0.7366666666666667; loss: 2.2208914756774902\n",
      "Training epoch 7226 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7226 ; accuracy: 0.7366666666666667; loss: 2.2209105491638184\n",
      "Training epoch 7227 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7227 ; accuracy: 0.7366666666666667; loss: 2.220925807952881\n",
      "Training epoch 7228 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7228 ; accuracy: 0.7366666666666667; loss: 2.2209391593933105\n",
      "Training epoch 7229 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7229 ; accuracy: 0.7366666666666667; loss: 2.220958948135376\n",
      "Training epoch 7230 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7230 ; accuracy: 0.7366666666666667; loss: 2.220966339111328\n",
      "Training epoch 7231 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7231 ; accuracy: 0.7366666666666667; loss: 2.2209689617156982\n",
      "Training epoch 7232 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7232 ; accuracy: 0.7366666666666667; loss: 2.2209837436676025\n",
      "Training epoch 7233 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7233 ; accuracy: 0.7366666666666667; loss: 2.2210047245025635\n",
      "Training epoch 7234 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7234 ; accuracy: 0.7366666666666667; loss: 2.221022367477417\n",
      "Training epoch 7235 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7235 ; accuracy: 0.7366666666666667; loss: 2.2210278511047363\n",
      "Training epoch 7236 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7236 ; accuracy: 0.7366666666666667; loss: 2.2210299968719482\n",
      "Training epoch 7237 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7237 ; accuracy: 0.7366666666666667; loss: 2.2210326194763184\n",
      "Training epoch 7238 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7238 ; accuracy: 0.7366666666666667; loss: 2.221035957336426\n",
      "Training epoch 7239 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7239 ; accuracy: 0.7366666666666667; loss: 2.2210311889648438\n",
      "Training epoch 7240 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7240 ; accuracy: 0.7366666666666667; loss: 2.2210276126861572\n",
      "Training epoch 7241 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7241 ; accuracy: 0.7366666666666667; loss: 2.2210192680358887\n",
      "Training epoch 7242 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7242 ; accuracy: 0.7366666666666667; loss: 2.2210161685943604\n",
      "Training epoch 7243 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7243 ; accuracy: 0.7366666666666667; loss: 2.2210261821746826\n",
      "Training epoch 7244 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7244 ; accuracy: 0.7366666666666667; loss: 2.221040964126587\n",
      "Training epoch 7245 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7245 ; accuracy: 0.7366666666666667; loss: 2.2210631370544434\n",
      "Training epoch 7246 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7246 ; accuracy: 0.7366666666666667; loss: 2.22109055519104\n",
      "Training epoch 7247 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7247 ; accuracy: 0.7366666666666667; loss: 2.221118211746216\n",
      "Training epoch 7248 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7248 ; accuracy: 0.7366666666666667; loss: 2.221142292022705\n",
      "Training epoch 7249 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7249 ; accuracy: 0.7366666666666667; loss: 2.221165657043457\n",
      "Training epoch 7250 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7250 ; accuracy: 0.7366666666666667; loss: 2.2211902141571045\n",
      "Training epoch 7251 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 7251 ; accuracy: 0.7366666666666667; loss: 2.221306085586548\n",
      "Training epoch 7252 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7252 ; accuracy: 0.7366666666666667; loss: 2.2213850021362305\n",
      "Training epoch 7253 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7253 ; accuracy: 0.7366666666666667; loss: 2.221458673477173\n",
      "Training epoch 7254 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7254 ; accuracy: 0.7366666666666667; loss: 2.2215335369110107\n",
      "Training epoch 7255 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7255 ; accuracy: 0.7366666666666667; loss: 2.221606969833374\n",
      "Training epoch 7256 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7256 ; accuracy: 0.7366666666666667; loss: 2.22168231010437\n",
      "Training epoch 7257 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7257 ; accuracy: 0.7366666666666667; loss: 2.221752405166626\n",
      "Training epoch 7258 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7258 ; accuracy: 0.7366666666666667; loss: 2.221818208694458\n",
      "Training epoch 7259 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 7259 ; accuracy: 0.7366666666666667; loss: 2.2219176292419434\n",
      "Training epoch 7260 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7260 ; accuracy: 0.7366666666666667; loss: 2.222010850906372\n",
      "Training epoch 7261 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7261 ; accuracy: 0.7366666666666667; loss: 2.222102165222168\n",
      "Training epoch 7262 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7262 ; accuracy: 0.7366666666666667; loss: 2.222184658050537\n",
      "Training epoch 7263 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7263 ; accuracy: 0.7366666666666667; loss: 2.222261667251587\n",
      "Training epoch 7264 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7264 ; accuracy: 0.7366666666666667; loss: 2.222332239151001\n",
      "Training epoch 7265 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7265 ; accuracy: 0.7366666666666667; loss: 2.2224032878875732\n",
      "Training epoch 7266 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7266 ; accuracy: 0.7366666666666667; loss: 2.222468852996826\n",
      "Training epoch 7267 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7267 ; accuracy: 0.7366666666666667; loss: 2.2225284576416016\n",
      "Training epoch 7268 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7268 ; accuracy: 0.7366666666666667; loss: 2.2225897312164307\n",
      "Training epoch 7269 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7269 ; accuracy: 0.7366666666666667; loss: 2.222642660140991\n",
      "Training epoch 7270 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7270 ; accuracy: 0.7366666666666667; loss: 2.2226929664611816\n",
      "Training epoch 7271 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7271 ; accuracy: 0.7366666666666667; loss: 2.2227401733398438\n",
      "Training epoch 7272 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7272 ; accuracy: 0.7366666666666667; loss: 2.222782611846924\n",
      "Training epoch 7273 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7273 ; accuracy: 0.7366666666666667; loss: 2.2228219509124756\n",
      "Training epoch 7274 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7274 ; accuracy: 0.7366666666666667; loss: 2.222860336303711\n",
      "Training epoch 7275 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7275 ; accuracy: 0.7366666666666667; loss: 2.222895860671997\n",
      "Training epoch 7276 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7276 ; accuracy: 0.7366666666666667; loss: 2.222929000854492\n",
      "Training epoch 7277 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7277 ; accuracy: 0.7366666666666667; loss: 2.222961902618408\n",
      "Training epoch 7278 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7278 ; accuracy: 0.7366666666666667; loss: 2.223005533218384\n",
      "Training epoch 7279 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7279 ; accuracy: 0.7366666666666667; loss: 2.223037004470825\n",
      "Training epoch 7280 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7280 ; accuracy: 0.7366666666666667; loss: 2.223071813583374\n",
      "Training epoch 7281 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7281 ; accuracy: 0.7366666666666667; loss: 2.223109245300293\n",
      "Training epoch 7282 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7282 ; accuracy: 0.7366666666666667; loss: 2.2231452465057373\n",
      "Training epoch 7283 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7283 ; accuracy: 0.7366666666666667; loss: 2.223177194595337\n",
      "Training epoch 7284 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7284 ; accuracy: 0.7366666666666667; loss: 2.2232072353363037\n",
      "Training epoch 7285 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7285 ; accuracy: 0.7366666666666667; loss: 2.2232377529144287\n",
      "Training epoch 7286 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7286 ; accuracy: 0.7366666666666667; loss: 2.2232630252838135\n",
      "Training epoch 7287 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7287 ; accuracy: 0.7366666666666667; loss: 2.223284959793091\n",
      "Training epoch 7288 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7288 ; accuracy: 0.7366666666666667; loss: 2.2232983112335205\n",
      "Training epoch 7289 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7289 ; accuracy: 0.7366666666666667; loss: 2.223313570022583\n",
      "Training epoch 7290 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7290 ; accuracy: 0.7366666666666667; loss: 2.2233362197875977\n",
      "Training epoch 7291 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7291 ; accuracy: 0.7366666666666667; loss: 2.2233541011810303\n",
      "Training epoch 7292 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7292 ; accuracy: 0.7366666666666667; loss: 2.223365306854248\n",
      "Training epoch 7293 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7293 ; accuracy: 0.7366666666666667; loss: 2.223374843597412\n",
      "Training epoch 7294 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7294 ; accuracy: 0.7366666666666667; loss: 2.2233850955963135\n",
      "Training epoch 7295 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7295 ; accuracy: 0.7366666666666667; loss: 2.2233996391296387\n",
      "Training epoch 7296 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7296 ; accuracy: 0.7366666666666667; loss: 2.223417282104492\n",
      "Training epoch 7297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7297 ; accuracy: 0.7366666666666667; loss: 2.2234365940093994\n",
      "Training epoch 7298 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7298 ; accuracy: 0.7366666666666667; loss: 2.2234551906585693\n",
      "Training epoch 7299 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7299 ; accuracy: 0.7366666666666667; loss: 2.223471164703369\n",
      "Training epoch 7300 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7300 ; accuracy: 0.7366666666666667; loss: 2.2234573364257812\n",
      "Training epoch 7301 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7301 ; accuracy: 0.7366666666666667; loss: 2.2234551906585693\n",
      "Training epoch 7302 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7302 ; accuracy: 0.7366666666666667; loss: 2.2234561443328857\n",
      "Training epoch 7303 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7303 ; accuracy: 0.7366666666666667; loss: 2.223461627960205\n",
      "Training epoch 7304 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7304 ; accuracy: 0.7366666666666667; loss: 2.2234692573547363\n",
      "Training epoch 7305 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7305 ; accuracy: 0.7366666666666667; loss: 2.2234795093536377\n",
      "Training epoch 7306 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7306 ; accuracy: 0.7366666666666667; loss: 2.2234787940979004\n",
      "Training epoch 7307 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7307 ; accuracy: 0.7366666666666667; loss: 2.223482608795166\n",
      "Training epoch 7308 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7308 ; accuracy: 0.7366666666666667; loss: 2.223477840423584\n",
      "Training epoch 7309 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7309 ; accuracy: 0.7366666666666667; loss: 2.2234740257263184\n",
      "Training epoch 7310 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7310 ; accuracy: 0.7366666666666667; loss: 2.223473072052002\n",
      "Training epoch 7311 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7311 ; accuracy: 0.7366666666666667; loss: 2.2234747409820557\n",
      "Training epoch 7312 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7312 ; accuracy: 0.7366666666666667; loss: 2.2234718799591064\n",
      "Training epoch 7313 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7313 ; accuracy: 0.7366666666666667; loss: 2.223468780517578\n",
      "Training epoch 7314 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7314 ; accuracy: 0.7366666666666667; loss: 2.223466634750366\n",
      "Training epoch 7315 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7315 ; accuracy: 0.7366666666666667; loss: 2.22347092628479\n",
      "Training epoch 7316 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7316 ; accuracy: 0.7366666666666667; loss: 2.223480701446533\n",
      "Training epoch 7317 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7317 ; accuracy: 0.7366666666666667; loss: 2.223487138748169\n",
      "Training epoch 7318 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7318 ; accuracy: 0.7366666666666667; loss: 2.2234909534454346\n",
      "Training epoch 7319 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7319 ; accuracy: 0.7366666666666667; loss: 2.2235050201416016\n",
      "Training epoch 7320 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7320 ; accuracy: 0.7366666666666667; loss: 2.2235164642333984\n",
      "Training epoch 7321 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7321 ; accuracy: 0.7366666666666667; loss: 2.2235188484191895\n",
      "Training epoch 7322 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7322 ; accuracy: 0.7366666666666667; loss: 2.2235264778137207\n",
      "Training epoch 7323 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7323 ; accuracy: 0.7366666666666667; loss: 2.22352933883667\n",
      "Training epoch 7324 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7324 ; accuracy: 0.7366666666666667; loss: 2.2235302925109863\n",
      "Training epoch 7325 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7325 ; accuracy: 0.7366666666666667; loss: 2.2235333919525146\n",
      "Training epoch 7326 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7326 ; accuracy: 0.7366666666666667; loss: 2.223539352416992\n",
      "Training epoch 7327 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7327 ; accuracy: 0.7366666666666667; loss: 2.2235512733459473\n",
      "Training epoch 7328 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7328 ; accuracy: 0.7366666666666667; loss: 2.2235546112060547\n",
      "Training epoch 7329 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7329 ; accuracy: 0.7366666666666667; loss: 2.223569631576538\n",
      "Training epoch 7330 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7330 ; accuracy: 0.7366666666666667; loss: 2.2235891819000244\n",
      "Training epoch 7331 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7331 ; accuracy: 0.7366666666666667; loss: 2.2236011028289795\n",
      "Training epoch 7332 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7332 ; accuracy: 0.7366666666666667; loss: 2.2236075401306152\n",
      "Training epoch 7333 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7333 ; accuracy: 0.7366666666666667; loss: 2.2236125469207764\n",
      "Training epoch 7334 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7334 ; accuracy: 0.7366666666666667; loss: 2.223623037338257\n",
      "Training epoch 7335 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7335 ; accuracy: 0.7366666666666667; loss: 2.223628044128418\n",
      "Training epoch 7336 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7336 ; accuracy: 0.7366666666666667; loss: 2.2236392498016357\n",
      "Training epoch 7337 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7337 ; accuracy: 0.7366666666666667; loss: 2.2236483097076416\n",
      "Training epoch 7338 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7338 ; accuracy: 0.7366666666666667; loss: 2.223658561706543\n",
      "Training epoch 7339 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7339 ; accuracy: 0.7366666666666667; loss: 2.223665952682495\n",
      "Training epoch 7340 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7340 ; accuracy: 0.7366666666666667; loss: 2.223686933517456\n",
      "Training epoch 7341 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7341 ; accuracy: 0.7366666666666667; loss: 2.2237093448638916\n",
      "Training epoch 7342 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7342 ; accuracy: 0.7366666666666667; loss: 2.2237260341644287\n",
      "Training epoch 7343 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7343 ; accuracy: 0.7366666666666667; loss: 2.2237393856048584\n",
      "Training epoch 7344 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7344 ; accuracy: 0.7366666666666667; loss: 2.223741292953491\n",
      "Training epoch 7345 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7345 ; accuracy: 0.7366666666666667; loss: 2.2237443923950195\n",
      "Training epoch 7346 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7346 ; accuracy: 0.7366666666666667; loss: 2.223742961883545\n",
      "Training epoch 7347 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7347 ; accuracy: 0.7366666666666667; loss: 2.223745822906494\n",
      "Training epoch 7348 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7348 ; accuracy: 0.7366666666666667; loss: 2.223742723464966\n",
      "Training epoch 7349 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7349 ; accuracy: 0.7366666666666667; loss: 2.223742961883545\n",
      "Training epoch 7350 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7350 ; accuracy: 0.7366666666666667; loss: 2.223728656768799\n",
      "Training epoch 7351 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7351 ; accuracy: 0.7366666666666667; loss: 2.2237064838409424\n",
      "Training epoch 7352 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 7352 ; accuracy: 0.7366666666666667; loss: 2.223695993423462\n",
      "Training epoch 7353 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7353 ; accuracy: 0.7366666666666667; loss: 2.2236902713775635\n",
      "Training epoch 7354 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7354 ; accuracy: 0.7366666666666667; loss: 2.2236902713775635\n",
      "Training epoch 7355 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7355 ; accuracy: 0.7366666666666667; loss: 2.2236993312835693\n",
      "Training epoch 7356 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7356 ; accuracy: 0.7366666666666667; loss: 2.2237088680267334\n",
      "Training epoch 7357 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7357 ; accuracy: 0.7366666666666667; loss: 2.2237207889556885\n",
      "Training epoch 7358 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7358 ; accuracy: 0.7366666666666667; loss: 2.223736524581909\n",
      "Training epoch 7359 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7359 ; accuracy: 0.7366666666666667; loss: 2.2237517833709717\n",
      "Training epoch 7360 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7360 ; accuracy: 0.7366666666666667; loss: 2.223766565322876\n",
      "Training epoch 7361 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7361 ; accuracy: 0.7366666666666667; loss: 2.2237906455993652\n",
      "Training epoch 7362 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7362 ; accuracy: 0.7366666666666667; loss: 2.22381591796875\n",
      "Training epoch 7363 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7363 ; accuracy: 0.7366666666666667; loss: 2.223845958709717\n",
      "Training epoch 7364 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7364 ; accuracy: 0.7366666666666667; loss: 2.2238805294036865\n",
      "Training epoch 7365 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7365 ; accuracy: 0.7366666666666667; loss: 2.2239067554473877\n",
      "Training epoch 7366 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7366 ; accuracy: 0.7366666666666667; loss: 2.223924398422241\n",
      "Training epoch 7367 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7367 ; accuracy: 0.7366666666666667; loss: 2.2239387035369873\n",
      "Training epoch 7368 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7368 ; accuracy: 0.7366666666666667; loss: 2.223956823348999\n",
      "Training epoch 7369 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7369 ; accuracy: 0.7366666666666667; loss: 2.2239885330200195\n",
      "Training epoch 7370 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7370 ; accuracy: 0.7366666666666667; loss: 2.2240161895751953\n",
      "Training epoch 7371 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7371 ; accuracy: 0.7366666666666667; loss: 2.2240376472473145\n",
      "Training epoch 7372 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7372 ; accuracy: 0.7366666666666667; loss: 2.2240543365478516\n",
      "Training epoch 7373 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7373 ; accuracy: 0.7366666666666667; loss: 2.2240753173828125\n",
      "Training epoch 7374 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7374 ; accuracy: 0.7366666666666667; loss: 2.224093198776245\n",
      "Training epoch 7375 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7375 ; accuracy: 0.7366666666666667; loss: 2.224106788635254\n",
      "Training epoch 7376 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7376 ; accuracy: 0.7366666666666667; loss: 2.224118947982788\n",
      "Training epoch 7377 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7377 ; accuracy: 0.7366666666666667; loss: 2.2241370677948\n",
      "Training epoch 7378 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7378 ; accuracy: 0.7366666666666667; loss: 2.224167585372925\n",
      "Training epoch 7379 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7379 ; accuracy: 0.7366666666666667; loss: 2.224200963973999\n",
      "Training epoch 7380 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7380 ; accuracy: 0.7366666666666667; loss: 2.224231481552124\n",
      "Training epoch 7381 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7381 ; accuracy: 0.7366666666666667; loss: 2.2242465019226074\n",
      "Training epoch 7382 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7382 ; accuracy: 0.7366666666666667; loss: 2.2242655754089355\n",
      "Training epoch 7383 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7383 ; accuracy: 0.7366666666666667; loss: 2.2242889404296875\n",
      "Training epoch 7384 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7384 ; accuracy: 0.7366666666666667; loss: 2.224303722381592\n",
      "Training epoch 7385 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7385 ; accuracy: 0.7366666666666667; loss: 2.2243192195892334\n",
      "Training epoch 7386 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7386 ; accuracy: 0.7366666666666667; loss: 2.2243306636810303\n",
      "Training epoch 7387 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7387 ; accuracy: 0.7366666666666667; loss: 2.224346399307251\n",
      "Training epoch 7388 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7388 ; accuracy: 0.7366666666666667; loss: 2.224360466003418\n",
      "Training epoch 7389 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7389 ; accuracy: 0.7366666666666667; loss: 2.2243683338165283\n",
      "Training epoch 7390 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7390 ; accuracy: 0.7366666666666667; loss: 2.2243812084198\n",
      "Training epoch 7391 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7391 ; accuracy: 0.7366666666666667; loss: 2.224367380142212\n",
      "Training epoch 7392 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7392 ; accuracy: 0.7366666666666667; loss: 2.224351644515991\n",
      "Training epoch 7393 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7393 ; accuracy: 0.7366666666666667; loss: 2.2243263721466064\n",
      "Training epoch 7394 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7394 ; accuracy: 0.7366666666666667; loss: 2.224309206008911\n",
      "Training epoch 7395 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7395 ; accuracy: 0.7366666666666667; loss: 2.2242910861968994\n",
      "Training epoch 7396 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7396 ; accuracy: 0.7366666666666667; loss: 2.2242743968963623\n",
      "Training epoch 7397 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7397 ; accuracy: 0.7366666666666667; loss: 2.2242562770843506\n",
      "Training epoch 7398 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7398 ; accuracy: 0.7366666666666667; loss: 2.224240779876709\n",
      "Training epoch 7399 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7399 ; accuracy: 0.7366666666666667; loss: 2.2242281436920166\n",
      "Training epoch 7400 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7400 ; accuracy: 0.7366666666666667; loss: 2.2242226600646973\n",
      "Training epoch 7401 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7401 ; accuracy: 0.7366666666666667; loss: 2.224233627319336\n",
      "Training epoch 7402 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7402 ; accuracy: 0.7366666666666667; loss: 2.224252700805664\n",
      "Training epoch 7403 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7403 ; accuracy: 0.7366666666666667; loss: 2.2242748737335205\n",
      "Training epoch 7404 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7404 ; accuracy: 0.7366666666666667; loss: 2.2243003845214844\n",
      "Training epoch 7405 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7405 ; accuracy: 0.7366666666666667; loss: 2.2243428230285645\n",
      "Training epoch 7406 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7406 ; accuracy: 0.7366666666666667; loss: 2.2243826389312744\n",
      "Training epoch 7407 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7407 ; accuracy: 0.7366666666666667; loss: 2.2244131565093994\n",
      "Training epoch 7408 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7408 ; accuracy: 0.7366666666666667; loss: 2.224376916885376\n",
      "Training epoch 7409 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7409 ; accuracy: 0.7366666666666667; loss: 2.224343776702881\n",
      "Training epoch 7410 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7410 ; accuracy: 0.7366666666666667; loss: 2.2243170738220215\n",
      "Training epoch 7411 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7411 ; accuracy: 0.7366666666666667; loss: 2.2243030071258545\n",
      "Training epoch 7412 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7412 ; accuracy: 0.7366666666666667; loss: 2.224283456802368\n",
      "Training epoch 7413 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7413 ; accuracy: 0.7366666666666667; loss: 2.2242655754089355\n",
      "Training epoch 7414 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7414 ; accuracy: 0.7366666666666667; loss: 2.2242515087127686\n",
      "Training epoch 7415 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7415 ; accuracy: 0.7366666666666667; loss: 2.2242417335510254\n",
      "Training epoch 7416 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7416 ; accuracy: 0.7366666666666667; loss: 2.224214792251587\n",
      "Training epoch 7417 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7417 ; accuracy: 0.7366666666666667; loss: 2.2241907119750977\n",
      "Training epoch 7418 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7418 ; accuracy: 0.7366666666666667; loss: 2.224170684814453\n",
      "Training epoch 7419 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7419 ; accuracy: 0.7366666666666667; loss: 2.2241599559783936\n",
      "Training epoch 7420 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7420 ; accuracy: 0.7366666666666667; loss: 2.2241604328155518\n",
      "Training epoch 7421 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7421 ; accuracy: 0.7366666666666667; loss: 2.224170446395874\n",
      "Training epoch 7422 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7422 ; accuracy: 0.7366666666666667; loss: 2.2241928577423096\n",
      "Training epoch 7423 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7423 ; accuracy: 0.7366666666666667; loss: 2.224203586578369\n",
      "Training epoch 7424 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7424 ; accuracy: 0.7366666666666667; loss: 2.224214792251587\n",
      "Training epoch 7425 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7425 ; accuracy: 0.7366666666666667; loss: 2.224226474761963\n",
      "Training epoch 7426 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7426 ; accuracy: 0.7366666666666667; loss: 2.2242393493652344\n",
      "Training epoch 7427 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7427 ; accuracy: 0.7366666666666667; loss: 2.224261522293091\n",
      "Training epoch 7428 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7428 ; accuracy: 0.7366666666666667; loss: 2.2242798805236816\n",
      "Training epoch 7429 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7429 ; accuracy: 0.7366666666666667; loss: 2.2242910861968994\n",
      "Training epoch 7430 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7430 ; accuracy: 0.7366666666666667; loss: 2.224303722381592\n",
      "Training epoch 7431 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7431 ; accuracy: 0.7366666666666667; loss: 2.2243223190307617\n",
      "Training epoch 7432 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7432 ; accuracy: 0.7366666666666667; loss: 2.2243432998657227\n",
      "Training epoch 7433 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7433 ; accuracy: 0.7366666666666667; loss: 2.2243669033050537\n",
      "Training epoch 7434 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7434 ; accuracy: 0.7366666666666667; loss: 2.224385976791382\n",
      "Training epoch 7435 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7435 ; accuracy: 0.7366666666666667; loss: 2.2243635654449463\n",
      "Training epoch 7436 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7436 ; accuracy: 0.7366666666666667; loss: 2.2243576049804688\n",
      "Training epoch 7437 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7437 ; accuracy: 0.7366666666666667; loss: 2.224360704421997\n",
      "Training epoch 7438 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7438 ; accuracy: 0.7366666666666667; loss: 2.2243666648864746\n",
      "Training epoch 7439 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7439 ; accuracy: 0.7366666666666667; loss: 2.224374532699585\n",
      "Training epoch 7440 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7440 ; accuracy: 0.7366666666666667; loss: 2.2243757247924805\n",
      "Training epoch 7441 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7441 ; accuracy: 0.7366666666666667; loss: 2.2243824005126953\n",
      "Training epoch 7442 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7442 ; accuracy: 0.7366666666666667; loss: 2.2243881225585938\n",
      "Training epoch 7443 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7443 ; accuracy: 0.7366666666666667; loss: 2.2244019508361816\n",
      "Training epoch 7444 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7444 ; accuracy: 0.7366666666666667; loss: 2.22441029548645\n",
      "Training epoch 7445 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7445 ; accuracy: 0.7366666666666667; loss: 2.224414587020874\n",
      "Training epoch 7446 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7446 ; accuracy: 0.7366666666666667; loss: 2.224428653717041\n",
      "Training epoch 7447 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7447 ; accuracy: 0.7366666666666667; loss: 2.224447727203369\n",
      "Training epoch 7448 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7448 ; accuracy: 0.7366666666666667; loss: 2.2244672775268555\n",
      "Training epoch 7449 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7449 ; accuracy: 0.7366666666666667; loss: 2.2244937419891357\n",
      "Training epoch 7450 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7450 ; accuracy: 0.7366666666666667; loss: 2.2245073318481445\n",
      "Training epoch 7451 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7451 ; accuracy: 0.7366666666666667; loss: 2.224525213241577\n",
      "Training epoch 7452 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7452 ; accuracy: 0.7366666666666667; loss: 2.2245421409606934\n",
      "Training epoch 7453 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7453 ; accuracy: 0.7366666666666667; loss: 2.2245519161224365\n",
      "Training epoch 7454 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7454 ; accuracy: 0.7366666666666667; loss: 2.2245662212371826\n",
      "Training epoch 7455 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7455 ; accuracy: 0.7366666666666667; loss: 2.2245864868164062\n",
      "Training epoch 7456 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7456 ; accuracy: 0.7366666666666667; loss: 2.2246134281158447\n",
      "Training epoch 7457 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7457 ; accuracy: 0.7366666666666667; loss: 2.224640130996704\n",
      "Training epoch 7458 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7458 ; accuracy: 0.7366666666666667; loss: 2.2246618270874023\n",
      "Training epoch 7459 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7459 ; accuracy: 0.7366666666666667; loss: 2.2246949672698975\n",
      "Training epoch 7460 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7460 ; accuracy: 0.7366666666666667; loss: 2.2247250080108643\n",
      "Training epoch 7461 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7461 ; accuracy: 0.7366666666666667; loss: 2.2247493267059326\n",
      "Training epoch 7462 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7462 ; accuracy: 0.7366666666666667; loss: 2.224780321121216\n",
      "Training epoch 7463 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7463 ; accuracy: 0.7366666666666667; loss: 2.2248053550720215\n",
      "Training epoch 7464 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7464 ; accuracy: 0.7366666666666667; loss: 2.224841833114624\n",
      "Training epoch 7465 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7465 ; accuracy: 0.7366666666666667; loss: 2.2248785495758057\n",
      "Training epoch 7466 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 7466 ; accuracy: 0.7366666666666667; loss: 2.2250232696533203\n",
      "Training epoch 7467 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7467 ; accuracy: 0.7366666666666667; loss: 2.2251577377319336\n",
      "Training epoch 7468 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7468 ; accuracy: 0.7366666666666667; loss: 2.2252955436706543\n",
      "Training epoch 7469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7469 ; accuracy: 0.7366666666666667; loss: 2.2254252433776855\n",
      "Training epoch 7470 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7470 ; accuracy: 0.7366666666666667; loss: 2.2255444526672363\n",
      "Training epoch 7471 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7471 ; accuracy: 0.7366666666666667; loss: 2.2256579399108887\n",
      "Training epoch 7472 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7472 ; accuracy: 0.7366666666666667; loss: 2.2257816791534424\n",
      "Training epoch 7473 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7473 ; accuracy: 0.7366666666666667; loss: 2.225900173187256\n",
      "Training epoch 7474 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7474 ; accuracy: 0.7366666666666667; loss: 2.226013422012329\n",
      "Training epoch 7475 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7475 ; accuracy: 0.7366666666666667; loss: 2.2261109352111816\n",
      "Training epoch 7476 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7476 ; accuracy: 0.7366666666666667; loss: 2.226200819015503\n",
      "Training epoch 7477 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7477 ; accuracy: 0.7366666666666667; loss: 2.2262604236602783\n",
      "Training epoch 7478 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7478 ; accuracy: 0.7366666666666667; loss: 2.2263107299804688\n",
      "Training epoch 7479 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7479 ; accuracy: 0.7366666666666667; loss: 2.2263619899749756\n",
      "Training epoch 7480 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7480 ; accuracy: 0.7366666666666667; loss: 2.226407766342163\n",
      "Training epoch 7481 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7481 ; accuracy: 0.7366666666666667; loss: 2.226449728012085\n",
      "Training epoch 7482 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7482 ; accuracy: 0.7366666666666667; loss: 2.2264862060546875\n",
      "Training epoch 7483 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7483 ; accuracy: 0.7366666666666667; loss: 2.226517677307129\n",
      "Training epoch 7484 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7484 ; accuracy: 0.7366666666666667; loss: 2.2265470027923584\n",
      "Training epoch 7485 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7485 ; accuracy: 0.7366666666666667; loss: 2.226577043533325\n",
      "Training epoch 7486 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7486 ; accuracy: 0.7366666666666667; loss: 2.226595878601074\n",
      "Training epoch 7487 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7487 ; accuracy: 0.7366666666666667; loss: 2.2266151905059814\n",
      "Training epoch 7488 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7488 ; accuracy: 0.7366666666666667; loss: 2.226637840270996\n",
      "Training epoch 7489 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7489 ; accuracy: 0.7366666666666667; loss: 2.2266652584075928\n",
      "Training epoch 7490 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7490 ; accuracy: 0.7366666666666667; loss: 2.226694345474243\n",
      "Training epoch 7491 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7491 ; accuracy: 0.7366666666666667; loss: 2.2267072200775146\n",
      "Training epoch 7492 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7492 ; accuracy: 0.7366666666666667; loss: 2.2267231941223145\n",
      "Training epoch 7493 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7493 ; accuracy: 0.7366666666666667; loss: 2.226733922958374\n",
      "Training epoch 7494 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7494 ; accuracy: 0.7366666666666667; loss: 2.2267463207244873\n",
      "Training epoch 7495 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7495 ; accuracy: 0.7366666666666667; loss: 2.2267537117004395\n",
      "Training epoch 7496 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7496 ; accuracy: 0.7366666666666667; loss: 2.2267613410949707\n",
      "Training epoch 7497 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7497 ; accuracy: 0.7366666666666667; loss: 2.226767063140869\n",
      "Training epoch 7498 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7498 ; accuracy: 0.7366666666666667; loss: 2.2267777919769287\n",
      "Training epoch 7499 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7499 ; accuracy: 0.7366666666666667; loss: 2.2267863750457764\n",
      "Training epoch 7500 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7500 ; accuracy: 0.7366666666666667; loss: 2.2267866134643555\n",
      "Training epoch 7501 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7501 ; accuracy: 0.7366666666666667; loss: 2.226792812347412\n",
      "Training epoch 7502 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7502 ; accuracy: 0.7366666666666667; loss: 2.2267777919769287\n",
      "Training epoch 7503 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7503 ; accuracy: 0.7366666666666667; loss: 2.226755380630493\n",
      "Training epoch 7504 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7504 ; accuracy: 0.7366666666666667; loss: 2.2267379760742188\n",
      "Training epoch 7505 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7505 ; accuracy: 0.7366666666666667; loss: 2.2267165184020996\n",
      "Training epoch 7506 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7506 ; accuracy: 0.7366666666666667; loss: 2.226700782775879\n",
      "Training epoch 7507 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7507 ; accuracy: 0.7366666666666667; loss: 2.226686954498291\n",
      "Training epoch 7508 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7508 ; accuracy: 0.7366666666666667; loss: 2.2266809940338135\n",
      "Training epoch 7509 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7509 ; accuracy: 0.7366666666666667; loss: 2.2266809940338135\n",
      "Training epoch 7510 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7510 ; accuracy: 0.7366666666666667; loss: 2.226677656173706\n",
      "Training epoch 7511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7511 ; accuracy: 0.7366666666666667; loss: 2.2266881465911865\n",
      "Training epoch 7512 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7512 ; accuracy: 0.7366666666666667; loss: 2.226699113845825\n",
      "Training epoch 7513 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7513 ; accuracy: 0.7366666666666667; loss: 2.2266979217529297\n",
      "Training epoch 7514 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7514 ; accuracy: 0.7366666666666667; loss: 2.2266921997070312\n",
      "Training epoch 7515 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7515 ; accuracy: 0.7366666666666667; loss: 2.2266910076141357\n",
      "Training epoch 7516 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7516 ; accuracy: 0.7366666666666667; loss: 2.226684808731079\n",
      "Training epoch 7517 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7517 ; accuracy: 0.7366666666666667; loss: 2.2266852855682373\n",
      "Training epoch 7518 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7518 ; accuracy: 0.7366666666666667; loss: 2.2266898155212402\n",
      "Training epoch 7519 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7519 ; accuracy: 0.7366666666666667; loss: 2.2266902923583984\n",
      "Training epoch 7520 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7520 ; accuracy: 0.7366666666666667; loss: 2.226679801940918\n",
      "Training epoch 7521 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7521 ; accuracy: 0.7366666666666667; loss: 2.2266855239868164\n",
      "Training epoch 7522 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7522 ; accuracy: 0.7366666666666667; loss: 2.2266502380371094\n",
      "Training epoch 7523 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7523 ; accuracy: 0.7366666666666667; loss: 2.2266271114349365\n",
      "Training epoch 7524 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7524 ; accuracy: 0.7366666666666667; loss: 2.2266125679016113\n",
      "Training epoch 7525 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7525 ; accuracy: 0.7366666666666667; loss: 2.226588726043701\n",
      "Training epoch 7526 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7526 ; accuracy: 0.7366666666666667; loss: 2.2265689373016357\n",
      "Training epoch 7527 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7527 ; accuracy: 0.7366666666666667; loss: 2.226547956466675\n",
      "Training epoch 7528 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 7528 ; accuracy: 0.7366666666666667; loss: 2.22668194770813\n",
      "Training epoch 7529 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7529 ; accuracy: 0.7366666666666667; loss: 2.2268025875091553\n",
      "Training epoch 7530 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7530 ; accuracy: 0.7366666666666667; loss: 2.2269182205200195\n",
      "Training epoch 7531 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7531 ; accuracy: 0.7366666666666667; loss: 2.227017402648926\n",
      "Training epoch 7532 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7532 ; accuracy: 0.7366666666666667; loss: 2.227105140686035\n",
      "Training epoch 7533 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7533 ; accuracy: 0.7366666666666667; loss: 2.2271900177001953\n",
      "Training epoch 7534 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7534 ; accuracy: 0.7366666666666667; loss: 2.2272562980651855\n",
      "Training epoch 7535 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7535 ; accuracy: 0.7366666666666667; loss: 2.2273128032684326\n",
      "Training epoch 7536 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7536 ; accuracy: 0.7366666666666667; loss: 2.2273643016815186\n",
      "Training epoch 7537 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7537 ; accuracy: 0.7366666666666667; loss: 2.227414846420288\n",
      "Training epoch 7538 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7538 ; accuracy: 0.7366666666666667; loss: 2.2274608612060547\n",
      "Training epoch 7539 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7539 ; accuracy: 0.7366666666666667; loss: 2.2275028228759766\n",
      "Training epoch 7540 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7540 ; accuracy: 0.7366666666666667; loss: 2.2275381088256836\n",
      "Training epoch 7541 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7541 ; accuracy: 0.7366666666666667; loss: 2.227578639984131\n",
      "Training epoch 7542 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7542 ; accuracy: 0.7366666666666667; loss: 2.227616548538208\n",
      "Training epoch 7543 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7543 ; accuracy: 0.7366666666666667; loss: 2.227646827697754\n",
      "Training epoch 7544 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7544 ; accuracy: 0.7366666666666667; loss: 2.2276711463928223\n",
      "Training epoch 7545 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7545 ; accuracy: 0.7366666666666667; loss: 2.227703332901001\n",
      "Training epoch 7546 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7546 ; accuracy: 0.7366666666666667; loss: 2.227724313735962\n",
      "Training epoch 7547 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7547 ; accuracy: 0.7366666666666667; loss: 2.2277538776397705\n",
      "Training epoch 7548 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7548 ; accuracy: 0.7366666666666667; loss: 2.227782964706421\n",
      "Training epoch 7549 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7549 ; accuracy: 0.7366666666666667; loss: 2.2278149127960205\n",
      "Training epoch 7550 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7550 ; accuracy: 0.7366666666666667; loss: 2.22784686088562\n",
      "Training epoch 7551 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7551 ; accuracy: 0.7366666666666667; loss: 2.227872848510742\n",
      "Training epoch 7552 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7552 ; accuracy: 0.7366666666666667; loss: 2.2278952598571777\n",
      "Training epoch 7553 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7553 ; accuracy: 0.7366666666666667; loss: 2.2279162406921387\n",
      "Training epoch 7554 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7554 ; accuracy: 0.7366666666666667; loss: 2.227926254272461\n",
      "Training epoch 7555 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7555 ; accuracy: 0.7366666666666667; loss: 2.227944850921631\n",
      "Training epoch 7556 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7556 ; accuracy: 0.7366666666666667; loss: 2.227959394454956\n",
      "Training epoch 7557 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7557 ; accuracy: 0.7366666666666667; loss: 2.227983236312866\n",
      "Training epoch 7558 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7558 ; accuracy: 0.7366666666666667; loss: 2.228006601333618\n",
      "Training epoch 7559 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7559 ; accuracy: 0.7366666666666667; loss: 2.2280280590057373\n",
      "Training epoch 7560 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7560 ; accuracy: 0.7366666666666667; loss: 2.228044271469116\n",
      "Training epoch 7561 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7561 ; accuracy: 0.7366666666666667; loss: 2.228062152862549\n",
      "Training epoch 7562 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7562 ; accuracy: 0.7366666666666667; loss: 2.228076934814453\n",
      "Training epoch 7563 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7563 ; accuracy: 0.7366666666666667; loss: 2.228102922439575\n",
      "Training epoch 7564 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7564 ; accuracy: 0.7366666666666667; loss: 2.2281250953674316\n",
      "Training epoch 7565 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7565 ; accuracy: 0.7366666666666667; loss: 2.2281389236450195\n",
      "Training epoch 7566 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7566 ; accuracy: 0.7366666666666667; loss: 2.2281320095062256\n",
      "Training epoch 7567 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7567 ; accuracy: 0.7366666666666667; loss: 2.228116989135742\n",
      "Training epoch 7568 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7568 ; accuracy: 0.7366666666666667; loss: 2.228097438812256\n",
      "Training epoch 7569 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7569 ; accuracy: 0.7366666666666667; loss: 2.2280900478363037\n",
      "Training epoch 7570 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7570 ; accuracy: 0.7366666666666667; loss: 2.2280845642089844\n",
      "Training epoch 7571 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7571 ; accuracy: 0.7366666666666667; loss: 2.228073835372925\n",
      "Training epoch 7572 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7572 ; accuracy: 0.7366666666666667; loss: 2.228062629699707\n",
      "Training epoch 7573 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7573 ; accuracy: 0.7366666666666667; loss: 2.2280490398406982\n",
      "Training epoch 7574 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7574 ; accuracy: 0.7366666666666667; loss: 2.2280330657958984\n",
      "Training epoch 7575 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7575 ; accuracy: 0.7366666666666667; loss: 2.228019952774048\n",
      "Training epoch 7576 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7576 ; accuracy: 0.7366666666666667; loss: 2.2280123233795166\n",
      "Training epoch 7577 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7577 ; accuracy: 0.7366666666666667; loss: 2.228001356124878\n",
      "Training epoch 7578 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7578 ; accuracy: 0.7366666666666667; loss: 2.2279837131500244\n",
      "Training epoch 7579 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7579 ; accuracy: 0.7366666666666667; loss: 2.2279698848724365\n",
      "Training epoch 7580 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7580 ; accuracy: 0.7366666666666667; loss: 2.227959394454956\n",
      "Training epoch 7581 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7581 ; accuracy: 0.7366666666666667; loss: 2.2279486656188965\n",
      "Training epoch 7582 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7582 ; accuracy: 0.7366666666666667; loss: 2.227938413619995\n",
      "Training epoch 7583 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7583 ; accuracy: 0.7366666666666667; loss: 2.2279226779937744\n",
      "Training epoch 7584 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7584 ; accuracy: 0.7366666666666667; loss: 2.2279212474823\n",
      "Training epoch 7585 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7585 ; accuracy: 0.7366666666666667; loss: 2.2279157638549805\n",
      "Training epoch 7586 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7586 ; accuracy: 0.7366666666666667; loss: 2.2279045581817627\n",
      "Training epoch 7587 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7587 ; accuracy: 0.7366666666666667; loss: 2.227898120880127\n",
      "Training epoch 7588 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7588 ; accuracy: 0.7366666666666667; loss: 2.2279040813446045\n",
      "Training epoch 7589 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7589 ; accuracy: 0.7366666666666667; loss: 2.2278971672058105\n",
      "Training epoch 7590 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7590 ; accuracy: 0.7366666666666667; loss: 2.227902889251709\n",
      "Training epoch 7591 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7591 ; accuracy: 0.7366666666666667; loss: 2.227907180786133\n",
      "Training epoch 7592 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7592 ; accuracy: 0.7366666666666667; loss: 2.227915048599243\n",
      "Training epoch 7593 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7593 ; accuracy: 0.7366666666666667; loss: 2.2279317378997803\n",
      "Training epoch 7594 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7594 ; accuracy: 0.7366666666666667; loss: 2.227944850921631\n",
      "Training epoch 7595 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7595 ; accuracy: 0.7366666666666667; loss: 2.2279529571533203\n",
      "Training epoch 7596 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7596 ; accuracy: 0.7366666666666667; loss: 2.227963924407959\n",
      "Training epoch 7597 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7597 ; accuracy: 0.7366666666666667; loss: 2.2279717922210693\n",
      "Training epoch 7598 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7598 ; accuracy: 0.7366666666666667; loss: 2.2279767990112305\n",
      "Training epoch 7599 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7599 ; accuracy: 0.7366666666666667; loss: 2.227979898452759\n",
      "Training epoch 7600 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7600 ; accuracy: 0.7366666666666667; loss: 2.227982997894287\n",
      "Training epoch 7601 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7601 ; accuracy: 0.7366666666666667; loss: 2.2279906272888184\n",
      "Training epoch 7602 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7602 ; accuracy: 0.7366666666666667; loss: 2.2279953956604004\n",
      "Training epoch 7603 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7603 ; accuracy: 0.7366666666666667; loss: 2.2280056476593018\n",
      "Training epoch 7604 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7604 ; accuracy: 0.7366666666666667; loss: 2.228018283843994\n",
      "Training epoch 7605 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7605 ; accuracy: 0.7366666666666667; loss: 2.228029489517212\n",
      "Training epoch 7606 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7606 ; accuracy: 0.7366666666666667; loss: 2.2280538082122803\n",
      "Training epoch 7607 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7607 ; accuracy: 0.7366666666666667; loss: 2.2280759811401367\n",
      "Training epoch 7608 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7608 ; accuracy: 0.7366666666666667; loss: 2.228105068206787\n",
      "Training epoch 7609 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7609 ; accuracy: 0.7366666666666667; loss: 2.228130578994751\n",
      "Training epoch 7610 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7610 ; accuracy: 0.7366666666666667; loss: 2.2281529903411865\n",
      "Training epoch 7611 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7611 ; accuracy: 0.7366666666666667; loss: 2.228168249130249\n",
      "Training epoch 7612 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7612 ; accuracy: 0.7366666666666667; loss: 2.228212833404541\n",
      "Training epoch 7613 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7613 ; accuracy: 0.7366666666666667; loss: 2.2282469272613525\n",
      "Training epoch 7614 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7614 ; accuracy: 0.7366666666666667; loss: 2.2282707691192627\n",
      "Training epoch 7615 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7615 ; accuracy: 0.7366666666666667; loss: 2.22829270362854\n",
      "Training epoch 7616 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7616 ; accuracy: 0.7366666666666667; loss: 2.2283198833465576\n",
      "Training epoch 7617 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7617 ; accuracy: 0.7366666666666667; loss: 2.2283456325531006\n",
      "Training epoch 7618 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7618 ; accuracy: 0.7366666666666667; loss: 2.2283687591552734\n",
      "Training epoch 7619 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7619 ; accuracy: 0.7366666666666667; loss: 2.2283875942230225\n",
      "Training epoch 7620 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7620 ; accuracy: 0.7366666666666667; loss: 2.228408098220825\n",
      "Training epoch 7621 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7621 ; accuracy: 0.7366666666666667; loss: 2.2284345626831055\n",
      "Training epoch 7622 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7622 ; accuracy: 0.7366666666666667; loss: 2.228450298309326\n",
      "Training epoch 7623 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7623 ; accuracy: 0.7366666666666667; loss: 2.228468894958496\n",
      "Training epoch 7624 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7624 ; accuracy: 0.7366666666666667; loss: 2.228496551513672\n",
      "Training epoch 7625 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7625 ; accuracy: 0.7366666666666667; loss: 2.228522539138794\n",
      "Training epoch 7626 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7626 ; accuracy: 0.7366666666666667; loss: 2.2285397052764893\n",
      "Training epoch 7627 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7627 ; accuracy: 0.7366666666666667; loss: 2.228553533554077\n",
      "Training epoch 7628 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7628 ; accuracy: 0.7366666666666667; loss: 2.228571891784668\n",
      "Training epoch 7629 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7629 ; accuracy: 0.7366666666666667; loss: 2.2285869121551514\n",
      "Training epoch 7630 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7630 ; accuracy: 0.7366666666666667; loss: 2.228590965270996\n",
      "Training epoch 7631 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7631 ; accuracy: 0.7366666666666667; loss: 2.228597640991211\n",
      "Training epoch 7632 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7632 ; accuracy: 0.7366666666666667; loss: 2.228605031967163\n",
      "Training epoch 7633 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7633 ; accuracy: 0.7366666666666667; loss: 2.228628635406494\n",
      "Training epoch 7634 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7634 ; accuracy: 0.7366666666666667; loss: 2.2286486625671387\n",
      "Training epoch 7635 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7635 ; accuracy: 0.7366666666666667; loss: 2.2286665439605713\n",
      "Training epoch 7636 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7636 ; accuracy: 0.7366666666666667; loss: 2.228685140609741\n",
      "Training epoch 7637 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7637 ; accuracy: 0.7366666666666667; loss: 2.228696346282959\n",
      "Training epoch 7638 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7638 ; accuracy: 0.7366666666666667; loss: 2.2287049293518066\n",
      "Training epoch 7639 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7639 ; accuracy: 0.7366666666666667; loss: 2.2287092208862305\n",
      "Training epoch 7640 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7640 ; accuracy: 0.7366666666666667; loss: 2.228710889816284\n",
      "Training epoch 7641 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7641 ; accuracy: 0.7366666666666667; loss: 2.2287352085113525\n",
      "Training epoch 7642 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7642 ; accuracy: 0.7366666666666667; loss: 2.2287487983703613\n",
      "Training epoch 7643 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7643 ; accuracy: 0.7366666666666667; loss: 2.2287659645080566\n",
      "Training epoch 7644 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7644 ; accuracy: 0.7366666666666667; loss: 2.228778839111328\n",
      "Training epoch 7645 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7645 ; accuracy: 0.7366666666666667; loss: 2.2288005352020264\n",
      "Training epoch 7646 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7646 ; accuracy: 0.7366666666666667; loss: 2.2288119792938232\n",
      "Training epoch 7647 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7647 ; accuracy: 0.7366666666666667; loss: 2.2288265228271484\n",
      "Training epoch 7648 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7648 ; accuracy: 0.7366666666666667; loss: 2.22884464263916\n",
      "Training epoch 7649 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7649 ; accuracy: 0.7366666666666667; loss: 2.2288641929626465\n",
      "Training epoch 7650 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7650 ; accuracy: 0.7366666666666667; loss: 2.2288875579833984\n",
      "Training epoch 7651 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7651 ; accuracy: 0.7366666666666667; loss: 2.2289161682128906\n",
      "Training epoch 7652 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7652 ; accuracy: 0.7366666666666667; loss: 2.2289376258850098\n",
      "Training epoch 7653 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7653 ; accuracy: 0.7366666666666667; loss: 2.228957176208496\n",
      "Training epoch 7654 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7654 ; accuracy: 0.7366666666666667; loss: 2.2289650440216064\n",
      "Training epoch 7655 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7655 ; accuracy: 0.7366666666666667; loss: 2.228970527648926\n",
      "Training epoch 7656 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7656 ; accuracy: 0.7366666666666667; loss: 2.228976011276245\n",
      "Training epoch 7657 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7657 ; accuracy: 0.7366666666666667; loss: 2.228986978530884\n",
      "Training epoch 7658 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7658 ; accuracy: 0.7366666666666667; loss: 2.2290022373199463\n",
      "Training epoch 7659 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7659 ; accuracy: 0.7366666666666667; loss: 2.2290215492248535\n",
      "Training epoch 7660 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7660 ; accuracy: 0.7366666666666667; loss: 2.2290408611297607\n",
      "Training epoch 7661 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7661 ; accuracy: 0.7366666666666667; loss: 2.2290616035461426\n",
      "Training epoch 7662 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7662 ; accuracy: 0.7366666666666667; loss: 2.2290821075439453\n",
      "Training epoch 7663 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7663 ; accuracy: 0.7366666666666667; loss: 2.229097843170166\n",
      "Training epoch 7664 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7664 ; accuracy: 0.7366666666666667; loss: 2.229113817214966\n",
      "Training epoch 7665 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7665 ; accuracy: 0.7366666666666667; loss: 2.229135751724243\n",
      "Training epoch 7666 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7666 ; accuracy: 0.7366666666666667; loss: 2.2291595935821533\n",
      "Training epoch 7667 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7667 ; accuracy: 0.7366666666666667; loss: 2.229179620742798\n",
      "Training epoch 7668 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7668 ; accuracy: 0.7366666666666667; loss: 2.229196310043335\n",
      "Training epoch 7669 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7669 ; accuracy: 0.7366666666666667; loss: 2.229207992553711\n",
      "Training epoch 7670 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7670 ; accuracy: 0.7366666666666667; loss: 2.2292251586914062\n",
      "Training epoch 7671 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7671 ; accuracy: 0.7366666666666667; loss: 2.229235887527466\n",
      "Training epoch 7672 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7672 ; accuracy: 0.7366666666666667; loss: 2.229262113571167\n",
      "Training epoch 7673 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7673 ; accuracy: 0.7366666666666667; loss: 2.229290008544922\n",
      "Training epoch 7674 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7674 ; accuracy: 0.7366666666666667; loss: 2.229306936264038\n",
      "Training epoch 7675 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7675 ; accuracy: 0.7366666666666667; loss: 2.229318857192993\n",
      "Training epoch 7676 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7676 ; accuracy: 0.7366666666666667; loss: 2.229334831237793\n",
      "Training epoch 7677 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7677 ; accuracy: 0.7366666666666667; loss: 2.229351282119751\n",
      "Training epoch 7678 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7678 ; accuracy: 0.7366666666666667; loss: 2.229372978210449\n",
      "Training epoch 7679 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7679 ; accuracy: 0.7366666666666667; loss: 2.2293896675109863\n",
      "Training epoch 7680 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7680 ; accuracy: 0.7366666666666667; loss: 2.229405641555786\n",
      "Training epoch 7681 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7681 ; accuracy: 0.7366666666666667; loss: 2.2294528484344482\n",
      "Training epoch 7682 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7682 ; accuracy: 0.7366666666666667; loss: 2.2294976711273193\n",
      "Training epoch 7683 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7683 ; accuracy: 0.7366666666666667; loss: 2.229555606842041\n",
      "Training epoch 7684 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7684 ; accuracy: 0.7366666666666667; loss: 2.2296018600463867\n",
      "Training epoch 7685 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7685 ; accuracy: 0.7366666666666667; loss: 2.2296478748321533\n",
      "Training epoch 7686 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7686 ; accuracy: 0.7366666666666667; loss: 2.2296831607818604\n",
      "Training epoch 7687 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7687 ; accuracy: 0.7366666666666667; loss: 2.2297189235687256\n",
      "Training epoch 7688 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7688 ; accuracy: 0.7366666666666667; loss: 2.229753255844116\n",
      "Training epoch 7689 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7689 ; accuracy: 0.7366666666666667; loss: 2.229780912399292\n",
      "Training epoch 7690 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7690 ; accuracy: 0.7366666666666667; loss: 2.2298038005828857\n",
      "Training epoch 7691 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7691 ; accuracy: 0.7366666666666667; loss: 2.2298085689544678\n",
      "Training epoch 7692 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7692 ; accuracy: 0.7366666666666667; loss: 2.229823112487793\n",
      "Training epoch 7693 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7693 ; accuracy: 0.7366666666666667; loss: 2.229834794998169\n",
      "Training epoch 7694 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7694 ; accuracy: 0.7366666666666667; loss: 2.229846954345703\n",
      "Training epoch 7695 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 7695 ; accuracy: 0.7366666666666667; loss: 2.229921340942383\n",
      "Training epoch 7696 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7696 ; accuracy: 0.7366666666666667; loss: 2.2299821376800537\n",
      "Training epoch 7697 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7697 ; accuracy: 0.7366666666666667; loss: 2.230034112930298\n",
      "Training epoch 7698 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7698 ; accuracy: 0.7366666666666667; loss: 2.2300784587860107\n",
      "Training epoch 7699 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7699 ; accuracy: 0.7366666666666667; loss: 2.2301185131073\n",
      "Training epoch 7700 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7700 ; accuracy: 0.7366666666666667; loss: 2.2301502227783203\n",
      "Training epoch 7701 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7701 ; accuracy: 0.7366666666666667; loss: 2.230180501937866\n",
      "Training epoch 7702 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7702 ; accuracy: 0.7366666666666667; loss: 2.2302350997924805\n",
      "Training epoch 7703 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7703 ; accuracy: 0.7366666666666667; loss: 2.2302794456481934\n",
      "Training epoch 7704 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7704 ; accuracy: 0.7366666666666667; loss: 2.230314016342163\n",
      "Training epoch 7705 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7705 ; accuracy: 0.7366666666666667; loss: 2.230344533920288\n",
      "Training epoch 7706 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7706 ; accuracy: 0.7366666666666667; loss: 2.230379581451416\n",
      "Training epoch 7707 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7707 ; accuracy: 0.7366666666666667; loss: 2.230416774749756\n",
      "Training epoch 7708 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7708 ; accuracy: 0.7366666666666667; loss: 2.2304494380950928\n",
      "Training epoch 7709 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7709 ; accuracy: 0.7366666666666667; loss: 2.2304739952087402\n",
      "Training epoch 7710 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7710 ; accuracy: 0.7366666666666667; loss: 2.2304937839508057\n",
      "Training epoch 7711 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7711 ; accuracy: 0.7366666666666667; loss: 2.230518102645874\n",
      "Training epoch 7712 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7712 ; accuracy: 0.7366666666666667; loss: 2.2305402755737305\n",
      "Training epoch 7713 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7713 ; accuracy: 0.7366666666666667; loss: 2.2305641174316406\n",
      "Training epoch 7714 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7714 ; accuracy: 0.7366666666666667; loss: 2.2305827140808105\n",
      "Training epoch 7715 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7715 ; accuracy: 0.7366666666666667; loss: 2.2305984497070312\n",
      "Training epoch 7716 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7716 ; accuracy: 0.7366666666666667; loss: 2.2306509017944336\n",
      "Training epoch 7717 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7717 ; accuracy: 0.7366666666666667; loss: 2.2307093143463135\n",
      "Training epoch 7718 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7718 ; accuracy: 0.7366666666666667; loss: 2.230764150619507\n",
      "Training epoch 7719 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7719 ; accuracy: 0.7366666666666667; loss: 2.2308154106140137\n",
      "Training epoch 7720 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7720 ; accuracy: 0.7366666666666667; loss: 2.2308568954467773\n",
      "Training epoch 7721 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7721 ; accuracy: 0.7366666666666667; loss: 2.2308924198150635\n",
      "Training epoch 7722 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7722 ; accuracy: 0.7366666666666667; loss: 2.2309272289276123\n",
      "Training epoch 7723 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7723 ; accuracy: 0.7366666666666667; loss: 2.2309508323669434\n",
      "Training epoch 7724 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7724 ; accuracy: 0.7366666666666667; loss: 2.230971574783325\n",
      "Training epoch 7725 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7725 ; accuracy: 0.7366666666666667; loss: 2.2309911251068115\n",
      "Training epoch 7726 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7726 ; accuracy: 0.7366666666666667; loss: 2.231008291244507\n",
      "Training epoch 7727 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7727 ; accuracy: 0.7366666666666667; loss: 2.2310259342193604\n",
      "Training epoch 7728 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7728 ; accuracy: 0.7366666666666667; loss: 2.231034517288208\n",
      "Training epoch 7729 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7729 ; accuracy: 0.7366666666666667; loss: 2.231044054031372\n",
      "Training epoch 7730 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7730 ; accuracy: 0.7366666666666667; loss: 2.23105788230896\n",
      "Training epoch 7731 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7731 ; accuracy: 0.7366666666666667; loss: 2.2310659885406494\n",
      "Training epoch 7732 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7732 ; accuracy: 0.7366666666666667; loss: 2.2310853004455566\n",
      "Training epoch 7733 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7733 ; accuracy: 0.7366666666666667; loss: 2.2311010360717773\n",
      "Training epoch 7734 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7734 ; accuracy: 0.7366666666666667; loss: 2.23111891746521\n",
      "Training epoch 7735 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7735 ; accuracy: 0.7366666666666667; loss: 2.2311291694641113\n",
      "Training epoch 7736 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7736 ; accuracy: 0.7366666666666667; loss: 2.231137990951538\n",
      "Training epoch 7737 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7737 ; accuracy: 0.7366666666666667; loss: 2.2311673164367676\n",
      "Training epoch 7738 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7738 ; accuracy: 0.7366666666666667; loss: 2.231191635131836\n",
      "Training epoch 7739 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7739 ; accuracy: 0.7366666666666667; loss: 2.231224536895752\n",
      "Training epoch 7740 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7740 ; accuracy: 0.7366666666666667; loss: 2.2312567234039307\n",
      "Training epoch 7741 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7741 ; accuracy: 0.7366666666666667; loss: 2.231290817260742\n",
      "Training epoch 7742 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7742 ; accuracy: 0.7366666666666667; loss: 2.2313265800476074\n",
      "Training epoch 7743 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7743 ; accuracy: 0.7366666666666667; loss: 2.2313568592071533\n",
      "Training epoch 7744 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7744 ; accuracy: 0.7366666666666667; loss: 2.231383800506592\n",
      "Training epoch 7745 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7745 ; accuracy: 0.7366666666666667; loss: 2.231405735015869\n",
      "Training epoch 7746 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7746 ; accuracy: 0.7366666666666667; loss: 2.2314295768737793\n",
      "Training epoch 7747 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7747 ; accuracy: 0.7366666666666667; loss: 2.231449842453003\n",
      "Training epoch 7748 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7748 ; accuracy: 0.7366666666666667; loss: 2.2314722537994385\n",
      "Training epoch 7749 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7749 ; accuracy: 0.7366666666666667; loss: 2.2314987182617188\n",
      "Training epoch 7750 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7750 ; accuracy: 0.7366666666666667; loss: 2.231529474258423\n",
      "Training epoch 7751 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7751 ; accuracy: 0.7366666666666667; loss: 2.231565237045288\n",
      "Training epoch 7752 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7752 ; accuracy: 0.7366666666666667; loss: 2.231595993041992\n",
      "Training epoch 7753 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7753 ; accuracy: 0.7366666666666667; loss: 2.2316389083862305\n",
      "Training epoch 7754 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7754 ; accuracy: 0.7366666666666667; loss: 2.231674909591675\n",
      "Training epoch 7755 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7755 ; accuracy: 0.7366666666666667; loss: 2.231708526611328\n",
      "Training epoch 7756 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7756 ; accuracy: 0.7366666666666667; loss: 2.2317304611206055\n",
      "Training epoch 7757 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7757 ; accuracy: 0.7366666666666667; loss: 2.2317428588867188\n",
      "Training epoch 7758 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7758 ; accuracy: 0.7366666666666667; loss: 2.2317585945129395\n",
      "Training epoch 7759 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7759 ; accuracy: 0.7366666666666667; loss: 2.231768846511841\n",
      "Training epoch 7760 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7760 ; accuracy: 0.7366666666666667; loss: 2.231785774230957\n",
      "Training epoch 7761 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7761 ; accuracy: 0.7366666666666667; loss: 2.2318108081817627\n",
      "Training epoch 7762 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7762 ; accuracy: 0.7366666666666667; loss: 2.2318379878997803\n",
      "Training epoch 7763 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7763 ; accuracy: 0.7366666666666667; loss: 2.2318663597106934\n",
      "Training epoch 7764 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7764 ; accuracy: 0.7366666666666667; loss: 2.2318928241729736\n",
      "Training epoch 7765 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7765 ; accuracy: 0.7366666666666667; loss: 2.2319185733795166\n",
      "Training epoch 7766 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7766 ; accuracy: 0.7366666666666667; loss: 2.231943130493164\n",
      "Training epoch 7767 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7767 ; accuracy: 0.7366666666666667; loss: 2.231947183609009\n",
      "Training epoch 7768 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7768 ; accuracy: 0.7366666666666667; loss: 2.2319514751434326\n",
      "Training epoch 7769 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7769 ; accuracy: 0.7366666666666667; loss: 2.2319538593292236\n",
      "Training epoch 7770 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7770 ; accuracy: 0.7366666666666667; loss: 2.2319583892822266\n",
      "Training epoch 7771 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7771 ; accuracy: 0.7366666666666667; loss: 2.2319705486297607\n",
      "Training epoch 7772 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7772 ; accuracy: 0.7366666666666667; loss: 2.2319791316986084\n",
      "Training epoch 7773 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7773 ; accuracy: 0.7366666666666667; loss: 2.2319836616516113\n",
      "Training epoch 7774 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7774 ; accuracy: 0.7366666666666667; loss: 2.2319860458374023\n",
      "Training epoch 7775 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7775 ; accuracy: 0.7366666666666667; loss: 2.2319884300231934\n",
      "Training epoch 7776 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7776 ; accuracy: 0.7366666666666667; loss: 2.2319839000701904\n",
      "Training epoch 7777 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7777 ; accuracy: 0.7366666666666667; loss: 2.2319893836975098\n",
      "Training epoch 7778 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7778 ; accuracy: 0.7366666666666667; loss: 2.2319979667663574\n",
      "Training epoch 7779 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7779 ; accuracy: 0.7366666666666667; loss: 2.2320094108581543\n",
      "Training epoch 7780 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7780 ; accuracy: 0.7366666666666667; loss: 2.232027053833008\n",
      "Training epoch 7781 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7781 ; accuracy: 0.7366666666666667; loss: 2.232043743133545\n",
      "Training epoch 7782 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7782 ; accuracy: 0.7366666666666667; loss: 2.232046604156494\n",
      "Training epoch 7783 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7783 ; accuracy: 0.7366666666666667; loss: 2.2320542335510254\n",
      "Training epoch 7784 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7784 ; accuracy: 0.7366666666666667; loss: 2.2320680618286133\n",
      "Training epoch 7785 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7785 ; accuracy: 0.7366666666666667; loss: 2.2320849895477295\n",
      "Training epoch 7786 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7786 ; accuracy: 0.7366666666666667; loss: 2.2320945262908936\n",
      "Training epoch 7787 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7787 ; accuracy: 0.7366666666666667; loss: 2.2321219444274902\n",
      "Training epoch 7788 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7788 ; accuracy: 0.7366666666666667; loss: 2.2321577072143555\n",
      "Training epoch 7789 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7789 ; accuracy: 0.7366666666666667; loss: 2.2321977615356445\n",
      "Training epoch 7790 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7790 ; accuracy: 0.7366666666666667; loss: 2.2322306632995605\n",
      "Training epoch 7791 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7791 ; accuracy: 0.7366666666666667; loss: 2.2321856021881104\n",
      "Training epoch 7792 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7792 ; accuracy: 0.7366666666666667; loss: 2.2321438789367676\n",
      "Training epoch 7793 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7793 ; accuracy: 0.7366666666666667; loss: 2.2321088314056396\n",
      "Training epoch 7794 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7794 ; accuracy: 0.7366666666666667; loss: 2.2320868968963623\n",
      "Training epoch 7795 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7795 ; accuracy: 0.7366666666666667; loss: 2.2320637702941895\n",
      "Training epoch 7796 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7796 ; accuracy: 0.7366666666666667; loss: 2.232043981552124\n",
      "Training epoch 7797 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7797 ; accuracy: 0.7366666666666667; loss: 2.232022762298584\n",
      "Training epoch 7798 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7798 ; accuracy: 0.7366666666666667; loss: 2.2319934368133545\n",
      "Training epoch 7799 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7799 ; accuracy: 0.7366666666666667; loss: 2.2319657802581787\n",
      "Training epoch 7800 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7800 ; accuracy: 0.7366666666666667; loss: 2.2319400310516357\n",
      "Training epoch 7801 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7801 ; accuracy: 0.7366666666666667; loss: 2.2319085597991943\n",
      "Training epoch 7802 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7802 ; accuracy: 0.7366666666666667; loss: 2.231882333755493\n",
      "Training epoch 7803 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7803 ; accuracy: 0.7366666666666667; loss: 2.231860637664795\n",
      "Training epoch 7804 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7804 ; accuracy: 0.7366666666666667; loss: 2.2318382263183594\n",
      "Training epoch 7805 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7805 ; accuracy: 0.7366666666666667; loss: 2.2318341732025146\n",
      "Training epoch 7806 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7806 ; accuracy: 0.7366666666666667; loss: 2.2318201065063477\n",
      "Training epoch 7807 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7807 ; accuracy: 0.7366666666666667; loss: 2.2318079471588135\n",
      "Training epoch 7808 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7808 ; accuracy: 0.7366666666666667; loss: 2.2317874431610107\n",
      "Training epoch 7809 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7809 ; accuracy: 0.7366666666666667; loss: 2.2317731380462646\n",
      "Training epoch 7810 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7810 ; accuracy: 0.7366666666666667; loss: 2.2317562103271484\n",
      "Training epoch 7811 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7811 ; accuracy: 0.7366666666666667; loss: 2.2317423820495605\n",
      "Training epoch 7812 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7812 ; accuracy: 0.7366666666666667; loss: 2.2317330837249756\n",
      "Training epoch 7813 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7813 ; accuracy: 0.7366666666666667; loss: 2.231726884841919\n",
      "Training epoch 7814 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7814 ; accuracy: 0.7366666666666667; loss: 2.2317254543304443\n",
      "Training epoch 7815 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7815 ; accuracy: 0.7366666666666667; loss: 2.2317216396331787\n",
      "Training epoch 7816 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7816 ; accuracy: 0.7366666666666667; loss: 2.2317216396331787\n",
      "Training epoch 7817 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7817 ; accuracy: 0.7366666666666667; loss: 2.2317168712615967\n",
      "Training epoch 7818 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7818 ; accuracy: 0.7366666666666667; loss: 2.2317166328430176\n",
      "Training epoch 7819 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7819 ; accuracy: 0.7366666666666667; loss: 2.231718063354492\n",
      "Training epoch 7820 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7820 ; accuracy: 0.7366666666666667; loss: 2.2317192554473877\n",
      "Training epoch 7821 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7821 ; accuracy: 0.7366666666666667; loss: 2.2316999435424805\n",
      "Training epoch 7822 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7822 ; accuracy: 0.7366666666666667; loss: 2.2316792011260986\n",
      "Training epoch 7823 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7823 ; accuracy: 0.7366666666666667; loss: 2.2316582202911377\n",
      "Training epoch 7824 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7824 ; accuracy: 0.7366666666666667; loss: 2.231642961502075\n",
      "Training epoch 7825 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7825 ; accuracy: 0.7366666666666667; loss: 2.231630325317383\n",
      "Training epoch 7826 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7826 ; accuracy: 0.7366666666666667; loss: 2.2316198348999023\n",
      "Training epoch 7827 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7827 ; accuracy: 0.7366666666666667; loss: 2.2316207885742188\n",
      "Training epoch 7828 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7828 ; accuracy: 0.7366666666666667; loss: 2.2316277027130127\n",
      "Training epoch 7829 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7829 ; accuracy: 0.7366666666666667; loss: 2.2316339015960693\n",
      "Training epoch 7830 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7830 ; accuracy: 0.7366666666666667; loss: 2.2316231727600098\n",
      "Training epoch 7831 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7831 ; accuracy: 0.7366666666666667; loss: 2.231613874435425\n",
      "Training epoch 7832 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7832 ; accuracy: 0.7366666666666667; loss: 2.231609582901001\n",
      "Training epoch 7833 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7833 ; accuracy: 0.7366666666666667; loss: 2.231605052947998\n",
      "Training epoch 7834 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7834 ; accuracy: 0.7366666666666667; loss: 2.231609582901001\n",
      "Training epoch 7835 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7835 ; accuracy: 0.7366666666666667; loss: 2.2316129207611084\n",
      "Training epoch 7836 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7836 ; accuracy: 0.7366666666666667; loss: 2.2316248416900635\n",
      "Training epoch 7837 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7837 ; accuracy: 0.7366666666666667; loss: 2.231635332107544\n",
      "Training epoch 7838 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7838 ; accuracy: 0.7366666666666667; loss: 2.2316513061523438\n",
      "Training epoch 7839 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7839 ; accuracy: 0.7366666666666667; loss: 2.2316694259643555\n",
      "Training epoch 7840 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7840 ; accuracy: 0.7366666666666667; loss: 2.2316808700561523\n",
      "Training epoch 7841 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7841 ; accuracy: 0.7366666666666667; loss: 2.231691837310791\n",
      "Training epoch 7842 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7842 ; accuracy: 0.7366666666666667; loss: 2.2317140102386475\n",
      "Training epoch 7843 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7843 ; accuracy: 0.7366666666666667; loss: 2.2317240238189697\n",
      "Training epoch 7844 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7844 ; accuracy: 0.7366666666666667; loss: 2.231733798980713\n",
      "Training epoch 7845 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7845 ; accuracy: 0.7366666666666667; loss: 2.231745958328247\n",
      "Training epoch 7846 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7846 ; accuracy: 0.7366666666666667; loss: 2.2317612171173096\n",
      "Training epoch 7847 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7847 ; accuracy: 0.7366666666666667; loss: 2.2317771911621094\n",
      "Training epoch 7848 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7848 ; accuracy: 0.7366666666666667; loss: 2.2318015098571777\n",
      "Training epoch 7849 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7849 ; accuracy: 0.7366666666666667; loss: 2.231826066970825\n",
      "Training epoch 7850 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7850 ; accuracy: 0.7366666666666667; loss: 2.2318506240844727\n",
      "Training epoch 7851 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7851 ; accuracy: 0.7366666666666667; loss: 2.2318825721740723\n",
      "Training epoch 7852 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7852 ; accuracy: 0.7366666666666667; loss: 2.2319259643554688\n",
      "Training epoch 7853 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7853 ; accuracy: 0.7366666666666667; loss: 2.2319629192352295\n",
      "Training epoch 7854 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7854 ; accuracy: 0.7366666666666667; loss: 2.2320001125335693\n",
      "Training epoch 7855 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7855 ; accuracy: 0.7366666666666667; loss: 2.232032060623169\n",
      "Training epoch 7856 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7856 ; accuracy: 0.7366666666666667; loss: 2.2320618629455566\n",
      "Training epoch 7857 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7857 ; accuracy: 0.7366666666666667; loss: 2.2320849895477295\n",
      "Training epoch 7858 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7858 ; accuracy: 0.7366666666666667; loss: 2.2321064472198486\n",
      "Training epoch 7859 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7859 ; accuracy: 0.7366666666666667; loss: 2.2321324348449707\n",
      "Training epoch 7860 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7860 ; accuracy: 0.7366666666666667; loss: 2.232158899307251\n",
      "Training epoch 7861 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7861 ; accuracy: 0.7366666666666667; loss: 2.2321910858154297\n",
      "Training epoch 7862 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7862 ; accuracy: 0.7366666666666667; loss: 2.2322282791137695\n",
      "Training epoch 7863 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7863 ; accuracy: 0.7366666666666667; loss: 2.232252359390259\n",
      "Training epoch 7864 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7864 ; accuracy: 0.7366666666666667; loss: 2.2322754859924316\n",
      "Training epoch 7865 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7865 ; accuracy: 0.7366666666666667; loss: 2.2322959899902344\n",
      "Training epoch 7866 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7866 ; accuracy: 0.7366666666666667; loss: 2.232325553894043\n",
      "Training epoch 7867 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7867 ; accuracy: 0.7366666666666667; loss: 2.2323553562164307\n",
      "Training epoch 7868 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7868 ; accuracy: 0.7366666666666667; loss: 2.2323808670043945\n",
      "Training epoch 7869 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7869 ; accuracy: 0.7366666666666667; loss: 2.232396125793457\n",
      "Training epoch 7870 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7870 ; accuracy: 0.7366666666666667; loss: 2.2324106693267822\n",
      "Training epoch 7871 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7871 ; accuracy: 0.7366666666666667; loss: 2.2324044704437256\n",
      "Training epoch 7872 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7872 ; accuracy: 0.7366666666666667; loss: 2.232405662536621\n",
      "Training epoch 7873 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7873 ; accuracy: 0.7366666666666667; loss: 2.232417583465576\n",
      "Training epoch 7874 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7874 ; accuracy: 0.7366666666666667; loss: 2.232431173324585\n",
      "Training epoch 7875 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7875 ; accuracy: 0.7366666666666667; loss: 2.232445240020752\n",
      "Training epoch 7876 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7876 ; accuracy: 0.7366666666666667; loss: 2.232480764389038\n",
      "Training epoch 7877 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7877 ; accuracy: 0.7366666666666667; loss: 2.2325127124786377\n",
      "Training epoch 7878 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7878 ; accuracy: 0.7366666666666667; loss: 2.232555389404297\n",
      "Training epoch 7879 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7879 ; accuracy: 0.7366666666666667; loss: 2.232590436935425\n",
      "Training epoch 7880 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7880 ; accuracy: 0.7366666666666667; loss: 2.232611656188965\n",
      "Training epoch 7881 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7881 ; accuracy: 0.7366666666666667; loss: 2.232635974884033\n",
      "Training epoch 7882 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7882 ; accuracy: 0.7366666666666667; loss: 2.2326550483703613\n",
      "Training epoch 7883 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7883 ; accuracy: 0.7366666666666667; loss: 2.2326748371124268\n",
      "Training epoch 7884 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7884 ; accuracy: 0.7366666666666667; loss: 2.232696533203125\n",
      "Training epoch 7885 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7885 ; accuracy: 0.7366666666666667; loss: 2.232722282409668\n",
      "Training epoch 7886 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7886 ; accuracy: 0.7366666666666667; loss: 2.232745885848999\n",
      "Training epoch 7887 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7887 ; accuracy: 0.7366666666666667; loss: 2.2327513694763184\n",
      "Training epoch 7888 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7888 ; accuracy: 0.7366666666666667; loss: 2.2327425479888916\n",
      "Training epoch 7889 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7889 ; accuracy: 0.7366666666666667; loss: 2.2327377796173096\n",
      "Training epoch 7890 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7890 ; accuracy: 0.7366666666666667; loss: 2.232740640640259\n",
      "Training epoch 7891 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7891 ; accuracy: 0.7366666666666667; loss: 2.2327423095703125\n",
      "Training epoch 7892 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7892 ; accuracy: 0.7366666666666667; loss: 2.2327489852905273\n",
      "Training epoch 7893 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7893 ; accuracy: 0.7366666666666667; loss: 2.2327611446380615\n",
      "Training epoch 7894 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7894 ; accuracy: 0.7366666666666667; loss: 2.232786178588867\n",
      "Training epoch 7895 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7895 ; accuracy: 0.7366666666666667; loss: 2.2328104972839355\n",
      "Training epoch 7896 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7896 ; accuracy: 0.7366666666666667; loss: 2.2328250408172607\n",
      "Training epoch 7897 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7897 ; accuracy: 0.7366666666666667; loss: 2.2328600883483887\n",
      "Training epoch 7898 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7898 ; accuracy: 0.7366666666666667; loss: 2.2328953742980957\n",
      "Training epoch 7899 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7899 ; accuracy: 0.7366666666666667; loss: 2.2329325675964355\n",
      "Training epoch 7900 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7900 ; accuracy: 0.7366666666666667; loss: 2.232973337173462\n",
      "Training epoch 7901 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7901 ; accuracy: 0.7366666666666667; loss: 2.2330212593078613\n",
      "Training epoch 7902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7902 ; accuracy: 0.7366666666666667; loss: 2.233071804046631\n",
      "Training epoch 7903 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7903 ; accuracy: 0.7366666666666667; loss: 2.2331249713897705\n",
      "Training epoch 7904 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7904 ; accuracy: 0.7366666666666667; loss: 2.233175039291382\n",
      "Training epoch 7905 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7905 ; accuracy: 0.7366666666666667; loss: 2.233212947845459\n",
      "Training epoch 7906 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7906 ; accuracy: 0.7366666666666667; loss: 2.233247756958008\n",
      "Training epoch 7907 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7907 ; accuracy: 0.7366666666666667; loss: 2.233283519744873\n",
      "Training epoch 7908 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7908 ; accuracy: 0.7366666666666667; loss: 2.2333295345306396\n",
      "Training epoch 7909 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7909 ; accuracy: 0.7366666666666667; loss: 2.233365535736084\n",
      "Training epoch 7910 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7910 ; accuracy: 0.7366666666666667; loss: 2.233400344848633\n",
      "Training epoch 7911 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7911 ; accuracy: 0.7366666666666667; loss: 2.233417272567749\n",
      "Training epoch 7912 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7912 ; accuracy: 0.7366666666666667; loss: 2.2334299087524414\n",
      "Training epoch 7913 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7913 ; accuracy: 0.7366666666666667; loss: 2.2334372997283936\n",
      "Training epoch 7914 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7914 ; accuracy: 0.7366666666666667; loss: 2.2334561347961426\n",
      "Training epoch 7915 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7915 ; accuracy: 0.7366666666666667; loss: 2.233478307723999\n",
      "Training epoch 7916 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7916 ; accuracy: 0.7366666666666667; loss: 2.233503818511963\n",
      "Training epoch 7917 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7917 ; accuracy: 0.7366666666666667; loss: 2.2335307598114014\n",
      "Training epoch 7918 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7918 ; accuracy: 0.7366666666666667; loss: 2.2335565090179443\n",
      "Training epoch 7919 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7919 ; accuracy: 0.7366666666666667; loss: 2.2335782051086426\n",
      "Training epoch 7920 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7920 ; accuracy: 0.7366666666666667; loss: 2.23360013961792\n",
      "Training epoch 7921 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7921 ; accuracy: 0.7366666666666667; loss: 2.233630418777466\n",
      "Training epoch 7922 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7922 ; accuracy: 0.7366666666666667; loss: 2.233651876449585\n",
      "Training epoch 7923 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7923 ; accuracy: 0.7366666666666667; loss: 2.2336771488189697\n",
      "Training epoch 7924 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7924 ; accuracy: 0.7366666666666667; loss: 2.233705759048462\n",
      "Training epoch 7925 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7925 ; accuracy: 0.7366666666666667; loss: 2.233720302581787\n",
      "Training epoch 7926 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7926 ; accuracy: 0.7366666666666667; loss: 2.233757257461548\n",
      "Training epoch 7927 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7927 ; accuracy: 0.7366666666666667; loss: 2.233785629272461\n",
      "Training epoch 7928 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7928 ; accuracy: 0.7366666666666667; loss: 2.233814001083374\n",
      "Training epoch 7929 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7929 ; accuracy: 0.7366666666666667; loss: 2.2338385581970215\n",
      "Training epoch 7930 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7930 ; accuracy: 0.7366666666666667; loss: 2.233875274658203\n",
      "Training epoch 7931 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7931 ; accuracy: 0.7366666666666667; loss: 2.233910083770752\n",
      "Training epoch 7932 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7932 ; accuracy: 0.7366666666666667; loss: 2.23393177986145\n",
      "Training epoch 7933 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7933 ; accuracy: 0.7366666666666667; loss: 2.2339420318603516\n",
      "Training epoch 7934 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7934 ; accuracy: 0.7366666666666667; loss: 2.233950614929199\n",
      "Training epoch 7935 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7935 ; accuracy: 0.7366666666666667; loss: 2.2339489459991455\n",
      "Training epoch 7936 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7936 ; accuracy: 0.7366666666666667; loss: 2.233945846557617\n",
      "Training epoch 7937 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7937 ; accuracy: 0.7366666666666667; loss: 2.233942747116089\n",
      "Training epoch 7938 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7938 ; accuracy: 0.7366666666666667; loss: 2.2339413166046143\n",
      "Training epoch 7939 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7939 ; accuracy: 0.7366666666666667; loss: 2.233949661254883\n",
      "Training epoch 7940 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7940 ; accuracy: 0.7366666666666667; loss: 2.233964681625366\n",
      "Training epoch 7941 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7941 ; accuracy: 0.7366666666666667; loss: 2.2339746952056885\n",
      "Training epoch 7942 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7942 ; accuracy: 0.7366666666666667; loss: 2.2339820861816406\n",
      "Training epoch 7943 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7943 ; accuracy: 0.7366666666666667; loss: 2.23398756980896\n",
      "Training epoch 7944 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7944 ; accuracy: 0.7366666666666667; loss: 2.2339894771575928\n",
      "Training epoch 7945 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7945 ; accuracy: 0.7366666666666667; loss: 2.2339928150177\n",
      "Training epoch 7946 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7946 ; accuracy: 0.7366666666666667; loss: 2.2339956760406494\n",
      "Training epoch 7947 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7947 ; accuracy: 0.7366666666666667; loss: 2.233980417251587\n",
      "Training epoch 7948 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7948 ; accuracy: 0.7366666666666667; loss: 2.233964443206787\n",
      "Training epoch 7949 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7949 ; accuracy: 0.7366666666666667; loss: 2.2339518070220947\n",
      "Training epoch 7950 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7950 ; accuracy: 0.7366666666666667; loss: 2.2339417934417725\n",
      "Training epoch 7951 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7951 ; accuracy: 0.7366666666666667; loss: 2.2339351177215576\n",
      "Training epoch 7952 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7952 ; accuracy: 0.7366666666666667; loss: 2.233936071395874\n",
      "Training epoch 7953 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7953 ; accuracy: 0.7366666666666667; loss: 2.2339417934417725\n",
      "Training epoch 7954 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7954 ; accuracy: 0.7366666666666667; loss: 2.2339441776275635\n",
      "Training epoch 7955 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7955 ; accuracy: 0.7366666666666667; loss: 2.233942747116089\n",
      "Training epoch 7956 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7956 ; accuracy: 0.7366666666666667; loss: 2.233947277069092\n",
      "Training epoch 7957 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7957 ; accuracy: 0.7366666666666667; loss: 2.233966112136841\n",
      "Training epoch 7958 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7958 ; accuracy: 0.7366666666666667; loss: 2.233990430831909\n",
      "Training epoch 7959 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7959 ; accuracy: 0.7366666666666667; loss: 2.234009027481079\n",
      "Training epoch 7960 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7960 ; accuracy: 0.7366666666666667; loss: 2.2340283393859863\n",
      "Training epoch 7961 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7961 ; accuracy: 0.7366666666666667; loss: 2.2340495586395264\n",
      "Training epoch 7962 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7962 ; accuracy: 0.7366666666666667; loss: 2.2340800762176514\n",
      "Training epoch 7963 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7963 ; accuracy: 0.7366666666666667; loss: 2.234099864959717\n",
      "Training epoch 7964 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7964 ; accuracy: 0.7366666666666667; loss: 2.234118938446045\n",
      "Training epoch 7965 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7965 ; accuracy: 0.7366666666666667; loss: 2.2341225147247314\n",
      "Training epoch 7966 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7966 ; accuracy: 0.7366666666666667; loss: 2.2341270446777344\n",
      "Training epoch 7967 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7967 ; accuracy: 0.7366666666666667; loss: 2.234130859375\n",
      "Training epoch 7968 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7968 ; accuracy: 0.7366666666666667; loss: 2.234137535095215\n",
      "Training epoch 7969 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7969 ; accuracy: 0.7366666666666667; loss: 2.2341392040252686\n",
      "Training epoch 7970 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7970 ; accuracy: 0.7366666666666667; loss: 2.2341256141662598\n",
      "Training epoch 7971 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7971 ; accuracy: 0.7366666666666667; loss: 2.23410964012146\n",
      "Training epoch 7972 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7972 ; accuracy: 0.7366666666666667; loss: 2.234088182449341\n",
      "Training epoch 7973 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7973 ; accuracy: 0.7366666666666667; loss: 2.234071969985962\n",
      "Training epoch 7974 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7974 ; accuracy: 0.7366666666666667; loss: 2.2340664863586426\n",
      "Training epoch 7975 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7975 ; accuracy: 0.7366666666666667; loss: 2.234067678451538\n",
      "Training epoch 7976 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7976 ; accuracy: 0.7366666666666667; loss: 2.234064817428589\n",
      "Training epoch 7977 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7977 ; accuracy: 0.7366666666666667; loss: 2.234072685241699\n",
      "Training epoch 7978 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7978 ; accuracy: 0.7366666666666667; loss: 2.234072208404541\n",
      "Training epoch 7979 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7979 ; accuracy: 0.7366666666666667; loss: 2.2340705394744873\n",
      "Training epoch 7980 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7980 ; accuracy: 0.7366666666666667; loss: 2.234067440032959\n",
      "Training epoch 7981 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7981 ; accuracy: 0.7366666666666667; loss: 2.2340662479400635\n",
      "Training epoch 7982 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7982 ; accuracy: 0.7366666666666667; loss: 2.234065294265747\n",
      "Training epoch 7983 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7983 ; accuracy: 0.7366666666666667; loss: 2.234065055847168\n",
      "Training epoch 7984 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7984 ; accuracy: 0.7366666666666667; loss: 2.2340681552886963\n",
      "Training epoch 7985 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7985 ; accuracy: 0.7366666666666667; loss: 2.234070301055908\n",
      "Training epoch 7986 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7986 ; accuracy: 0.7366666666666667; loss: 2.234071969985962\n",
      "Training epoch 7987 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7987 ; accuracy: 0.7366666666666667; loss: 2.234088659286499\n",
      "Training epoch 7988 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7988 ; accuracy: 0.7366666666666667; loss: 2.234103202819824\n",
      "Training epoch 7989 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7989 ; accuracy: 0.7366666666666667; loss: 2.234133243560791\n",
      "Training epoch 7990 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7990 ; accuracy: 0.7366666666666667; loss: 2.234160900115967\n",
      "Training epoch 7991 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7991 ; accuracy: 0.7366666666666667; loss: 2.2341954708099365\n",
      "Training epoch 7992 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7992 ; accuracy: 0.7366666666666667; loss: 2.2342324256896973\n",
      "Training epoch 7993 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7993 ; accuracy: 0.7366666666666667; loss: 2.2342658042907715\n",
      "Training epoch 7994 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7994 ; accuracy: 0.7366666666666667; loss: 2.2342965602874756\n",
      "Training epoch 7995 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7995 ; accuracy: 0.7366666666666667; loss: 2.234311819076538\n",
      "Training epoch 7996 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7996 ; accuracy: 0.7366666666666667; loss: 2.2343289852142334\n",
      "Training epoch 7997 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 7997 ; accuracy: 0.7366666666666667; loss: 2.2343475818634033\n",
      "Training epoch 7998 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 7998 ; accuracy: 0.7366666666666667; loss: 2.234363317489624\n",
      "Training epoch 7999 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 7999 ; accuracy: 0.7366666666666667; loss: 2.234377861022949\n",
      "Training epoch 8000 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8000 ; accuracy: 0.7366666666666667; loss: 2.2343780994415283\n",
      "Training epoch 8001 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8001 ; accuracy: 0.7366666666666667; loss: 2.234370231628418\n",
      "Training epoch 8002 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8002 ; accuracy: 0.7366666666666667; loss: 2.2343623638153076\n",
      "Training epoch 8003 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8003 ; accuracy: 0.7366666666666667; loss: 2.2343590259552\n",
      "Training epoch 8004 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8004 ; accuracy: 0.7366666666666667; loss: 2.2343616485595703\n",
      "Training epoch 8005 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8005 ; accuracy: 0.7366666666666667; loss: 2.234370470046997\n",
      "Training epoch 8006 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8006 ; accuracy: 0.7366666666666667; loss: 2.234379291534424\n",
      "Training epoch 8007 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8007 ; accuracy: 0.7366666666666667; loss: 2.2343947887420654\n",
      "Training epoch 8008 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8008 ; accuracy: 0.7366666666666667; loss: 2.2343978881835938\n",
      "Training epoch 8009 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8009 ; accuracy: 0.7366666666666667; loss: 2.2344038486480713\n",
      "Training epoch 8010 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8010 ; accuracy: 0.7366666666666667; loss: 2.2344138622283936\n",
      "Training epoch 8011 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8011 ; accuracy: 0.7366666666666667; loss: 2.234424591064453\n",
      "Training epoch 8012 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8012 ; accuracy: 0.7366666666666667; loss: 2.234431266784668\n",
      "Training epoch 8013 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8013 ; accuracy: 0.7366666666666667; loss: 2.234431266784668\n",
      "Training epoch 8014 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8014 ; accuracy: 0.7366666666666667; loss: 2.234431266784668\n",
      "Training epoch 8015 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8015 ; accuracy: 0.7366666666666667; loss: 2.234426975250244\n",
      "Training epoch 8016 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8016 ; accuracy: 0.7366666666666667; loss: 2.2344250679016113\n",
      "Training epoch 8017 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8017 ; accuracy: 0.7366666666666667; loss: 2.2344143390655518\n",
      "Training epoch 8018 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8018 ; accuracy: 0.7366666666666667; loss: 2.2344160079956055\n",
      "Training epoch 8019 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8019 ; accuracy: 0.7366666666666667; loss: 2.234433174133301\n",
      "Training epoch 8020 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8020 ; accuracy: 0.7366666666666667; loss: 2.2344491481781006\n",
      "Training epoch 8021 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8021 ; accuracy: 0.7366666666666667; loss: 2.23445987701416\n",
      "Training epoch 8022 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8022 ; accuracy: 0.7366666666666667; loss: 2.2344915866851807\n",
      "Training epoch 8023 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8023 ; accuracy: 0.7366666666666667; loss: 2.234518527984619\n",
      "Training epoch 8024 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8024 ; accuracy: 0.7366666666666667; loss: 2.2345528602600098\n",
      "Training epoch 8025 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8025 ; accuracy: 0.7366666666666667; loss: 2.2345657348632812\n",
      "Training epoch 8026 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8026 ; accuracy: 0.7366666666666667; loss: 2.234628200531006\n",
      "Training epoch 8027 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8027 ; accuracy: 0.7366666666666667; loss: 2.234700918197632\n",
      "Training epoch 8028 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8028 ; accuracy: 0.7366666666666667; loss: 2.2347660064697266\n",
      "Training epoch 8029 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8029 ; accuracy: 0.7366666666666667; loss: 2.2348475456237793\n",
      "Training epoch 8030 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8030 ; accuracy: 0.7366666666666667; loss: 2.234919786453247\n",
      "Training epoch 8031 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8031 ; accuracy: 0.7366666666666667; loss: 2.2349910736083984\n",
      "Training epoch 8032 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8032 ; accuracy: 0.7366666666666667; loss: 2.235057830810547\n",
      "Training epoch 8033 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8033 ; accuracy: 0.7366666666666667; loss: 2.2351131439208984\n",
      "Training epoch 8034 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8034 ; accuracy: 0.7366666666666667; loss: 2.235163688659668\n",
      "Training epoch 8035 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8035 ; accuracy: 0.7366666666666667; loss: 2.235217332839966\n",
      "Training epoch 8036 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8036 ; accuracy: 0.7366666666666667; loss: 2.2352750301361084\n",
      "Training epoch 8037 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8037 ; accuracy: 0.7366666666666667; loss: 2.235332727432251\n",
      "Training epoch 8038 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8038 ; accuracy: 0.7366666666666667; loss: 2.2353975772857666\n",
      "Training epoch 8039 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8039 ; accuracy: 0.7366666666666667; loss: 2.235466957092285\n",
      "Training epoch 8040 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8040 ; accuracy: 0.7366666666666667; loss: 2.2355263233184814\n",
      "Training epoch 8041 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8041 ; accuracy: 0.7366666666666667; loss: 2.2355825901031494\n",
      "Training epoch 8042 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8042 ; accuracy: 0.7366666666666667; loss: 2.2356457710266113\n",
      "Training epoch 8043 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8043 ; accuracy: 0.7366666666666667; loss: 2.2356956005096436\n",
      "Training epoch 8044 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8044 ; accuracy: 0.7366666666666667; loss: 2.2357542514801025\n",
      "Training epoch 8045 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8045 ; accuracy: 0.7366666666666667; loss: 2.235807180404663\n",
      "Training epoch 8046 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8046 ; accuracy: 0.7366666666666667; loss: 2.235853672027588\n",
      "Training epoch 8047 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8047 ; accuracy: 0.7366666666666667; loss: 2.2359297275543213\n",
      "Training epoch 8048 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8048 ; accuracy: 0.7366666666666667; loss: 2.2360026836395264\n",
      "Training epoch 8049 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8049 ; accuracy: 0.7366666666666667; loss: 2.2360706329345703\n",
      "Training epoch 8050 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8050 ; accuracy: 0.7366666666666667; loss: 2.2361388206481934\n",
      "Training epoch 8051 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8051 ; accuracy: 0.7366666666666667; loss: 2.2361977100372314\n",
      "Training epoch 8052 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8052 ; accuracy: 0.7366666666666667; loss: 2.2362563610076904\n",
      "Training epoch 8053 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8053 ; accuracy: 0.7366666666666667; loss: 2.236316442489624\n",
      "Training epoch 8054 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8054 ; accuracy: 0.7366666666666667; loss: 2.2363662719726562\n",
      "Training epoch 8055 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8055 ; accuracy: 0.7366666666666667; loss: 2.2364139556884766\n",
      "Training epoch 8056 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8056 ; accuracy: 0.7366666666666667; loss: 2.2364604473114014\n",
      "Training epoch 8057 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8057 ; accuracy: 0.7366666666666667; loss: 2.236504077911377\n",
      "Training epoch 8058 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8058 ; accuracy: 0.7366666666666667; loss: 2.236551523208618\n",
      "Training epoch 8059 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8059 ; accuracy: 0.7366666666666667; loss: 2.2365965843200684\n",
      "Training epoch 8060 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8060 ; accuracy: 0.7366666666666667; loss: 2.236647129058838\n",
      "Training epoch 8061 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8061 ; accuracy: 0.7366666666666667; loss: 2.2366905212402344\n",
      "Training epoch 8062 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8062 ; accuracy: 0.7366666666666667; loss: 2.2367279529571533\n",
      "Training epoch 8063 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8063 ; accuracy: 0.7366666666666667; loss: 2.2367725372314453\n",
      "Training epoch 8064 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8064 ; accuracy: 0.7366666666666667; loss: 2.236815929412842\n",
      "Training epoch 8065 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8065 ; accuracy: 0.7366666666666667; loss: 2.236855983734131\n",
      "Training epoch 8066 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8066 ; accuracy: 0.7366666666666667; loss: 2.2368950843811035\n",
      "Training epoch 8067 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8067 ; accuracy: 0.7366666666666667; loss: 2.2369327545166016\n",
      "Training epoch 8068 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8068 ; accuracy: 0.7366666666666667; loss: 2.2369589805603027\n",
      "Training epoch 8069 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8069 ; accuracy: 0.7366666666666667; loss: 2.23698353767395\n",
      "Training epoch 8070 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8070 ; accuracy: 0.7366666666666667; loss: 2.2370123863220215\n",
      "Training epoch 8071 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8071 ; accuracy: 0.7366666666666667; loss: 2.237036943435669\n",
      "Training epoch 8072 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8072 ; accuracy: 0.7366666666666667; loss: 2.2370598316192627\n",
      "Training epoch 8073 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8073 ; accuracy: 0.7366666666666667; loss: 2.2370805740356445\n",
      "Training epoch 8074 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8074 ; accuracy: 0.7366666666666667; loss: 2.2370986938476562\n",
      "Training epoch 8075 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8075 ; accuracy: 0.7366666666666667; loss: 2.237114667892456\n",
      "Training epoch 8076 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8076 ; accuracy: 0.7366666666666667; loss: 2.237128734588623\n",
      "Training epoch 8077 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8077 ; accuracy: 0.7366666666666667; loss: 2.237147569656372\n",
      "Training epoch 8078 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8078 ; accuracy: 0.7366666666666667; loss: 2.237161159515381\n",
      "Training epoch 8079 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8079 ; accuracy: 0.7366666666666667; loss: 2.237159490585327\n",
      "Training epoch 8080 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8080 ; accuracy: 0.7366666666666667; loss: 2.2371559143066406\n",
      "Training epoch 8081 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8081 ; accuracy: 0.7366666666666667; loss: 2.2371532917022705\n",
      "Training epoch 8082 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8082 ; accuracy: 0.7366666666666667; loss: 2.237154006958008\n",
      "Training epoch 8083 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8083 ; accuracy: 0.7366666666666667; loss: 2.237149953842163\n",
      "Training epoch 8084 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8084 ; accuracy: 0.7366666666666667; loss: 2.237157106399536\n",
      "Training epoch 8085 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8085 ; accuracy: 0.7366666666666667; loss: 2.2371597290039062\n",
      "Training epoch 8086 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8086 ; accuracy: 0.7366666666666667; loss: 2.2371745109558105\n",
      "Training epoch 8087 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8087 ; accuracy: 0.7366666666666667; loss: 2.237182855606079\n",
      "Training epoch 8088 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8088 ; accuracy: 0.7366666666666667; loss: 2.237178087234497\n",
      "Training epoch 8089 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8089 ; accuracy: 0.7366666666666667; loss: 2.2371749877929688\n",
      "Training epoch 8090 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8090 ; accuracy: 0.7366666666666667; loss: 2.237177848815918\n",
      "Training epoch 8091 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8091 ; accuracy: 0.7366666666666667; loss: 2.2371950149536133\n",
      "Training epoch 8092 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8092 ; accuracy: 0.7366666666666667; loss: 2.2372026443481445\n",
      "Training epoch 8093 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8093 ; accuracy: 0.7366666666666667; loss: 2.2372148036956787\n",
      "Training epoch 8094 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8094 ; accuracy: 0.7366666666666667; loss: 2.2372241020202637\n",
      "Training epoch 8095 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8095 ; accuracy: 0.7366666666666667; loss: 2.237232208251953\n",
      "Training epoch 8096 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8096 ; accuracy: 0.7366666666666667; loss: 2.237238883972168\n",
      "Training epoch 8097 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8097 ; accuracy: 0.7366666666666667; loss: 2.2372524738311768\n",
      "Training epoch 8098 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8098 ; accuracy: 0.7366666666666667; loss: 2.23725962638855\n",
      "Training epoch 8099 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8099 ; accuracy: 0.7366666666666667; loss: 2.237273931503296\n",
      "Training epoch 8100 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8100 ; accuracy: 0.7366666666666667; loss: 2.2373087406158447\n",
      "Training epoch 8101 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8101 ; accuracy: 0.7366666666666667; loss: 2.2373428344726562\n",
      "Training epoch 8102 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8102 ; accuracy: 0.7366666666666667; loss: 2.2373709678649902\n",
      "Training epoch 8103 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8103 ; accuracy: 0.7366666666666667; loss: 2.2373974323272705\n",
      "Training epoch 8104 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8104 ; accuracy: 0.7366666666666667; loss: 2.2374165058135986\n",
      "Training epoch 8105 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8105 ; accuracy: 0.7366666666666667; loss: 2.2374215126037598\n",
      "Training epoch 8106 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8106 ; accuracy: 0.7366666666666667; loss: 2.2374236583709717\n",
      "Training epoch 8107 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8107 ; accuracy: 0.7366666666666667; loss: 2.2374305725097656\n",
      "Training epoch 8108 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8108 ; accuracy: 0.7366666666666667; loss: 2.2374343872070312\n",
      "Training epoch 8109 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8109 ; accuracy: 0.7366666666666667; loss: 2.237442970275879\n",
      "Training epoch 8110 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8110 ; accuracy: 0.7366666666666667; loss: 2.2374532222747803\n",
      "Training epoch 8111 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8111 ; accuracy: 0.7366666666666667; loss: 2.237462043762207\n",
      "Training epoch 8112 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8112 ; accuracy: 0.7366666666666667; loss: 2.237471103668213\n",
      "Training epoch 8113 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8113 ; accuracy: 0.7366666666666667; loss: 2.2374842166900635\n",
      "Training epoch 8114 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8114 ; accuracy: 0.7366666666666667; loss: 2.237501859664917\n",
      "Training epoch 8115 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8115 ; accuracy: 0.7366666666666667; loss: 2.2375199794769287\n",
      "Training epoch 8116 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8116 ; accuracy: 0.7366666666666667; loss: 2.237529993057251\n",
      "Training epoch 8117 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8117 ; accuracy: 0.7366666666666667; loss: 2.237541913986206\n",
      "Training epoch 8118 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8118 ; accuracy: 0.7366666666666667; loss: 2.237563371658325\n",
      "Training epoch 8119 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8119 ; accuracy: 0.7366666666666667; loss: 2.2375917434692383\n",
      "Training epoch 8120 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8120 ; accuracy: 0.7366666666666667; loss: 2.2376174926757812\n",
      "Training epoch 8121 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8121 ; accuracy: 0.7366666666666667; loss: 2.237643003463745\n",
      "Training epoch 8122 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8122 ; accuracy: 0.7366666666666667; loss: 2.237659215927124\n",
      "Training epoch 8123 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8123 ; accuracy: 0.7366666666666667; loss: 2.2376675605773926\n",
      "Training epoch 8124 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8124 ; accuracy: 0.7366666666666667; loss: 2.2376856803894043\n",
      "Training epoch 8125 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8125 ; accuracy: 0.7366666666666667; loss: 2.237703561782837\n",
      "Training epoch 8126 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8126 ; accuracy: 0.7366666666666667; loss: 2.2377185821533203\n",
      "Training epoch 8127 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8127 ; accuracy: 0.7366666666666667; loss: 2.237732410430908\n",
      "Training epoch 8128 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8128 ; accuracy: 0.7366666666666667; loss: 2.2377381324768066\n",
      "Training epoch 8129 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8129 ; accuracy: 0.7366666666666667; loss: 2.237748622894287\n",
      "Training epoch 8130 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8130 ; accuracy: 0.7366666666666667; loss: 2.2377707958221436\n",
      "Training epoch 8131 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8131 ; accuracy: 0.7366666666666667; loss: 2.237785577774048\n",
      "Training epoch 8132 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8132 ; accuracy: 0.7366666666666667; loss: 2.237795829772949\n",
      "Training epoch 8133 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8133 ; accuracy: 0.7366666666666667; loss: 2.2378177642822266\n",
      "Training epoch 8134 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8134 ; accuracy: 0.7366666666666667; loss: 2.2378361225128174\n",
      "Training epoch 8135 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8135 ; accuracy: 0.7366666666666667; loss: 2.2378482818603516\n",
      "Training epoch 8136 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8136 ; accuracy: 0.7366666666666667; loss: 2.2378571033477783\n",
      "Training epoch 8137 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8137 ; accuracy: 0.7366666666666667; loss: 2.237868547439575\n",
      "Training epoch 8138 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8138 ; accuracy: 0.7366666666666667; loss: 2.2378790378570557\n",
      "Training epoch 8139 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8139 ; accuracy: 0.7366666666666667; loss: 2.2378885746002197\n",
      "Training epoch 8140 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8140 ; accuracy: 0.7366666666666667; loss: 2.2379000186920166\n",
      "Training epoch 8141 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8141 ; accuracy: 0.7366666666666667; loss: 2.2379024028778076\n",
      "Training epoch 8142 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8142 ; accuracy: 0.7366666666666667; loss: 2.2378997802734375\n",
      "Training epoch 8143 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8143 ; accuracy: 0.7366666666666667; loss: 2.237901210784912\n",
      "Training epoch 8144 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8144 ; accuracy: 0.7366666666666667; loss: 2.2378897666931152\n",
      "Training epoch 8145 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8145 ; accuracy: 0.7366666666666667; loss: 2.237879514694214\n",
      "Training epoch 8146 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8146 ; accuracy: 0.7366666666666667; loss: 2.237868547439575\n",
      "Training epoch 8147 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8147 ; accuracy: 0.7366666666666667; loss: 2.2378573417663574\n",
      "Training epoch 8148 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8148 ; accuracy: 0.7366666666666667; loss: 2.2378530502319336\n",
      "Training epoch 8149 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8149 ; accuracy: 0.7366666666666667; loss: 2.2378547191619873\n",
      "Training epoch 8150 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8150 ; accuracy: 0.7366666666666667; loss: 2.2378501892089844\n",
      "Training epoch 8151 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8151 ; accuracy: 0.7366666666666667; loss: 2.2378575801849365\n",
      "Training epoch 8152 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8152 ; accuracy: 0.7366666666666667; loss: 2.237868547439575\n",
      "Training epoch 8153 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8153 ; accuracy: 0.7366666666666667; loss: 2.237875461578369\n",
      "Training epoch 8154 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8154 ; accuracy: 0.7366666666666667; loss: 2.2378926277160645\n",
      "Training epoch 8155 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8155 ; accuracy: 0.7366666666666667; loss: 2.2379097938537598\n",
      "Training epoch 8156 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8156 ; accuracy: 0.7366666666666667; loss: 2.2379188537597656\n",
      "Training epoch 8157 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8157 ; accuracy: 0.7366666666666667; loss: 2.2379276752471924\n",
      "Training epoch 8158 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8158 ; accuracy: 0.7366666666666667; loss: 2.2379355430603027\n",
      "Training epoch 8159 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8159 ; accuracy: 0.7366666666666667; loss: 2.2379393577575684\n",
      "Training epoch 8160 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8160 ; accuracy: 0.7366666666666667; loss: 2.237950086593628\n",
      "Training epoch 8161 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8161 ; accuracy: 0.7366666666666667; loss: 2.2379486560821533\n",
      "Training epoch 8162 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8162 ; accuracy: 0.7366666666666667; loss: 2.2379467487335205\n",
      "Training epoch 8163 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8163 ; accuracy: 0.7366666666666667; loss: 2.2379212379455566\n",
      "Training epoch 8164 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8164 ; accuracy: 0.7366666666666667; loss: 2.2379050254821777\n",
      "Training epoch 8165 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8165 ; accuracy: 0.7366666666666667; loss: 2.237881898880005\n",
      "Training epoch 8166 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8166 ; accuracy: 0.7366666666666667; loss: 2.237860918045044\n",
      "Training epoch 8167 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8167 ; accuracy: 0.7366666666666667; loss: 2.2378556728363037\n",
      "Training epoch 8168 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8168 ; accuracy: 0.7366666666666667; loss: 2.2378621101379395\n",
      "Training epoch 8169 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8169 ; accuracy: 0.7366666666666667; loss: 2.2378711700439453\n",
      "Training epoch 8170 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8170 ; accuracy: 0.7366666666666667; loss: 2.2378828525543213\n",
      "Training epoch 8171 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8171 ; accuracy: 0.7366666666666667; loss: 2.2379016876220703\n",
      "Training epoch 8172 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8172 ; accuracy: 0.7366666666666667; loss: 2.237905979156494\n",
      "Training epoch 8173 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8173 ; accuracy: 0.7366666666666667; loss: 2.2379143238067627\n",
      "Training epoch 8174 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8174 ; accuracy: 0.7366666666666667; loss: 2.2379202842712402\n",
      "Training epoch 8175 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8175 ; accuracy: 0.7366666666666667; loss: 2.2379257678985596\n",
      "Training epoch 8176 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8176 ; accuracy: 0.7366666666666667; loss: 2.2379310131073\n",
      "Training epoch 8177 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8177 ; accuracy: 0.7366666666666667; loss: 2.2379279136657715\n",
      "Training epoch 8178 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8178 ; accuracy: 0.7366666666666667; loss: 2.237927198410034\n",
      "Training epoch 8179 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8179 ; accuracy: 0.7366666666666667; loss: 2.237924814224243\n",
      "Training epoch 8180 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8180 ; accuracy: 0.7366666666666667; loss: 2.237924337387085\n",
      "Training epoch 8181 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8181 ; accuracy: 0.7366666666666667; loss: 2.2379214763641357\n",
      "Training epoch 8182 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8182 ; accuracy: 0.7366666666666667; loss: 2.2379257678985596\n",
      "Training epoch 8183 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8183 ; accuracy: 0.7366666666666667; loss: 2.2379353046417236\n",
      "Training epoch 8184 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8184 ; accuracy: 0.7366666666666667; loss: 2.2379393577575684\n",
      "Training epoch 8185 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8185 ; accuracy: 0.7366666666666667; loss: 2.2379462718963623\n",
      "Training epoch 8186 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8186 ; accuracy: 0.7366666666666667; loss: 2.2379589080810547\n",
      "Training epoch 8187 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8187 ; accuracy: 0.7366666666666667; loss: 2.2379627227783203\n",
      "Training epoch 8188 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8188 ; accuracy: 0.7366666666666667; loss: 2.23796010017395\n",
      "Training epoch 8189 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8189 ; accuracy: 0.7366666666666667; loss: 2.2379560470581055\n",
      "Training epoch 8190 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8190 ; accuracy: 0.7366666666666667; loss: 2.23795223236084\n",
      "Training epoch 8191 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8191 ; accuracy: 0.7366666666666667; loss: 2.2379469871520996\n",
      "Training epoch 8192 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8192 ; accuracy: 0.7366666666666667; loss: 2.2379419803619385\n",
      "Training epoch 8193 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8193 ; accuracy: 0.7366666666666667; loss: 2.23793625831604\n",
      "Training epoch 8194 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8194 ; accuracy: 0.7366666666666667; loss: 2.237928628921509\n",
      "Training epoch 8195 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8195 ; accuracy: 0.7366666666666667; loss: 2.2379283905029297\n",
      "Training epoch 8196 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8196 ; accuracy: 0.7366666666666667; loss: 2.2379226684570312\n",
      "Training epoch 8197 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8197 ; accuracy: 0.7366666666666667; loss: 2.2379255294799805\n",
      "Training epoch 8198 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8198 ; accuracy: 0.7366666666666667; loss: 2.2379324436187744\n",
      "Training epoch 8199 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8199 ; accuracy: 0.7366666666666667; loss: 2.2379372119903564\n",
      "Training epoch 8200 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8200 ; accuracy: 0.7366666666666667; loss: 2.237940788269043\n",
      "Training epoch 8201 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8201 ; accuracy: 0.7366666666666667; loss: 2.2379448413848877\n",
      "Training epoch 8202 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8202 ; accuracy: 0.7366666666666667; loss: 2.23795485496521\n",
      "Training epoch 8203 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8203 ; accuracy: 0.7366666666666667; loss: 2.2379648685455322\n",
      "Training epoch 8204 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8204 ; accuracy: 0.7366666666666667; loss: 2.2379813194274902\n",
      "Training epoch 8205 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8205 ; accuracy: 0.7366666666666667; loss: 2.2379982471466064\n",
      "Training epoch 8206 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8206 ; accuracy: 0.7366666666666667; loss: 2.2380106449127197\n",
      "Training epoch 8207 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8207 ; accuracy: 0.7366666666666667; loss: 2.238015651702881\n",
      "Training epoch 8208 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8208 ; accuracy: 0.7366666666666667; loss: 2.238023042678833\n",
      "Training epoch 8209 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8209 ; accuracy: 0.7366666666666667; loss: 2.2380290031433105\n",
      "Training epoch 8210 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8210 ; accuracy: 0.7366666666666667; loss: 2.238032817840576\n",
      "Training epoch 8211 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8211 ; accuracy: 0.7366666666666667; loss: 2.2380404472351074\n",
      "Training epoch 8212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8212 ; accuracy: 0.7366666666666667; loss: 2.2380502223968506\n",
      "Training epoch 8213 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8213 ; accuracy: 0.7366666666666667; loss: 2.238076686859131\n",
      "Training epoch 8214 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8214 ; accuracy: 0.7366666666666667; loss: 2.2380917072296143\n",
      "Training epoch 8215 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8215 ; accuracy: 0.7366666666666667; loss: 2.238102674484253\n",
      "Training epoch 8216 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8216 ; accuracy: 0.7366666666666667; loss: 2.238119602203369\n",
      "Training epoch 8217 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8217 ; accuracy: 0.7366666666666667; loss: 2.238128900527954\n",
      "Training epoch 8218 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8218 ; accuracy: 0.7366666666666667; loss: 2.2381515502929688\n",
      "Training epoch 8219 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8219 ; accuracy: 0.7366666666666667; loss: 2.238168239593506\n",
      "Training epoch 8220 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8220 ; accuracy: 0.7366666666666667; loss: 2.2382028102874756\n",
      "Training epoch 8221 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8221 ; accuracy: 0.7366666666666667; loss: 2.238237142562866\n",
      "Training epoch 8222 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8222 ; accuracy: 0.7366666666666667; loss: 2.2382709980010986\n",
      "Training epoch 8223 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8223 ; accuracy: 0.7366666666666667; loss: 2.23832106590271\n",
      "Training epoch 8224 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8224 ; accuracy: 0.7366666666666667; loss: 2.238368511199951\n",
      "Training epoch 8225 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8225 ; accuracy: 0.7366666666666667; loss: 2.238415479660034\n",
      "Training epoch 8226 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8226 ; accuracy: 0.7366666666666667; loss: 2.2384727001190186\n",
      "Training epoch 8227 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8227 ; accuracy: 0.7366666666666667; loss: 2.2385311126708984\n",
      "Training epoch 8228 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8228 ; accuracy: 0.7366666666666667; loss: 2.2385809421539307\n",
      "Training epoch 8229 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8229 ; accuracy: 0.7366666666666667; loss: 2.2386248111724854\n",
      "Training epoch 8230 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8230 ; accuracy: 0.7366666666666667; loss: 2.238659143447876\n",
      "Training epoch 8231 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8231 ; accuracy: 0.7366666666666667; loss: 2.238696336746216\n",
      "Training epoch 8232 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8232 ; accuracy: 0.7366666666666667; loss: 2.2387332916259766\n",
      "Training epoch 8233 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8233 ; accuracy: 0.7366666666666667; loss: 2.238771438598633\n",
      "Training epoch 8234 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8234 ; accuracy: 0.7366666666666667; loss: 2.238816022872925\n",
      "Training epoch 8235 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8235 ; accuracy: 0.7366666666666667; loss: 2.238863945007324\n",
      "Training epoch 8236 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8236 ; accuracy: 0.7366666666666667; loss: 2.238896369934082\n",
      "Training epoch 8237 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8237 ; accuracy: 0.7366666666666667; loss: 2.2389354705810547\n",
      "Training epoch 8238 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8238 ; accuracy: 0.7366666666666667; loss: 2.2389681339263916\n",
      "Training epoch 8239 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8239 ; accuracy: 0.7366666666666667; loss: 2.2389984130859375\n",
      "Training epoch 8240 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8240 ; accuracy: 0.7366666666666667; loss: 2.239029884338379\n",
      "Training epoch 8241 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8241 ; accuracy: 0.7366666666666667; loss: 2.2390551567077637\n",
      "Training epoch 8242 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8242 ; accuracy: 0.7366666666666667; loss: 2.2390694618225098\n",
      "Training epoch 8243 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8243 ; accuracy: 0.7366666666666667; loss: 2.2390904426574707\n",
      "Training epoch 8244 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8244 ; accuracy: 0.7366666666666667; loss: 2.239107370376587\n",
      "Training epoch 8245 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8245 ; accuracy: 0.7366666666666667; loss: 2.2391281127929688\n",
      "Training epoch 8246 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8246 ; accuracy: 0.7366666666666667; loss: 2.239149808883667\n",
      "Training epoch 8247 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8247 ; accuracy: 0.7366666666666667; loss: 2.2391741275787354\n",
      "Training epoch 8248 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8248 ; accuracy: 0.7366666666666667; loss: 2.239196300506592\n",
      "Training epoch 8249 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8249 ; accuracy: 0.7366666666666667; loss: 2.2392332553863525\n",
      "Training epoch 8250 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8250 ; accuracy: 0.7366666666666667; loss: 2.2392561435699463\n",
      "Training epoch 8251 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8251 ; accuracy: 0.7366666666666667; loss: 2.2392752170562744\n",
      "Training epoch 8252 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8252 ; accuracy: 0.7366666666666667; loss: 2.239285469055176\n",
      "Training epoch 8253 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8253 ; accuracy: 0.7366666666666667; loss: 2.239276647567749\n",
      "Training epoch 8254 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8254 ; accuracy: 0.7366666666666667; loss: 2.2392661571502686\n",
      "Training epoch 8255 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8255 ; accuracy: 0.7366666666666667; loss: 2.2392568588256836\n",
      "Training epoch 8256 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8256 ; accuracy: 0.7366666666666667; loss: 2.2392492294311523\n",
      "Training epoch 8257 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8257 ; accuracy: 0.7366666666666667; loss: 2.2392473220825195\n",
      "Training epoch 8258 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8258 ; accuracy: 0.7366666666666667; loss: 2.2392525672912598\n",
      "Training epoch 8259 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8259 ; accuracy: 0.7366666666666667; loss: 2.239255666732788\n",
      "Training epoch 8260 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8260 ; accuracy: 0.7366666666666667; loss: 2.2392611503601074\n",
      "Training epoch 8261 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8261 ; accuracy: 0.7366666666666667; loss: 2.2392618656158447\n",
      "Training epoch 8262 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8262 ; accuracy: 0.7366666666666667; loss: 2.239264488220215\n",
      "Training epoch 8263 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8263 ; accuracy: 0.7366666666666667; loss: 2.2392654418945312\n",
      "Training epoch 8264 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8264 ; accuracy: 0.7366666666666667; loss: 2.2392654418945312\n",
      "Training epoch 8265 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8265 ; accuracy: 0.7366666666666667; loss: 2.2392709255218506\n",
      "Training epoch 8266 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8266 ; accuracy: 0.7366666666666667; loss: 2.239271879196167\n",
      "Training epoch 8267 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8267 ; accuracy: 0.7366666666666667; loss: 2.239272356033325\n",
      "Training epoch 8268 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8268 ; accuracy: 0.7366666666666667; loss: 2.2392773628234863\n",
      "Training epoch 8269 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8269 ; accuracy: 0.7366666666666667; loss: 2.2392778396606445\n",
      "Training epoch 8270 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8270 ; accuracy: 0.7366666666666667; loss: 2.239267110824585\n",
      "Training epoch 8271 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8271 ; accuracy: 0.7366666666666667; loss: 2.239262580871582\n",
      "Training epoch 8272 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8272 ; accuracy: 0.7366666666666667; loss: 2.23926043510437\n",
      "Training epoch 8273 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8273 ; accuracy: 0.7366666666666667; loss: 2.2392663955688477\n",
      "Training epoch 8274 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8274 ; accuracy: 0.7366666666666667; loss: 2.2392704486846924\n",
      "Training epoch 8275 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8275 ; accuracy: 0.7366666666666667; loss: 2.239274740219116\n",
      "Training epoch 8276 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8276 ; accuracy: 0.7366666666666667; loss: 2.2392680644989014\n",
      "Training epoch 8277 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8277 ; accuracy: 0.7366666666666667; loss: 2.2392661571502686\n",
      "Training epoch 8278 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8278 ; accuracy: 0.7366666666666667; loss: 2.239265203475952\n",
      "Training epoch 8279 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8279 ; accuracy: 0.7366666666666667; loss: 2.2392683029174805\n",
      "Training epoch 8280 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8280 ; accuracy: 0.7366666666666667; loss: 2.2392640113830566\n",
      "Training epoch 8281 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8281 ; accuracy: 0.7366666666666667; loss: 2.2392578125\n",
      "Training epoch 8282 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8282 ; accuracy: 0.7366666666666667; loss: 2.2392585277557373\n",
      "Training epoch 8283 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8283 ; accuracy: 0.7366666666666667; loss: 2.239260196685791\n",
      "Training epoch 8284 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8284 ; accuracy: 0.7366666666666667; loss: 2.239267110824585\n",
      "Training epoch 8285 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8285 ; accuracy: 0.7366666666666667; loss: 2.2392795085906982\n",
      "Training epoch 8286 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8286 ; accuracy: 0.7366666666666667; loss: 2.2392921447753906\n",
      "Training epoch 8287 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8287 ; accuracy: 0.7366666666666667; loss: 2.239309072494507\n",
      "Training epoch 8288 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8288 ; accuracy: 0.7366666666666667; loss: 2.2393243312835693\n",
      "Training epoch 8289 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8289 ; accuracy: 0.7366666666666667; loss: 2.2393417358398438\n",
      "Training epoch 8290 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8290 ; accuracy: 0.7366666666666667; loss: 2.2393598556518555\n",
      "Training epoch 8291 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8291 ; accuracy: 0.7366666666666667; loss: 2.23937726020813\n",
      "Training epoch 8292 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8292 ; accuracy: 0.7366666666666667; loss: 2.239393472671509\n",
      "Training epoch 8293 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8293 ; accuracy: 0.7366666666666667; loss: 2.239401340484619\n",
      "Training epoch 8294 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8294 ; accuracy: 0.7366666666666667; loss: 2.239409923553467\n",
      "Training epoch 8295 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8295 ; accuracy: 0.7366666666666667; loss: 2.23941707611084\n",
      "Training epoch 8296 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8296 ; accuracy: 0.7366666666666667; loss: 2.2394256591796875\n",
      "Training epoch 8297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8297 ; accuracy: 0.7366666666666667; loss: 2.2394347190856934\n",
      "Training epoch 8298 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8298 ; accuracy: 0.7366666666666667; loss: 2.239443063735962\n",
      "Training epoch 8299 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8299 ; accuracy: 0.7366666666666667; loss: 2.2394375801086426\n",
      "Training epoch 8300 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8300 ; accuracy: 0.7366666666666667; loss: 2.2394323348999023\n",
      "Training epoch 8301 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8301 ; accuracy: 0.7366666666666667; loss: 2.239421844482422\n",
      "Training epoch 8302 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8302 ; accuracy: 0.7366666666666667; loss: 2.2394118309020996\n",
      "Training epoch 8303 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8303 ; accuracy: 0.7366666666666667; loss: 2.2393875122070312\n",
      "Training epoch 8304 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8304 ; accuracy: 0.7366666666666667; loss: 2.2393639087677\n",
      "Training epoch 8305 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8305 ; accuracy: 0.7366666666666667; loss: 2.2393457889556885\n",
      "Training epoch 8306 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8306 ; accuracy: 0.7366666666666667; loss: 2.239333152770996\n",
      "Training epoch 8307 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8307 ; accuracy: 0.7366666666666667; loss: 2.2393243312835693\n",
      "Training epoch 8308 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8308 ; accuracy: 0.7366666666666667; loss: 2.239328145980835\n",
      "Training epoch 8309 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8309 ; accuracy: 0.7366666666666667; loss: 2.239337921142578\n",
      "Training epoch 8310 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8310 ; accuracy: 0.7366666666666667; loss: 2.2393600940704346\n",
      "Training epoch 8311 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8311 ; accuracy: 0.7366666666666667; loss: 2.2393832206726074\n",
      "Training epoch 8312 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8312 ; accuracy: 0.7366666666666667; loss: 2.2394042015075684\n",
      "Training epoch 8313 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8313 ; accuracy: 0.7366666666666667; loss: 2.2394306659698486\n",
      "Training epoch 8314 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8314 ; accuracy: 0.7366666666666667; loss: 2.239461898803711\n",
      "Training epoch 8315 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8315 ; accuracy: 0.7366666666666667; loss: 2.2394912242889404\n",
      "Training epoch 8316 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8316 ; accuracy: 0.7366666666666667; loss: 2.2395169734954834\n",
      "Training epoch 8317 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8317 ; accuracy: 0.7366666666666667; loss: 2.239541530609131\n",
      "Training epoch 8318 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8318 ; accuracy: 0.7366666666666667; loss: 2.2395615577697754\n",
      "Training epoch 8319 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8319 ; accuracy: 0.7366666666666667; loss: 2.2395691871643066\n",
      "Training epoch 8320 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8320 ; accuracy: 0.7366666666666667; loss: 2.2395741939544678\n",
      "Training epoch 8321 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8321 ; accuracy: 0.7366666666666667; loss: 2.2395873069763184\n",
      "Training epoch 8322 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8322 ; accuracy: 0.7366666666666667; loss: 2.239602565765381\n",
      "Training epoch 8323 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8323 ; accuracy: 0.7366666666666667; loss: 2.2396011352539062\n",
      "Training epoch 8324 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8324 ; accuracy: 0.7366666666666667; loss: 2.239595413208008\n",
      "Training epoch 8325 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8325 ; accuracy: 0.7366666666666667; loss: 2.239579916000366\n",
      "Training epoch 8326 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8326 ; accuracy: 0.7366666666666667; loss: 2.239570140838623\n",
      "Training epoch 8327 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8327 ; accuracy: 0.7366666666666667; loss: 2.2395691871643066\n",
      "Training epoch 8328 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8328 ; accuracy: 0.7366666666666667; loss: 2.2395753860473633\n",
      "Training epoch 8329 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8329 ; accuracy: 0.7366666666666667; loss: 2.2395787239074707\n",
      "Training epoch 8330 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8330 ; accuracy: 0.7366666666666667; loss: 2.2395663261413574\n",
      "Training epoch 8331 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8331 ; accuracy: 0.7366666666666667; loss: 2.2395668029785156\n",
      "Training epoch 8332 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8332 ; accuracy: 0.7366666666666667; loss: 2.239572048187256\n",
      "Training epoch 8333 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8333 ; accuracy: 0.7366666666666667; loss: 2.2395761013031006\n",
      "Training epoch 8334 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8334 ; accuracy: 0.7366666666666667; loss: 2.2395968437194824\n",
      "Training epoch 8335 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8335 ; accuracy: 0.7366666666666667; loss: 2.2396128177642822\n",
      "Training epoch 8336 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8336 ; accuracy: 0.7366666666666667; loss: 2.239629030227661\n",
      "Training epoch 8337 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8337 ; accuracy: 0.7366666666666667; loss: 2.239651679992676\n",
      "Training epoch 8338 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8338 ; accuracy: 0.7366666666666667; loss: 2.2397549152374268\n",
      "Training epoch 8339 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8339 ; accuracy: 0.7366666666666667; loss: 2.2398481369018555\n",
      "Training epoch 8340 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8340 ; accuracy: 0.7366666666666667; loss: 2.2399423122406006\n",
      "Training epoch 8341 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8341 ; accuracy: 0.7366666666666667; loss: 2.2400269508361816\n",
      "Training epoch 8342 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8342 ; accuracy: 0.7366666666666667; loss: 2.2401082515716553\n",
      "Training epoch 8343 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8343 ; accuracy: 0.7366666666666667; loss: 2.240190029144287\n",
      "Training epoch 8344 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8344 ; accuracy: 0.7366666666666667; loss: 2.240271806716919\n",
      "Training epoch 8345 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8345 ; accuracy: 0.7366666666666667; loss: 2.240346908569336\n",
      "Training epoch 8346 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8346 ; accuracy: 0.7366666666666667; loss: 2.240415573120117\n",
      "Training epoch 8347 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8347 ; accuracy: 0.7366666666666667; loss: 2.240473747253418\n",
      "Training epoch 8348 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8348 ; accuracy: 0.7366666666666667; loss: 2.240518093109131\n",
      "Training epoch 8349 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8349 ; accuracy: 0.7366666666666667; loss: 2.2405619621276855\n",
      "Training epoch 8350 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8350 ; accuracy: 0.7366666666666667; loss: 2.2405989170074463\n",
      "Training epoch 8351 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8351 ; accuracy: 0.7366666666666667; loss: 2.2406296730041504\n",
      "Training epoch 8352 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8352 ; accuracy: 0.7366666666666667; loss: 2.2406678199768066\n",
      "Training epoch 8353 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8353 ; accuracy: 0.7366666666666667; loss: 2.240691900253296\n",
      "Training epoch 8354 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8354 ; accuracy: 0.7366666666666667; loss: 2.2406959533691406\n",
      "Training epoch 8355 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8355 ; accuracy: 0.7366666666666667; loss: 2.2406980991363525\n",
      "Training epoch 8356 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8356 ; accuracy: 0.7366666666666667; loss: 2.2407023906707764\n",
      "Training epoch 8357 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8357 ; accuracy: 0.7366666666666667; loss: 2.240701675415039\n",
      "Training epoch 8358 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8358 ; accuracy: 0.7366666666666667; loss: 2.240696907043457\n",
      "Training epoch 8359 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8359 ; accuracy: 0.7366666666666667; loss: 2.240701675415039\n",
      "Training epoch 8360 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8360 ; accuracy: 0.7366666666666667; loss: 2.240694284439087\n",
      "Training epoch 8361 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8361 ; accuracy: 0.7366666666666667; loss: 2.2406840324401855\n",
      "Training epoch 8362 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8362 ; accuracy: 0.7366666666666667; loss: 2.240680694580078\n",
      "Training epoch 8363 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8363 ; accuracy: 0.7366666666666667; loss: 2.2406864166259766\n",
      "Training epoch 8364 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8364 ; accuracy: 0.7366666666666667; loss: 2.2406833171844482\n",
      "Training epoch 8365 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8365 ; accuracy: 0.7366666666666667; loss: 2.240685224533081\n",
      "Training epoch 8366 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8366 ; accuracy: 0.7366666666666667; loss: 2.240694522857666\n",
      "Training epoch 8367 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8367 ; accuracy: 0.7366666666666667; loss: 2.240697145462036\n",
      "Training epoch 8368 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8368 ; accuracy: 0.7366666666666667; loss: 2.240706205368042\n",
      "Training epoch 8369 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8369 ; accuracy: 0.7366666666666667; loss: 2.2407121658325195\n",
      "Training epoch 8370 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8370 ; accuracy: 0.7366666666666667; loss: 2.240705728530884\n",
      "Training epoch 8371 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8371 ; accuracy: 0.7366666666666667; loss: 2.2406978607177734\n",
      "Training epoch 8372 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8372 ; accuracy: 0.7366666666666667; loss: 2.240713596343994\n",
      "Training epoch 8373 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8373 ; accuracy: 0.7366666666666667; loss: 2.2407240867614746\n",
      "Training epoch 8374 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8374 ; accuracy: 0.7366666666666667; loss: 2.2407350540161133\n",
      "Training epoch 8375 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8375 ; accuracy: 0.7366666666666667; loss: 2.240741491317749\n",
      "Training epoch 8376 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8376 ; accuracy: 0.7366666666666667; loss: 2.2407567501068115\n",
      "Training epoch 8377 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8377 ; accuracy: 0.7366666666666667; loss: 2.240779161453247\n",
      "Training epoch 8378 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8378 ; accuracy: 0.7366666666666667; loss: 2.2407944202423096\n",
      "Training epoch 8379 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8379 ; accuracy: 0.7366666666666667; loss: 2.240795135498047\n",
      "Training epoch 8380 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8380 ; accuracy: 0.7366666666666667; loss: 2.240784168243408\n",
      "Training epoch 8381 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8381 ; accuracy: 0.7366666666666667; loss: 2.2407751083374023\n",
      "Training epoch 8382 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8382 ; accuracy: 0.7366666666666667; loss: 2.240767478942871\n",
      "Training epoch 8383 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8383 ; accuracy: 0.7366666666666667; loss: 2.24076509475708\n",
      "Training epoch 8384 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8384 ; accuracy: 0.7366666666666667; loss: 2.2407593727111816\n",
      "Training epoch 8385 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8385 ; accuracy: 0.7366666666666667; loss: 2.240762233734131\n",
      "Training epoch 8386 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8386 ; accuracy: 0.7366666666666667; loss: 2.240762233734131\n",
      "Training epoch 8387 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8387 ; accuracy: 0.7366666666666667; loss: 2.2407772541046143\n",
      "Training epoch 8388 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8388 ; accuracy: 0.7366666666666667; loss: 2.2408039569854736\n",
      "Training epoch 8389 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8389 ; accuracy: 0.7366666666666667; loss: 2.2408244609832764\n",
      "Training epoch 8390 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8390 ; accuracy: 0.7366666666666667; loss: 2.240856885910034\n",
      "Training epoch 8391 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8391 ; accuracy: 0.7366666666666667; loss: 2.240882635116577\n",
      "Training epoch 8392 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8392 ; accuracy: 0.7366666666666667; loss: 2.2409021854400635\n",
      "Training epoch 8393 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8393 ; accuracy: 0.7366666666666667; loss: 2.240924835205078\n",
      "Training epoch 8394 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8394 ; accuracy: 0.7366666666666667; loss: 2.2409398555755615\n",
      "Training epoch 8395 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8395 ; accuracy: 0.7366666666666667; loss: 2.240959644317627\n",
      "Training epoch 8396 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8396 ; accuracy: 0.7366666666666667; loss: 2.2409744262695312\n",
      "Training epoch 8397 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8397 ; accuracy: 0.7366666666666667; loss: 2.24098539352417\n",
      "Training epoch 8398 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8398 ; accuracy: 0.7366666666666667; loss: 2.241001844406128\n",
      "Training epoch 8399 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8399 ; accuracy: 0.7366666666666667; loss: 2.2410240173339844\n",
      "Training epoch 8400 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8400 ; accuracy: 0.7366666666666667; loss: 2.241044044494629\n",
      "Training epoch 8401 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8401 ; accuracy: 0.7366666666666667; loss: 2.2410688400268555\n",
      "Training epoch 8402 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8402 ; accuracy: 0.7366666666666667; loss: 2.241091012954712\n",
      "Training epoch 8403 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8403 ; accuracy: 0.7366666666666667; loss: 2.2411134243011475\n",
      "Training epoch 8404 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8404 ; accuracy: 0.7366666666666667; loss: 2.241154670715332\n",
      "Training epoch 8405 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8405 ; accuracy: 0.7366666666666667; loss: 2.241196393966675\n",
      "Training epoch 8406 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8406 ; accuracy: 0.7366666666666667; loss: 2.2412402629852295\n",
      "Training epoch 8407 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8407 ; accuracy: 0.7366666666666667; loss: 2.241286039352417\n",
      "Training epoch 8408 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8408 ; accuracy: 0.7366666666666667; loss: 2.2413241863250732\n",
      "Training epoch 8409 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8409 ; accuracy: 0.7366666666666667; loss: 2.241349935531616\n",
      "Training epoch 8410 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8410 ; accuracy: 0.7366666666666667; loss: 2.24137544631958\n",
      "Training epoch 8411 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8411 ; accuracy: 0.7366666666666667; loss: 2.241396903991699\n",
      "Training epoch 8412 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8412 ; accuracy: 0.7366666666666667; loss: 2.241431474685669\n",
      "Training epoch 8413 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8413 ; accuracy: 0.7366666666666667; loss: 2.2414615154266357\n",
      "Training epoch 8414 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8414 ; accuracy: 0.7366666666666667; loss: 2.241494655609131\n",
      "Training epoch 8415 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8415 ; accuracy: 0.7366666666666667; loss: 2.24151349067688\n",
      "Training epoch 8416 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8416 ; accuracy: 0.7366666666666667; loss: 2.2415308952331543\n",
      "Training epoch 8417 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8417 ; accuracy: 0.7366666666666667; loss: 2.2415459156036377\n",
      "Training epoch 8418 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8418 ; accuracy: 0.7366666666666667; loss: 2.2415623664855957\n",
      "Training epoch 8419 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8419 ; accuracy: 0.7366666666666667; loss: 2.2415733337402344\n",
      "Training epoch 8420 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8420 ; accuracy: 0.7366666666666667; loss: 2.2415850162506104\n",
      "Training epoch 8421 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8421 ; accuracy: 0.7366666666666667; loss: 2.241600513458252\n",
      "Training epoch 8422 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8422 ; accuracy: 0.7366666666666667; loss: 2.2416155338287354\n",
      "Training epoch 8423 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8423 ; accuracy: 0.7366666666666667; loss: 2.2416138648986816\n",
      "Training epoch 8424 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8424 ; accuracy: 0.7366666666666667; loss: 2.241609573364258\n",
      "Training epoch 8425 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8425 ; accuracy: 0.7366666666666667; loss: 2.2416117191314697\n",
      "Training epoch 8426 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8426 ; accuracy: 0.7366666666666667; loss: 2.241671323776245\n",
      "Training epoch 8427 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8427 ; accuracy: 0.7366666666666667; loss: 2.241734266281128\n",
      "Training epoch 8428 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8428 ; accuracy: 0.7366666666666667; loss: 2.2417917251586914\n",
      "Training epoch 8429 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8429 ; accuracy: 0.7366666666666667; loss: 2.2418456077575684\n",
      "Training epoch 8430 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8430 ; accuracy: 0.7366666666666667; loss: 2.2418856620788574\n",
      "Training epoch 8431 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8431 ; accuracy: 0.7366666666666667; loss: 2.2419211864471436\n",
      "Training epoch 8432 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8432 ; accuracy: 0.7366666666666667; loss: 2.241948127746582\n",
      "Training epoch 8433 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8433 ; accuracy: 0.7366666666666667; loss: 2.24198317527771\n",
      "Training epoch 8434 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8434 ; accuracy: 0.7366666666666667; loss: 2.242013454437256\n",
      "Training epoch 8435 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8435 ; accuracy: 0.7366666666666667; loss: 2.2420458793640137\n",
      "Training epoch 8436 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8436 ; accuracy: 0.7366666666666667; loss: 2.2420706748962402\n",
      "Training epoch 8437 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8437 ; accuracy: 0.7366666666666667; loss: 2.2420828342437744\n",
      "Training epoch 8438 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8438 ; accuracy: 0.7366666666666667; loss: 2.242091655731201\n",
      "Training epoch 8439 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8439 ; accuracy: 0.7366666666666667; loss: 2.2420966625213623\n",
      "Training epoch 8440 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8440 ; accuracy: 0.7366666666666667; loss: 2.242098569869995\n",
      "Training epoch 8441 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8441 ; accuracy: 0.7366666666666667; loss: 2.24208927154541\n",
      "Training epoch 8442 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8442 ; accuracy: 0.7366666666666667; loss: 2.2420756816864014\n",
      "Training epoch 8443 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8443 ; accuracy: 0.7366666666666667; loss: 2.242063522338867\n",
      "Training epoch 8444 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8444 ; accuracy: 0.7366666666666667; loss: 2.2420554161071777\n",
      "Training epoch 8445 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8445 ; accuracy: 0.7366666666666667; loss: 2.242034912109375\n",
      "Training epoch 8446 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8446 ; accuracy: 0.7366666666666667; loss: 2.2420265674591064\n",
      "Training epoch 8447 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8447 ; accuracy: 0.7366666666666667; loss: 2.2420108318328857\n",
      "Training epoch 8448 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8448 ; accuracy: 0.7366666666666667; loss: 2.241992712020874\n",
      "Training epoch 8449 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8449 ; accuracy: 0.7366666666666667; loss: 2.241978406906128\n",
      "Training epoch 8450 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8450 ; accuracy: 0.7366666666666667; loss: 2.2419795989990234\n",
      "Training epoch 8451 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8451 ; accuracy: 0.7366666666666667; loss: 2.241987466812134\n",
      "Training epoch 8452 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8452 ; accuracy: 0.7366666666666667; loss: 2.241992950439453\n",
      "Training epoch 8453 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8453 ; accuracy: 0.7366666666666667; loss: 2.2420027256011963\n",
      "Training epoch 8454 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8454 ; accuracy: 0.7366666666666667; loss: 2.242013454437256\n",
      "Training epoch 8455 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8455 ; accuracy: 0.7366666666666667; loss: 2.2420125007629395\n",
      "Training epoch 8456 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8456 ; accuracy: 0.7366666666666667; loss: 2.2420177459716797\n",
      "Training epoch 8457 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8457 ; accuracy: 0.7366666666666667; loss: 2.2420148849487305\n",
      "Training epoch 8458 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8458 ; accuracy: 0.7366666666666667; loss: 2.2420120239257812\n",
      "Training epoch 8459 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8459 ; accuracy: 0.7366666666666667; loss: 2.242013692855835\n",
      "Training epoch 8460 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8460 ; accuracy: 0.7366666666666667; loss: 2.2420034408569336\n",
      "Training epoch 8461 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8461 ; accuracy: 0.7366666666666667; loss: 2.242004871368408\n",
      "Training epoch 8462 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8462 ; accuracy: 0.7366666666666667; loss: 2.242006301879883\n",
      "Training epoch 8463 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8463 ; accuracy: 0.7366666666666667; loss: 2.242042303085327\n",
      "Training epoch 8464 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8464 ; accuracy: 0.7366666666666667; loss: 2.2420711517333984\n",
      "Training epoch 8465 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8465 ; accuracy: 0.7366666666666667; loss: 2.242096424102783\n",
      "Training epoch 8466 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8466 ; accuracy: 0.7366666666666667; loss: 2.242112636566162\n",
      "Training epoch 8467 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8467 ; accuracy: 0.7366666666666667; loss: 2.242128372192383\n",
      "Training epoch 8468 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8468 ; accuracy: 0.7366666666666667; loss: 2.2421562671661377\n",
      "Training epoch 8469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8469 ; accuracy: 0.7366666666666667; loss: 2.2421841621398926\n",
      "Training epoch 8470 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8470 ; accuracy: 0.7366666666666667; loss: 2.2422101497650146\n",
      "Training epoch 8471 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8471 ; accuracy: 0.7366666666666667; loss: 2.242234230041504\n",
      "Training epoch 8472 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8472 ; accuracy: 0.7366666666666667; loss: 2.24224591255188\n",
      "Training epoch 8473 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8473 ; accuracy: 0.7366666666666667; loss: 2.2422547340393066\n",
      "Training epoch 8474 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8474 ; accuracy: 0.7366666666666667; loss: 2.2422540187835693\n",
      "Training epoch 8475 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8475 ; accuracy: 0.7366666666666667; loss: 2.242231845855713\n",
      "Training epoch 8476 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8476 ; accuracy: 0.7366666666666667; loss: 2.2422051429748535\n",
      "Training epoch 8477 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8477 ; accuracy: 0.7366666666666667; loss: 2.242199659347534\n",
      "Training epoch 8478 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8478 ; accuracy: 0.7366666666666667; loss: 2.2421979904174805\n",
      "Training epoch 8479 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8479 ; accuracy: 0.7366666666666667; loss: 2.242199659347534\n",
      "Training epoch 8480 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8480 ; accuracy: 0.7366666666666667; loss: 2.2422075271606445\n",
      "Training epoch 8481 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8481 ; accuracy: 0.7366666666666667; loss: 2.2422163486480713\n",
      "Training epoch 8482 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8482 ; accuracy: 0.7366666666666667; loss: 2.242225408554077\n",
      "Training epoch 8483 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8483 ; accuracy: 0.7366666666666667; loss: 2.2422361373901367\n",
      "Training epoch 8484 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8484 ; accuracy: 0.7366666666666667; loss: 2.2422497272491455\n",
      "Training epoch 8485 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8485 ; accuracy: 0.7366666666666667; loss: 2.242267608642578\n",
      "Training epoch 8486 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8486 ; accuracy: 0.7366666666666667; loss: 2.242283582687378\n",
      "Training epoch 8487 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8487 ; accuracy: 0.7366666666666667; loss: 2.2422802448272705\n",
      "Training epoch 8488 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8488 ; accuracy: 0.7366666666666667; loss: 2.242274045944214\n",
      "Training epoch 8489 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8489 ; accuracy: 0.7366666666666667; loss: 2.2422661781311035\n",
      "Training epoch 8490 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8490 ; accuracy: 0.7366666666666667; loss: 2.242253541946411\n",
      "Training epoch 8491 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8491 ; accuracy: 0.7366666666666667; loss: 2.242245674133301\n",
      "Training epoch 8492 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8492 ; accuracy: 0.7366666666666667; loss: 2.242250442504883\n",
      "Training epoch 8493 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8493 ; accuracy: 0.7366666666666667; loss: 2.242262840270996\n",
      "Training epoch 8494 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8494 ; accuracy: 0.7366666666666667; loss: 2.24227237701416\n",
      "Training epoch 8495 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8495 ; accuracy: 0.7366666666666667; loss: 2.2422854900360107\n",
      "Training epoch 8496 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8496 ; accuracy: 0.7366666666666667; loss: 2.2423079013824463\n",
      "Training epoch 8497 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8497 ; accuracy: 0.7366666666666667; loss: 2.242324113845825\n",
      "Training epoch 8498 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8498 ; accuracy: 0.7366666666666667; loss: 2.242331027984619\n",
      "Training epoch 8499 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8499 ; accuracy: 0.7366666666666667; loss: 2.2423505783081055\n",
      "Training epoch 8500 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8500 ; accuracy: 0.7366666666666667; loss: 2.2423744201660156\n",
      "Training epoch 8501 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8501 ; accuracy: 0.7366666666666667; loss: 2.242396593093872\n",
      "Training epoch 8502 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8502 ; accuracy: 0.7366666666666667; loss: 2.242419719696045\n",
      "Training epoch 8503 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8503 ; accuracy: 0.7366666666666667; loss: 2.242445945739746\n",
      "Training epoch 8504 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8504 ; accuracy: 0.7366666666666667; loss: 2.242473602294922\n",
      "Training epoch 8505 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8505 ; accuracy: 0.7366666666666667; loss: 2.2425031661987305\n",
      "Training epoch 8506 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8506 ; accuracy: 0.7366666666666667; loss: 2.2425377368927\n",
      "Training epoch 8507 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8507 ; accuracy: 0.7366666666666667; loss: 2.2425568103790283\n",
      "Training epoch 8508 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8508 ; accuracy: 0.7366666666666667; loss: 2.2425754070281982\n",
      "Training epoch 8509 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8509 ; accuracy: 0.7366666666666667; loss: 2.242600202560425\n",
      "Training epoch 8510 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8510 ; accuracy: 0.7366666666666667; loss: 2.2426223754882812\n",
      "Training epoch 8511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8511 ; accuracy: 0.7366666666666667; loss: 2.242642879486084\n",
      "Training epoch 8512 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8512 ; accuracy: 0.7366666666666667; loss: 2.242666482925415\n",
      "Training epoch 8513 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8513 ; accuracy: 0.7366666666666667; loss: 2.242694616317749\n",
      "Training epoch 8514 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8514 ; accuracy: 0.7366666666666667; loss: 2.2427213191986084\n",
      "Training epoch 8515 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8515 ; accuracy: 0.7366666666666667; loss: 2.242745876312256\n",
      "Training epoch 8516 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8516 ; accuracy: 0.7366666666666667; loss: 2.2427616119384766\n",
      "Training epoch 8517 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8517 ; accuracy: 0.7366666666666667; loss: 2.242774248123169\n",
      "Training epoch 8518 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8518 ; accuracy: 0.7366666666666667; loss: 2.2427704334259033\n",
      "Training epoch 8519 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8519 ; accuracy: 0.7366666666666667; loss: 2.242778778076172\n",
      "Training epoch 8520 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8520 ; accuracy: 0.7366666666666667; loss: 2.242788553237915\n",
      "Training epoch 8521 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8521 ; accuracy: 0.7366666666666667; loss: 2.2428014278411865\n",
      "Training epoch 8522 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8522 ; accuracy: 0.7366666666666667; loss: 2.242814302444458\n",
      "Training epoch 8523 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8523 ; accuracy: 0.7366666666666667; loss: 2.2428271770477295\n",
      "Training epoch 8524 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8524 ; accuracy: 0.7366666666666667; loss: 2.2428340911865234\n",
      "Training epoch 8525 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8525 ; accuracy: 0.7366666666666667; loss: 2.242835521697998\n",
      "Training epoch 8526 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8526 ; accuracy: 0.7366666666666667; loss: 2.242812156677246\n",
      "Training epoch 8527 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8527 ; accuracy: 0.7366666666666667; loss: 2.242769479751587\n",
      "Training epoch 8528 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8528 ; accuracy: 0.7366666666666667; loss: 2.2427306175231934\n",
      "Training epoch 8529 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8529 ; accuracy: 0.7366666666666667; loss: 2.242699384689331\n",
      "Training epoch 8530 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8530 ; accuracy: 0.7366666666666667; loss: 2.242673635482788\n",
      "Training epoch 8531 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8531 ; accuracy: 0.7366666666666667; loss: 2.242652416229248\n",
      "Training epoch 8532 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8532 ; accuracy: 0.7366666666666667; loss: 2.242633104324341\n",
      "Training epoch 8533 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8533 ; accuracy: 0.7366666666666667; loss: 2.2426140308380127\n",
      "Training epoch 8534 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8534 ; accuracy: 0.7366666666666667; loss: 2.2425594329833984\n",
      "Training epoch 8535 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8535 ; accuracy: 0.7366666666666667; loss: 2.2425224781036377\n",
      "Training epoch 8536 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8536 ; accuracy: 0.7366666666666667; loss: 2.2424962520599365\n",
      "Training epoch 8537 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8537 ; accuracy: 0.7366666666666667; loss: 2.2424778938293457\n",
      "Training epoch 8538 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8538 ; accuracy: 0.7366666666666667; loss: 2.2424652576446533\n",
      "Training epoch 8539 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8539 ; accuracy: 0.7366666666666667; loss: 2.2424421310424805\n",
      "Training epoch 8540 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8540 ; accuracy: 0.7366666666666667; loss: 2.242418050765991\n",
      "Training epoch 8541 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8541 ; accuracy: 0.7366666666666667; loss: 2.242408514022827\n",
      "Training epoch 8542 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8542 ; accuracy: 0.7366666666666667; loss: 2.242405652999878\n",
      "Training epoch 8543 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8543 ; accuracy: 0.7366666666666667; loss: 2.2424018383026123\n",
      "Training epoch 8544 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8544 ; accuracy: 0.7366666666666667; loss: 2.242403268814087\n",
      "Training epoch 8545 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8545 ; accuracy: 0.7366666666666667; loss: 2.2424023151397705\n",
      "Training epoch 8546 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8546 ; accuracy: 0.7366666666666667; loss: 2.242403030395508\n",
      "Training epoch 8547 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8547 ; accuracy: 0.7366666666666667; loss: 2.2423999309539795\n",
      "Training epoch 8548 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8548 ; accuracy: 0.7366666666666667; loss: 2.242389678955078\n",
      "Training epoch 8549 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8549 ; accuracy: 0.7366666666666667; loss: 2.242379665374756\n",
      "Training epoch 8550 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8550 ; accuracy: 0.7366666666666667; loss: 2.242370367050171\n",
      "Training epoch 8551 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8551 ; accuracy: 0.7366666666666667; loss: 2.2423620223999023\n",
      "Training epoch 8552 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8552 ; accuracy: 0.7366666666666667; loss: 2.2423532009124756\n",
      "Training epoch 8553 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8553 ; accuracy: 0.7366666666666667; loss: 2.2423431873321533\n",
      "Training epoch 8554 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8554 ; accuracy: 0.7366666666666667; loss: 2.2423338890075684\n",
      "Training epoch 8555 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8555 ; accuracy: 0.7366666666666667; loss: 2.2423207759857178\n",
      "Training epoch 8556 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8556 ; accuracy: 0.7366666666666667; loss: 2.242309093475342\n",
      "Training epoch 8557 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8557 ; accuracy: 0.7366666666666667; loss: 2.2422895431518555\n",
      "Training epoch 8558 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8558 ; accuracy: 0.7366666666666667; loss: 2.242269992828369\n",
      "Training epoch 8559 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8559 ; accuracy: 0.7366666666666667; loss: 2.242253303527832\n",
      "Training epoch 8560 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8560 ; accuracy: 0.7366666666666667; loss: 2.242234230041504\n",
      "Training epoch 8561 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8561 ; accuracy: 0.7366666666666667; loss: 2.2422149181365967\n",
      "Training epoch 8562 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8562 ; accuracy: 0.7366666666666667; loss: 2.242194652557373\n",
      "Training epoch 8563 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8563 ; accuracy: 0.7366666666666667; loss: 2.242173433303833\n",
      "Training epoch 8564 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8564 ; accuracy: 0.7366666666666667; loss: 2.242141008377075\n",
      "Training epoch 8565 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8565 ; accuracy: 0.7366666666666667; loss: 2.242088556289673\n",
      "Training epoch 8566 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8566 ; accuracy: 0.7366666666666667; loss: 2.242043972015381\n",
      "Training epoch 8567 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8567 ; accuracy: 0.7366666666666667; loss: 2.2420105934143066\n",
      "Training epoch 8568 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8568 ; accuracy: 0.7366666666666667; loss: 2.2419703006744385\n",
      "Training epoch 8569 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8569 ; accuracy: 0.7366666666666667; loss: 2.2419328689575195\n",
      "Training epoch 8570 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8570 ; accuracy: 0.7366666666666667; loss: 2.2418956756591797\n",
      "Training epoch 8571 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8571 ; accuracy: 0.7366666666666667; loss: 2.241861581802368\n",
      "Training epoch 8572 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8572 ; accuracy: 0.7366666666666667; loss: 2.241825819015503\n",
      "Training epoch 8573 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8573 ; accuracy: 0.7366666666666667; loss: 2.2417893409729004\n",
      "Training epoch 8574 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8574 ; accuracy: 0.7366666666666667; loss: 2.2417571544647217\n",
      "Training epoch 8575 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8575 ; accuracy: 0.7366666666666667; loss: 2.2417263984680176\n",
      "Training epoch 8576 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8576 ; accuracy: 0.7366666666666667; loss: 2.241710662841797\n",
      "Training epoch 8577 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8577 ; accuracy: 0.7366666666666667; loss: 2.2417032718658447\n",
      "Training epoch 8578 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8578 ; accuracy: 0.7366666666666667; loss: 2.241701126098633\n",
      "Training epoch 8579 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8579 ; accuracy: 0.7366666666666667; loss: 2.241706132888794\n",
      "Training epoch 8580 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8580 ; accuracy: 0.7366666666666667; loss: 2.2417075634002686\n",
      "Training epoch 8581 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8581 ; accuracy: 0.7366666666666667; loss: 2.2417168617248535\n",
      "Training epoch 8582 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8582 ; accuracy: 0.7366666666666667; loss: 2.241727828979492\n",
      "Training epoch 8583 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8583 ; accuracy: 0.7366666666666667; loss: 2.2417478561401367\n",
      "Training epoch 8584 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8584 ; accuracy: 0.7366666666666667; loss: 2.241774559020996\n",
      "Training epoch 8585 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8585 ; accuracy: 0.7366666666666667; loss: 2.2417972087860107\n",
      "Training epoch 8586 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8586 ; accuracy: 0.7366666666666667; loss: 2.241823196411133\n",
      "Training epoch 8587 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8587 ; accuracy: 0.7366666666666667; loss: 2.24186372756958\n",
      "Training epoch 8588 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8588 ; accuracy: 0.7366666666666667; loss: 2.241899013519287\n",
      "Training epoch 8589 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8589 ; accuracy: 0.7366666666666667; loss: 2.241929054260254\n",
      "Training epoch 8590 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8590 ; accuracy: 0.7366666666666667; loss: 2.2419426441192627\n",
      "Training epoch 8591 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8591 ; accuracy: 0.7366666666666667; loss: 2.2419614791870117\n",
      "Training epoch 8592 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8592 ; accuracy: 0.7366666666666667; loss: 2.241976022720337\n",
      "Training epoch 8593 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8593 ; accuracy: 0.7366666666666667; loss: 2.2419919967651367\n",
      "Training epoch 8594 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8594 ; accuracy: 0.7366666666666667; loss: 2.2420008182525635\n",
      "Training epoch 8595 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8595 ; accuracy: 0.7366666666666667; loss: 2.2419939041137695\n",
      "Training epoch 8596 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8596 ; accuracy: 0.7366666666666667; loss: 2.242004632949829\n",
      "Training epoch 8597 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8597 ; accuracy: 0.7366666666666667; loss: 2.2420032024383545\n",
      "Training epoch 8598 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8598 ; accuracy: 0.7366666666666667; loss: 2.2420079708099365\n",
      "Training epoch 8599 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8599 ; accuracy: 0.7366666666666667; loss: 2.242013454437256\n",
      "Training epoch 8600 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8600 ; accuracy: 0.7366666666666667; loss: 2.2420244216918945\n",
      "Training epoch 8601 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8601 ; accuracy: 0.7366666666666667; loss: 2.24204158782959\n",
      "Training epoch 8602 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8602 ; accuracy: 0.7366666666666667; loss: 2.242050886154175\n",
      "Training epoch 8603 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8603 ; accuracy: 0.7366666666666667; loss: 2.2420520782470703\n",
      "Training epoch 8604 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8604 ; accuracy: 0.7366666666666667; loss: 2.242050886154175\n",
      "Training epoch 8605 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8605 ; accuracy: 0.7366666666666667; loss: 2.2420430183410645\n",
      "Training epoch 8606 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8606 ; accuracy: 0.7366666666666667; loss: 2.242034435272217\n",
      "Training epoch 8607 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8607 ; accuracy: 0.7366666666666667; loss: 2.2420356273651123\n",
      "Training epoch 8608 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8608 ; accuracy: 0.7366666666666667; loss: 2.242046594619751\n",
      "Training epoch 8609 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8609 ; accuracy: 0.7366666666666667; loss: 2.24206280708313\n",
      "Training epoch 8610 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8610 ; accuracy: 0.7366666666666667; loss: 2.242081642150879\n",
      "Training epoch 8611 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8611 ; accuracy: 0.7366666666666667; loss: 2.2421023845672607\n",
      "Training epoch 8612 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8612 ; accuracy: 0.7366666666666667; loss: 2.2421207427978516\n",
      "Training epoch 8613 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8613 ; accuracy: 0.7366666666666667; loss: 2.2421364784240723\n",
      "Training epoch 8614 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8614 ; accuracy: 0.7366666666666667; loss: 2.2421586513519287\n",
      "Training epoch 8615 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8615 ; accuracy: 0.7366666666666667; loss: 2.242185354232788\n",
      "Training epoch 8616 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8616 ; accuracy: 0.7366666666666667; loss: 2.2422080039978027\n",
      "Training epoch 8617 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8617 ; accuracy: 0.7366666666666667; loss: 2.2422337532043457\n",
      "Training epoch 8618 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8618 ; accuracy: 0.7366666666666667; loss: 2.242250919342041\n",
      "Training epoch 8619 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8619 ; accuracy: 0.7366666666666667; loss: 2.2422702312469482\n",
      "Training epoch 8620 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8620 ; accuracy: 0.7366666666666667; loss: 2.242285966873169\n",
      "Training epoch 8621 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8621 ; accuracy: 0.7366666666666667; loss: 2.2422986030578613\n",
      "Training epoch 8622 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8622 ; accuracy: 0.7366666666666667; loss: 2.2423086166381836\n",
      "Training epoch 8623 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8623 ; accuracy: 0.7366666666666667; loss: 2.242314100265503\n",
      "Training epoch 8624 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8624 ; accuracy: 0.7366666666666667; loss: 2.242327928543091\n",
      "Training epoch 8625 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8625 ; accuracy: 0.7366666666666667; loss: 2.2423386573791504\n",
      "Training epoch 8626 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8626 ; accuracy: 0.7366666666666667; loss: 2.2423391342163086\n",
      "Training epoch 8627 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8627 ; accuracy: 0.7366666666666667; loss: 2.242331027984619\n",
      "Training epoch 8628 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8628 ; accuracy: 0.7366666666666667; loss: 2.2423858642578125\n",
      "Training epoch 8629 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8629 ; accuracy: 0.7366666666666667; loss: 2.2424283027648926\n",
      "Training epoch 8630 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8630 ; accuracy: 0.7366666666666667; loss: 2.2424745559692383\n",
      "Training epoch 8631 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8631 ; accuracy: 0.7366666666666667; loss: 2.242520809173584\n",
      "Training epoch 8632 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8632 ; accuracy: 0.7366666666666667; loss: 2.2425637245178223\n",
      "Training epoch 8633 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8633 ; accuracy: 0.7366666666666667; loss: 2.2426042556762695\n",
      "Training epoch 8634 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8634 ; accuracy: 0.7366666666666667; loss: 2.2426416873931885\n",
      "Training epoch 8635 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8635 ; accuracy: 0.7366666666666667; loss: 2.2426598072052\n",
      "Training epoch 8636 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8636 ; accuracy: 0.7366666666666667; loss: 2.242676019668579\n",
      "Training epoch 8637 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8637 ; accuracy: 0.7366666666666667; loss: 2.242690086364746\n",
      "Training epoch 8638 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8638 ; accuracy: 0.7366666666666667; loss: 2.2427027225494385\n",
      "Training epoch 8639 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8639 ; accuracy: 0.7366666666666667; loss: 2.242724895477295\n",
      "Training epoch 8640 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8640 ; accuracy: 0.7366666666666667; loss: 2.2427592277526855\n",
      "Training epoch 8641 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8641 ; accuracy: 0.7366666666666667; loss: 2.242788076400757\n",
      "Training epoch 8642 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8642 ; accuracy: 0.7366666666666667; loss: 2.2428135871887207\n",
      "Training epoch 8643 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8643 ; accuracy: 0.7366666666666667; loss: 2.242852210998535\n",
      "Training epoch 8644 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8644 ; accuracy: 0.7366666666666667; loss: 2.242882490158081\n",
      "Training epoch 8645 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8645 ; accuracy: 0.7366666666666667; loss: 2.2429141998291016\n",
      "Training epoch 8646 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8646 ; accuracy: 0.7366666666666667; loss: 2.242933750152588\n",
      "Training epoch 8647 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8647 ; accuracy: 0.7366666666666667; loss: 2.2429544925689697\n",
      "Training epoch 8648 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8648 ; accuracy: 0.7366666666666667; loss: 2.242985725402832\n",
      "Training epoch 8649 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8649 ; accuracy: 0.7366666666666667; loss: 2.243030309677124\n",
      "Training epoch 8650 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8650 ; accuracy: 0.7366666666666667; loss: 2.2430691719055176\n",
      "Training epoch 8651 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8651 ; accuracy: 0.7366666666666667; loss: 2.24310302734375\n",
      "Training epoch 8652 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8652 ; accuracy: 0.7366666666666667; loss: 2.2431273460388184\n",
      "Training epoch 8653 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8653 ; accuracy: 0.7366666666666667; loss: 2.243140697479248\n",
      "Training epoch 8654 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8654 ; accuracy: 0.7366666666666667; loss: 2.2431535720825195\n",
      "Training epoch 8655 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8655 ; accuracy: 0.7366666666666667; loss: 2.243161916732788\n",
      "Training epoch 8656 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8656 ; accuracy: 0.7366666666666667; loss: 2.2431700229644775\n",
      "Training epoch 8657 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8657 ; accuracy: 0.7366666666666667; loss: 2.243177890777588\n",
      "Training epoch 8658 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8658 ; accuracy: 0.7366666666666667; loss: 2.2431721687316895\n",
      "Training epoch 8659 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8659 ; accuracy: 0.7366666666666667; loss: 2.2431623935699463\n",
      "Training epoch 8660 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8660 ; accuracy: 0.7366666666666667; loss: 2.2431509494781494\n",
      "Training epoch 8661 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8661 ; accuracy: 0.7366666666666667; loss: 2.2431480884552\n",
      "Training epoch 8662 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8662 ; accuracy: 0.7366666666666667; loss: 2.2431480884552\n",
      "Training epoch 8663 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8663 ; accuracy: 0.7366666666666667; loss: 2.243149518966675\n",
      "Training epoch 8664 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8664 ; accuracy: 0.7366666666666667; loss: 2.2431530952453613\n",
      "Training epoch 8665 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8665 ; accuracy: 0.7366666666666667; loss: 2.2431626319885254\n",
      "Training epoch 8666 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8666 ; accuracy: 0.7366666666666667; loss: 2.2431724071502686\n",
      "Training epoch 8667 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8667 ; accuracy: 0.7366666666666667; loss: 2.243176221847534\n",
      "Training epoch 8668 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8668 ; accuracy: 0.7366666666666667; loss: 2.2431817054748535\n",
      "Training epoch 8669 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8669 ; accuracy: 0.7366666666666667; loss: 2.243180751800537\n",
      "Training epoch 8670 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8670 ; accuracy: 0.7366666666666667; loss: 2.2431790828704834\n",
      "Training epoch 8671 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8671 ; accuracy: 0.7366666666666667; loss: 2.243182897567749\n",
      "Training epoch 8672 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8672 ; accuracy: 0.7366666666666667; loss: 2.243183135986328\n",
      "Training epoch 8673 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8673 ; accuracy: 0.7366666666666667; loss: 2.2431883811950684\n",
      "Training epoch 8674 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8674 ; accuracy: 0.7366666666666667; loss: 2.243199110031128\n",
      "Training epoch 8675 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8675 ; accuracy: 0.7366666666666667; loss: 2.2432010173797607\n",
      "Training epoch 8676 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8676 ; accuracy: 0.7366666666666667; loss: 2.2432072162628174\n",
      "Training epoch 8677 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8677 ; accuracy: 0.7366666666666667; loss: 2.2432312965393066\n",
      "Training epoch 8678 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8678 ; accuracy: 0.7366666666666667; loss: 2.2432401180267334\n",
      "Training epoch 8679 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8679 ; accuracy: 0.7366666666666667; loss: 2.243238687515259\n",
      "Training epoch 8680 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8680 ; accuracy: 0.7366666666666667; loss: 2.243246078491211\n",
      "Training epoch 8681 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8681 ; accuracy: 0.7366666666666667; loss: 2.2432591915130615\n",
      "Training epoch 8682 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8682 ; accuracy: 0.7366666666666667; loss: 2.243269443511963\n",
      "Training epoch 8683 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8683 ; accuracy: 0.7366666666666667; loss: 2.243272304534912\n",
      "Training epoch 8684 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8684 ; accuracy: 0.7366666666666667; loss: 2.2432618141174316\n",
      "Training epoch 8685 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8685 ; accuracy: 0.7366666666666667; loss: 2.2432591915130615\n",
      "Training epoch 8686 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8686 ; accuracy: 0.7366666666666667; loss: 2.243255376815796\n",
      "Training epoch 8687 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8687 ; accuracy: 0.7366666666666667; loss: 2.24324631690979\n",
      "Training epoch 8688 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8688 ; accuracy: 0.7366666666666667; loss: 2.243236780166626\n",
      "Training epoch 8689 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8689 ; accuracy: 0.7366666666666667; loss: 2.243227243423462\n",
      "Training epoch 8690 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8690 ; accuracy: 0.7366666666666667; loss: 2.2432215213775635\n",
      "Training epoch 8691 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8691 ; accuracy: 0.7366666666666667; loss: 2.2432174682617188\n",
      "Training epoch 8692 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8692 ; accuracy: 0.7366666666666667; loss: 2.243218183517456\n",
      "Training epoch 8693 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8693 ; accuracy: 0.7366666666666667; loss: 2.243220329284668\n",
      "Training epoch 8694 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8694 ; accuracy: 0.7366666666666667; loss: 2.243227243423462\n",
      "Training epoch 8695 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8695 ; accuracy: 0.7366666666666667; loss: 2.2432217597961426\n",
      "Training epoch 8696 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8696 ; accuracy: 0.7366666666666667; loss: 2.2432119846343994\n",
      "Training epoch 8697 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8697 ; accuracy: 0.7366666666666667; loss: 2.243199110031128\n",
      "Training epoch 8698 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8698 ; accuracy: 0.7366666666666667; loss: 2.243180274963379\n",
      "Training epoch 8699 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8699 ; accuracy: 0.7366666666666667; loss: 2.243173122406006\n",
      "Training epoch 8700 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8700 ; accuracy: 0.7366666666666667; loss: 2.2431626319885254\n",
      "Training epoch 8701 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8701 ; accuracy: 0.7366666666666667; loss: 2.243116855621338\n",
      "Training epoch 8702 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8702 ; accuracy: 0.7366666666666667; loss: 2.243088722229004\n",
      "Training epoch 8703 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8703 ; accuracy: 0.7366666666666667; loss: 2.2430567741394043\n",
      "Training epoch 8704 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8704 ; accuracy: 0.7366666666666667; loss: 2.2430014610290527\n",
      "Training epoch 8705 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8705 ; accuracy: 0.7366666666666667; loss: 2.2429280281066895\n",
      "Training epoch 8706 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8706 ; accuracy: 0.7366666666666667; loss: 2.2428603172302246\n",
      "Training epoch 8707 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8707 ; accuracy: 0.7366666666666667; loss: 2.2428083419799805\n",
      "Training epoch 8708 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8708 ; accuracy: 0.7366666666666667; loss: 2.2427430152893066\n",
      "Training epoch 8709 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8709 ; accuracy: 0.7366666666666667; loss: 2.2426819801330566\n",
      "Training epoch 8710 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8710 ; accuracy: 0.7366666666666667; loss: 2.2426328659057617\n",
      "Training epoch 8711 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8711 ; accuracy: 0.7366666666666667; loss: 2.242589235305786\n",
      "Training epoch 8712 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8712 ; accuracy: 0.7366666666666667; loss: 2.242555856704712\n",
      "Training epoch 8713 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8713 ; accuracy: 0.7366666666666667; loss: 2.2425544261932373\n",
      "Training epoch 8714 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8714 ; accuracy: 0.7366666666666667; loss: 2.242546319961548\n",
      "Training epoch 8715 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8715 ; accuracy: 0.7366666666666667; loss: 2.242537021636963\n",
      "Training epoch 8716 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8716 ; accuracy: 0.7366666666666667; loss: 2.2425241470336914\n",
      "Training epoch 8717 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8717 ; accuracy: 0.7366666666666667; loss: 2.24251389503479\n",
      "Training epoch 8718 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8718 ; accuracy: 0.7366666666666667; loss: 2.2425146102905273\n",
      "Training epoch 8719 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8719 ; accuracy: 0.7366666666666667; loss: 2.2425241470336914\n",
      "Training epoch 8720 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8720 ; accuracy: 0.7366666666666667; loss: 2.2425336837768555\n",
      "Training epoch 8721 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8721 ; accuracy: 0.7366666666666667; loss: 2.242547035217285\n",
      "Training epoch 8722 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8722 ; accuracy: 0.7366666666666667; loss: 2.2425537109375\n",
      "Training epoch 8723 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8723 ; accuracy: 0.7366666666666667; loss: 2.2425668239593506\n",
      "Training epoch 8724 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8724 ; accuracy: 0.7366666666666667; loss: 2.242587089538574\n",
      "Training epoch 8725 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8725 ; accuracy: 0.7366666666666667; loss: 2.242605447769165\n",
      "Training epoch 8726 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8726 ; accuracy: 0.7366666666666667; loss: 2.242626667022705\n",
      "Training epoch 8727 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8727 ; accuracy: 0.7366666666666667; loss: 2.242647886276245\n",
      "Training epoch 8728 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8728 ; accuracy: 0.7366666666666667; loss: 2.242656946182251\n",
      "Training epoch 8729 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8729 ; accuracy: 0.7366666666666667; loss: 2.242676258087158\n",
      "Training epoch 8730 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8730 ; accuracy: 0.7366666666666667; loss: 2.24267840385437\n",
      "Training epoch 8731 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8731 ; accuracy: 0.7366666666666667; loss: 2.2426834106445312\n",
      "Training epoch 8732 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8732 ; accuracy: 0.7366666666666667; loss: 2.2426912784576416\n",
      "Training epoch 8733 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8733 ; accuracy: 0.7366666666666667; loss: 2.242704153060913\n",
      "Training epoch 8734 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8734 ; accuracy: 0.7366666666666667; loss: 2.2427163124084473\n",
      "Training epoch 8735 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 8735 ; accuracy: 0.7366666666666667; loss: 2.2427263259887695\n",
      "Training epoch 8736 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8736 ; accuracy: 0.7366666666666667; loss: 2.2427518367767334\n",
      "Training epoch 8737 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8737 ; accuracy: 0.7366666666666667; loss: 2.242783308029175\n",
      "Training epoch 8738 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8738 ; accuracy: 0.7366666666666667; loss: 2.2428371906280518\n",
      "Training epoch 8739 ; accuracy: 0.9; loss: 0.19475261867046356\n",
      "Validation epoch 8739 ; accuracy: 0.74; loss: 2.2288246154785156\n",
      "Training epoch 8740 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8740 ; accuracy: 0.74; loss: 2.2127468585968018\n",
      "Training epoch 8741 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 8741 ; accuracy: 0.74; loss: 2.1970629692077637\n",
      "Training epoch 8742 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 8742 ; accuracy: 0.7433333333333333; loss: 2.185025930404663\n",
      "Training epoch 8743 ; accuracy: 0.9; loss: 0.19459474086761475\n",
      "Validation epoch 8743 ; accuracy: 0.7433333333333333; loss: 2.1763672828674316\n",
      "Training epoch 8744 ; accuracy: 0.9; loss: 0.1946006566286087\n",
      "Validation epoch 8744 ; accuracy: 0.74; loss: 2.167208433151245\n",
      "Training epoch 8745 ; accuracy: 0.9; loss: 0.19461911916732788\n",
      "Validation epoch 8745 ; accuracy: 0.7333333333333333; loss: 2.1634390354156494\n",
      "Training epoch 8746 ; accuracy: 0.9; loss: 0.19461509585380554\n",
      "Validation epoch 8746 ; accuracy: 0.7333333333333333; loss: 2.1537976264953613\n",
      "Training epoch 8747 ; accuracy: 0.9; loss: 0.19460614025592804\n",
      "Validation epoch 8747 ; accuracy: 0.7333333333333333; loss: 2.147742509841919\n",
      "Training epoch 8748 ; accuracy: 0.9; loss: 0.1945980042219162\n",
      "Validation epoch 8748 ; accuracy: 0.7366666666666667; loss: 2.1424734592437744\n",
      "Training epoch 8749 ; accuracy: 0.9; loss: 0.19459345936775208\n",
      "Validation epoch 8749 ; accuracy: 0.7433333333333333; loss: 2.136841297149658\n",
      "Training epoch 8750 ; accuracy: 0.9; loss: 0.19459207355976105\n",
      "Validation epoch 8750 ; accuracy: 0.7433333333333333; loss: 2.1323137283325195\n",
      "Training epoch 8751 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 8751 ; accuracy: 0.74; loss: 2.1286683082580566\n",
      "Training epoch 8752 ; accuracy: 0.9; loss: 0.19459137320518494\n",
      "Validation epoch 8752 ; accuracy: 0.74; loss: 2.125049114227295\n",
      "Training epoch 8753 ; accuracy: 0.9; loss: 0.19459129869937897\n",
      "Validation epoch 8753 ; accuracy: 0.7366666666666667; loss: 2.1221208572387695\n",
      "Training epoch 8754 ; accuracy: 0.9; loss: 0.1945914626121521\n",
      "Validation epoch 8754 ; accuracy: 0.7333333333333333; loss: 2.1194889545440674\n",
      "Training epoch 8755 ; accuracy: 0.9; loss: 0.19459153711795807\n",
      "Validation epoch 8755 ; accuracy: 0.7333333333333333; loss: 2.116550922393799\n",
      "Training epoch 8756 ; accuracy: 0.9; loss: 0.19459155201911926\n",
      "Validation epoch 8756 ; accuracy: 0.73; loss: 2.113024950027466\n",
      "Training epoch 8757 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 8757 ; accuracy: 0.73; loss: 2.1091935634613037\n",
      "Training epoch 8758 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 8758 ; accuracy: 0.7266666666666667; loss: 2.1044816970825195\n",
      "Training epoch 8759 ; accuracy: 0.9; loss: 0.19459159672260284\n",
      "Validation epoch 8759 ; accuracy: 0.73; loss: 2.0991454124450684\n",
      "Training epoch 8760 ; accuracy: 0.9; loss: 0.19459185004234314\n",
      "Validation epoch 8760 ; accuracy: 0.73; loss: 2.09205961227417\n",
      "Training epoch 8761 ; accuracy: 0.9; loss: 0.19459174573421478\n",
      "Validation epoch 8761 ; accuracy: 0.73; loss: 2.0843753814697266\n",
      "Training epoch 8762 ; accuracy: 0.9; loss: 0.19459156692028046\n",
      "Validation epoch 8762 ; accuracy: 0.73; loss: 2.0773255825042725\n",
      "Training epoch 8763 ; accuracy: 0.9; loss: 0.1945917308330536\n",
      "Validation epoch 8763 ; accuracy: 0.7233333333333334; loss: 2.071256160736084\n",
      "Training epoch 8764 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 8764 ; accuracy: 0.7233333333333334; loss: 2.0656776428222656\n",
      "Training epoch 8765 ; accuracy: 0.9; loss: 0.19459150731563568\n",
      "Validation epoch 8765 ; accuracy: 0.7266666666666667; loss: 2.0604982376098633\n",
      "Training epoch 8766 ; accuracy: 0.9; loss: 0.1945914328098297\n",
      "Validation epoch 8766 ; accuracy: 0.7266666666666667; loss: 2.056183099746704\n",
      "Training epoch 8767 ; accuracy: 0.9; loss: 0.1945914477109909\n",
      "Validation epoch 8767 ; accuracy: 0.7266666666666667; loss: 2.0527102947235107\n",
      "Training epoch 8768 ; accuracy: 0.9; loss: 0.1945916712284088\n",
      "Validation epoch 8768 ; accuracy: 0.7233333333333334; loss: 2.0504143238067627\n",
      "Training epoch 8769 ; accuracy: 0.9; loss: 0.19459128379821777\n",
      "Validation epoch 8769 ; accuracy: 0.7233333333333334; loss: 2.048631191253662\n",
      "Training epoch 8770 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 8770 ; accuracy: 0.7233333333333334; loss: 2.0472519397735596\n",
      "Training epoch 8771 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 8771 ; accuracy: 0.7233333333333334; loss: 2.046198844909668\n",
      "Training epoch 8772 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 8772 ; accuracy: 0.7266666666666667; loss: 2.0455360412597656\n",
      "Training epoch 8773 ; accuracy: 0.9; loss: 0.19459114968776703\n",
      "Validation epoch 8773 ; accuracy: 0.73; loss: 2.0451745986938477\n",
      "Training epoch 8774 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 8774 ; accuracy: 0.73; loss: 2.045102834701538\n",
      "Training epoch 8775 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 8775 ; accuracy: 0.73; loss: 2.0452587604522705\n",
      "Training epoch 8776 ; accuracy: 0.9; loss: 0.19459132850170135\n",
      "Validation epoch 8776 ; accuracy: 0.73; loss: 2.0457053184509277\n",
      "Training epoch 8777 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 8777 ; accuracy: 0.73; loss: 2.0461270809173584\n",
      "Training epoch 8778 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 8778 ; accuracy: 0.73; loss: 2.0466911792755127\n",
      "Training epoch 8779 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8779 ; accuracy: 0.73; loss: 2.0473742485046387\n",
      "Training epoch 8780 ; accuracy: 0.9; loss: 0.19459113478660583\n",
      "Validation epoch 8780 ; accuracy: 0.73; loss: 2.048208713531494\n",
      "Training epoch 8781 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 8781 ; accuracy: 0.73; loss: 2.049166202545166\n",
      "Training epoch 8782 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8782 ; accuracy: 0.7333333333333333; loss: 2.0501906871795654\n",
      "Training epoch 8783 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8783 ; accuracy: 0.7333333333333333; loss: 2.051274299621582\n",
      "Training epoch 8784 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8784 ; accuracy: 0.7366666666666667; loss: 2.052285671234131\n",
      "Training epoch 8785 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8785 ; accuracy: 0.7366666666666667; loss: 2.053269863128662\n",
      "Training epoch 8786 ; accuracy: 0.9; loss: 0.1945911943912506\n",
      "Validation epoch 8786 ; accuracy: 0.7366666666666667; loss: 2.0544304847717285\n",
      "Training epoch 8787 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8787 ; accuracy: 0.7366666666666667; loss: 2.0556154251098633\n",
      "Training epoch 8788 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8788 ; accuracy: 0.7366666666666667; loss: 2.05684232711792\n",
      "Training epoch 8789 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8789 ; accuracy: 0.7366666666666667; loss: 2.0580873489379883\n",
      "Training epoch 8790 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 8790 ; accuracy: 0.74; loss: 2.059321165084839\n",
      "Training epoch 8791 ; accuracy: 0.9; loss: 0.1945912390947342\n",
      "Validation epoch 8791 ; accuracy: 0.74; loss: 2.0607666969299316\n",
      "Training epoch 8792 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8792 ; accuracy: 0.74; loss: 2.0621936321258545\n",
      "Training epoch 8793 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 8793 ; accuracy: 0.74; loss: 2.063570976257324\n",
      "Training epoch 8794 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8794 ; accuracy: 0.74; loss: 2.0649099349975586\n",
      "Training epoch 8795 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 8795 ; accuracy: 0.74; loss: 2.0662107467651367\n",
      "Training epoch 8796 ; accuracy: 0.9; loss: 0.19459111988544464\n",
      "Validation epoch 8796 ; accuracy: 0.74; loss: 2.0674965381622314\n",
      "Training epoch 8797 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8797 ; accuracy: 0.7433333333333333; loss: 2.06872296333313\n",
      "Training epoch 8798 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8798 ; accuracy: 0.7433333333333333; loss: 2.0698113441467285\n",
      "Training epoch 8799 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8799 ; accuracy: 0.7433333333333333; loss: 2.070873498916626\n",
      "Training epoch 8800 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 8800 ; accuracy: 0.7433333333333333; loss: 2.0719611644744873\n",
      "Training epoch 8801 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8801 ; accuracy: 0.7433333333333333; loss: 2.073096513748169\n",
      "Training epoch 8802 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8802 ; accuracy: 0.7433333333333333; loss: 2.0741336345672607\n",
      "Training epoch 8803 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8803 ; accuracy: 0.7433333333333333; loss: 2.075134754180908\n",
      "Training epoch 8804 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8804 ; accuracy: 0.7433333333333333; loss: 2.0761489868164062\n",
      "Training epoch 8805 ; accuracy: 0.9; loss: 0.19459116458892822\n",
      "Validation epoch 8805 ; accuracy: 0.7433333333333333; loss: 2.0771753787994385\n",
      "Training epoch 8806 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8806 ; accuracy: 0.7433333333333333; loss: 2.0782241821289062\n",
      "Training epoch 8807 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8807 ; accuracy: 0.7433333333333333; loss: 2.0792489051818848\n",
      "Training epoch 8808 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8808 ; accuracy: 0.7433333333333333; loss: 2.08026123046875\n",
      "Training epoch 8809 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8809 ; accuracy: 0.7433333333333333; loss: 2.081293821334839\n",
      "Training epoch 8810 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8810 ; accuracy: 0.7433333333333333; loss: 2.0822858810424805\n",
      "Training epoch 8811 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8811 ; accuracy: 0.7433333333333333; loss: 2.083216667175293\n",
      "Training epoch 8812 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 8812 ; accuracy: 0.74; loss: 2.0841403007507324\n",
      "Training epoch 8813 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8813 ; accuracy: 0.74; loss: 2.0850462913513184\n",
      "Training epoch 8814 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8814 ; accuracy: 0.74; loss: 2.0859062671661377\n",
      "Training epoch 8815 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8815 ; accuracy: 0.74; loss: 2.0867421627044678\n",
      "Training epoch 8816 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8816 ; accuracy: 0.74; loss: 2.0875473022460938\n",
      "Training epoch 8817 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8817 ; accuracy: 0.74; loss: 2.088332414627075\n",
      "Training epoch 8818 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8818 ; accuracy: 0.74; loss: 2.0891060829162598\n",
      "Training epoch 8819 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8819 ; accuracy: 0.74; loss: 2.089890956878662\n",
      "Training epoch 8820 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8820 ; accuracy: 0.74; loss: 2.0907163619995117\n",
      "Training epoch 8821 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8821 ; accuracy: 0.74; loss: 2.0915074348449707\n",
      "Training epoch 8822 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8822 ; accuracy: 0.74; loss: 2.0922608375549316\n",
      "Training epoch 8823 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8823 ; accuracy: 0.74; loss: 2.093021869659424\n",
      "Training epoch 8824 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8824 ; accuracy: 0.74; loss: 2.093782424926758\n",
      "Training epoch 8825 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8825 ; accuracy: 0.74; loss: 2.0944902896881104\n",
      "Training epoch 8826 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8826 ; accuracy: 0.74; loss: 2.0951645374298096\n",
      "Training epoch 8827 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8827 ; accuracy: 0.74; loss: 2.0958240032196045\n",
      "Training epoch 8828 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8828 ; accuracy: 0.74; loss: 2.0964736938476562\n",
      "Training epoch 8829 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8829 ; accuracy: 0.74; loss: 2.0970988273620605\n",
      "Training epoch 8830 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8830 ; accuracy: 0.74; loss: 2.097726821899414\n",
      "Training epoch 8831 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8831 ; accuracy: 0.74; loss: 2.098398447036743\n",
      "Training epoch 8832 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8832 ; accuracy: 0.74; loss: 2.0990679264068604\n",
      "Training epoch 8833 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8833 ; accuracy: 0.74; loss: 2.09973406791687\n",
      "Training epoch 8834 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8834 ; accuracy: 0.74; loss: 2.100388288497925\n",
      "Training epoch 8835 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8835 ; accuracy: 0.74; loss: 2.1010093688964844\n",
      "Training epoch 8836 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8836 ; accuracy: 0.74; loss: 2.1016056537628174\n",
      "Training epoch 8837 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8837 ; accuracy: 0.74; loss: 2.1022512912750244\n",
      "Training epoch 8838 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8838 ; accuracy: 0.74; loss: 2.102860450744629\n",
      "Training epoch 8839 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8839 ; accuracy: 0.74; loss: 2.103449821472168\n",
      "Training epoch 8840 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8840 ; accuracy: 0.74; loss: 2.1040048599243164\n",
      "Training epoch 8841 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8841 ; accuracy: 0.74; loss: 2.1045331954956055\n",
      "Training epoch 8842 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8842 ; accuracy: 0.74; loss: 2.1050782203674316\n",
      "Training epoch 8843 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8843 ; accuracy: 0.74; loss: 2.105609178543091\n",
      "Training epoch 8844 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8844 ; accuracy: 0.74; loss: 2.10612154006958\n",
      "Training epoch 8845 ; accuracy: 0.9; loss: 0.19459109008312225\n",
      "Validation epoch 8845 ; accuracy: 0.74; loss: 2.106649160385132\n",
      "Training epoch 8846 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8846 ; accuracy: 0.74; loss: 2.1071536540985107\n",
      "Training epoch 8847 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8847 ; accuracy: 0.74; loss: 2.107661485671997\n",
      "Training epoch 8848 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8848 ; accuracy: 0.74; loss: 2.1081340312957764\n",
      "Training epoch 8849 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8849 ; accuracy: 0.74; loss: 2.108588457107544\n",
      "Training epoch 8850 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8850 ; accuracy: 0.74; loss: 2.1090247631073\n",
      "Training epoch 8851 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8851 ; accuracy: 0.74; loss: 2.109527587890625\n",
      "Training epoch 8852 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8852 ; accuracy: 0.74; loss: 2.11002516746521\n",
      "Training epoch 8853 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8853 ; accuracy: 0.74; loss: 2.1105148792266846\n",
      "Training epoch 8854 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8854 ; accuracy: 0.74; loss: 2.1109888553619385\n",
      "Training epoch 8855 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8855 ; accuracy: 0.74; loss: 2.111475944519043\n",
      "Training epoch 8856 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8856 ; accuracy: 0.74; loss: 2.1119587421417236\n",
      "Training epoch 8857 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8857 ; accuracy: 0.74; loss: 2.1124351024627686\n",
      "Training epoch 8858 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8858 ; accuracy: 0.74; loss: 2.1129190921783447\n",
      "Training epoch 8859 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8859 ; accuracy: 0.74; loss: 2.1134276390075684\n",
      "Training epoch 8860 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8860 ; accuracy: 0.74; loss: 2.1139681339263916\n",
      "Training epoch 8861 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8861 ; accuracy: 0.74; loss: 2.1144814491271973\n",
      "Training epoch 8862 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8862 ; accuracy: 0.74; loss: 2.1149959564208984\n",
      "Training epoch 8863 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8863 ; accuracy: 0.74; loss: 2.1155624389648438\n",
      "Training epoch 8864 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8864 ; accuracy: 0.74; loss: 2.1161818504333496\n",
      "Training epoch 8865 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8865 ; accuracy: 0.74; loss: 2.116762399673462\n",
      "Training epoch 8866 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8866 ; accuracy: 0.74; loss: 2.117314338684082\n",
      "Training epoch 8867 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8867 ; accuracy: 0.74; loss: 2.1178951263427734\n",
      "Training epoch 8868 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8868 ; accuracy: 0.74; loss: 2.1184539794921875\n",
      "Training epoch 8869 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8869 ; accuracy: 0.74; loss: 2.118999719619751\n",
      "Training epoch 8870 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8870 ; accuracy: 0.74; loss: 2.1195144653320312\n",
      "Training epoch 8871 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8871 ; accuracy: 0.74; loss: 2.119997262954712\n",
      "Training epoch 8872 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8872 ; accuracy: 0.74; loss: 2.1204707622528076\n",
      "Training epoch 8873 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8873 ; accuracy: 0.74; loss: 2.1209099292755127\n",
      "Training epoch 8874 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8874 ; accuracy: 0.74; loss: 2.1213338375091553\n",
      "Training epoch 8875 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8875 ; accuracy: 0.74; loss: 2.121753454208374\n",
      "Training epoch 8876 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8876 ; accuracy: 0.74; loss: 2.1221764087677\n",
      "Training epoch 8877 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8877 ; accuracy: 0.74; loss: 2.1225781440734863\n",
      "Training epoch 8878 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8878 ; accuracy: 0.74; loss: 2.122978925704956\n",
      "Training epoch 8879 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8879 ; accuracy: 0.74; loss: 2.1233701705932617\n",
      "Training epoch 8880 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8880 ; accuracy: 0.74; loss: 2.1238064765930176\n",
      "Training epoch 8881 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8881 ; accuracy: 0.74; loss: 2.1242361068725586\n",
      "Training epoch 8882 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8882 ; accuracy: 0.74; loss: 2.124663829803467\n",
      "Training epoch 8883 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8883 ; accuracy: 0.74; loss: 2.125117063522339\n",
      "Training epoch 8884 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8884 ; accuracy: 0.74; loss: 2.1255686283111572\n",
      "Training epoch 8885 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8885 ; accuracy: 0.74; loss: 2.125981330871582\n",
      "Training epoch 8886 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8886 ; accuracy: 0.74; loss: 2.1263628005981445\n",
      "Training epoch 8887 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8887 ; accuracy: 0.74; loss: 2.1267426013946533\n",
      "Training epoch 8888 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8888 ; accuracy: 0.74; loss: 2.127112865447998\n",
      "Training epoch 8889 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8889 ; accuracy: 0.74; loss: 2.1274807453155518\n",
      "Training epoch 8890 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8890 ; accuracy: 0.74; loss: 2.127854824066162\n",
      "Training epoch 8891 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8891 ; accuracy: 0.74; loss: 2.1282145977020264\n",
      "Training epoch 8892 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8892 ; accuracy: 0.74; loss: 2.128577709197998\n",
      "Training epoch 8893 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8893 ; accuracy: 0.74; loss: 2.1289594173431396\n",
      "Training epoch 8894 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8894 ; accuracy: 0.74; loss: 2.129354953765869\n",
      "Training epoch 8895 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8895 ; accuracy: 0.74; loss: 2.1297781467437744\n",
      "Training epoch 8896 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8896 ; accuracy: 0.74; loss: 2.1301891803741455\n",
      "Training epoch 8897 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8897 ; accuracy: 0.74; loss: 2.130603313446045\n",
      "Training epoch 8898 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8898 ; accuracy: 0.74; loss: 2.131011486053467\n",
      "Training epoch 8899 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8899 ; accuracy: 0.74; loss: 2.131413221359253\n",
      "Training epoch 8900 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8900 ; accuracy: 0.74; loss: 2.1318166255950928\n",
      "Training epoch 8901 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8901 ; accuracy: 0.74; loss: 2.1322505474090576\n",
      "Training epoch 8902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8902 ; accuracy: 0.74; loss: 2.1326732635498047\n",
      "Training epoch 8903 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8903 ; accuracy: 0.74; loss: 2.133061408996582\n",
      "Training epoch 8904 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8904 ; accuracy: 0.74; loss: 2.133443832397461\n",
      "Training epoch 8905 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8905 ; accuracy: 0.74; loss: 2.1338589191436768\n",
      "Training epoch 8906 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8906 ; accuracy: 0.74; loss: 2.134265661239624\n",
      "Training epoch 8907 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8907 ; accuracy: 0.74; loss: 2.134711742401123\n",
      "Training epoch 8908 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8908 ; accuracy: 0.74; loss: 2.135176181793213\n",
      "Training epoch 8909 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8909 ; accuracy: 0.74; loss: 2.135640859603882\n",
      "Training epoch 8910 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8910 ; accuracy: 0.74; loss: 2.136080265045166\n",
      "Training epoch 8911 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8911 ; accuracy: 0.74; loss: 2.1365113258361816\n",
      "Training epoch 8912 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8912 ; accuracy: 0.74; loss: 2.136965036392212\n",
      "Training epoch 8913 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8913 ; accuracy: 0.74; loss: 2.137425184249878\n",
      "Training epoch 8914 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8914 ; accuracy: 0.74; loss: 2.1378657817840576\n",
      "Training epoch 8915 ; accuracy: 0.9; loss: 0.19459107518196106\n",
      "Validation epoch 8915 ; accuracy: 0.74; loss: 2.1382954120635986\n",
      "Training epoch 8916 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8916 ; accuracy: 0.74; loss: 2.138718843460083\n",
      "Training epoch 8917 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8917 ; accuracy: 0.74; loss: 2.139151096343994\n",
      "Training epoch 8918 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8918 ; accuracy: 0.74; loss: 2.1395604610443115\n",
      "Training epoch 8919 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8919 ; accuracy: 0.74; loss: 2.1399550437927246\n",
      "Training epoch 8920 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8920 ; accuracy: 0.74; loss: 2.1403427124023438\n",
      "Training epoch 8921 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8921 ; accuracy: 0.74; loss: 2.1407551765441895\n",
      "Training epoch 8922 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8922 ; accuracy: 0.74; loss: 2.141160726547241\n",
      "Training epoch 8923 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8923 ; accuracy: 0.74; loss: 2.141545295715332\n",
      "Training epoch 8924 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8924 ; accuracy: 0.74; loss: 2.141911506652832\n",
      "Training epoch 8925 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8925 ; accuracy: 0.74; loss: 2.142252206802368\n",
      "Training epoch 8926 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8926 ; accuracy: 0.74; loss: 2.1425952911376953\n",
      "Training epoch 8927 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8927 ; accuracy: 0.74; loss: 2.1429104804992676\n",
      "Training epoch 8928 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8928 ; accuracy: 0.74; loss: 2.143212080001831\n",
      "Training epoch 8929 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8929 ; accuracy: 0.74; loss: 2.143491744995117\n",
      "Training epoch 8930 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8930 ; accuracy: 0.74; loss: 2.143771171569824\n",
      "Training epoch 8931 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8931 ; accuracy: 0.74; loss: 2.1440324783325195\n",
      "Training epoch 8932 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8932 ; accuracy: 0.74; loss: 2.144310712814331\n",
      "Training epoch 8933 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8933 ; accuracy: 0.74; loss: 2.144578218460083\n",
      "Training epoch 8934 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8934 ; accuracy: 0.74; loss: 2.14485239982605\n",
      "Training epoch 8935 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8935 ; accuracy: 0.74; loss: 2.1451308727264404\n",
      "Training epoch 8936 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8936 ; accuracy: 0.74; loss: 2.1454193592071533\n",
      "Training epoch 8937 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8937 ; accuracy: 0.74; loss: 2.145700216293335\n",
      "Training epoch 8938 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8938 ; accuracy: 0.74; loss: 2.1459691524505615\n",
      "Training epoch 8939 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8939 ; accuracy: 0.74; loss: 2.1462461948394775\n",
      "Training epoch 8940 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8940 ; accuracy: 0.74; loss: 2.146529197692871\n",
      "Training epoch 8941 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8941 ; accuracy: 0.74; loss: 2.146804094314575\n",
      "Training epoch 8942 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8942 ; accuracy: 0.74; loss: 2.1470441818237305\n",
      "Training epoch 8943 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8943 ; accuracy: 0.74; loss: 2.147279977798462\n",
      "Training epoch 8944 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8944 ; accuracy: 0.74; loss: 2.1475348472595215\n",
      "Training epoch 8945 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8945 ; accuracy: 0.74; loss: 2.147825002670288\n",
      "Training epoch 8946 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8946 ; accuracy: 0.74; loss: 2.148088216781616\n",
      "Training epoch 8947 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8947 ; accuracy: 0.74; loss: 2.1483521461486816\n",
      "Training epoch 8948 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8948 ; accuracy: 0.74; loss: 2.148630380630493\n",
      "Training epoch 8949 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8949 ; accuracy: 0.74; loss: 2.148893117904663\n",
      "Training epoch 8950 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8950 ; accuracy: 0.74; loss: 2.1491658687591553\n",
      "Training epoch 8951 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8951 ; accuracy: 0.74; loss: 2.149466037750244\n",
      "Training epoch 8952 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8952 ; accuracy: 0.74; loss: 2.14974308013916\n",
      "Training epoch 8953 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8953 ; accuracy: 0.74; loss: 2.150017499923706\n",
      "Training epoch 8954 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8954 ; accuracy: 0.74; loss: 2.1502797603607178\n",
      "Training epoch 8955 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8955 ; accuracy: 0.74; loss: 2.1505587100982666\n",
      "Training epoch 8956 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8956 ; accuracy: 0.74; loss: 2.1508281230926514\n",
      "Training epoch 8957 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8957 ; accuracy: 0.74; loss: 2.1511099338531494\n",
      "Training epoch 8958 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8958 ; accuracy: 0.74; loss: 2.1513679027557373\n",
      "Training epoch 8959 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8959 ; accuracy: 0.74; loss: 2.1516435146331787\n",
      "Training epoch 8960 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8960 ; accuracy: 0.74; loss: 2.1519012451171875\n",
      "Training epoch 8961 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8961 ; accuracy: 0.74; loss: 2.1521527767181396\n",
      "Training epoch 8962 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8962 ; accuracy: 0.74; loss: 2.152400016784668\n",
      "Training epoch 8963 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8963 ; accuracy: 0.74; loss: 2.152625322341919\n",
      "Training epoch 8964 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8964 ; accuracy: 0.74; loss: 2.1528480052948\n",
      "Training epoch 8965 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8965 ; accuracy: 0.74; loss: 2.153101682662964\n",
      "Training epoch 8966 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8966 ; accuracy: 0.74; loss: 2.1533498764038086\n",
      "Training epoch 8967 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8967 ; accuracy: 0.74; loss: 2.153616189956665\n",
      "Training epoch 8968 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8968 ; accuracy: 0.74; loss: 2.153913736343384\n",
      "Training epoch 8969 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8969 ; accuracy: 0.74; loss: 2.15419602394104\n",
      "Training epoch 8970 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8970 ; accuracy: 0.74; loss: 2.1544735431671143\n",
      "Training epoch 8971 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8971 ; accuracy: 0.74; loss: 2.154733896255493\n",
      "Training epoch 8972 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8972 ; accuracy: 0.74; loss: 2.154970407485962\n",
      "Training epoch 8973 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8973 ; accuracy: 0.74; loss: 2.1552023887634277\n",
      "Training epoch 8974 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8974 ; accuracy: 0.74; loss: 2.1554207801818848\n",
      "Training epoch 8975 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8975 ; accuracy: 0.74; loss: 2.155648708343506\n",
      "Training epoch 8976 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8976 ; accuracy: 0.74; loss: 2.155871868133545\n",
      "Training epoch 8977 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8977 ; accuracy: 0.74; loss: 2.156099796295166\n",
      "Training epoch 8978 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8978 ; accuracy: 0.74; loss: 2.156332015991211\n",
      "Training epoch 8979 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8979 ; accuracy: 0.74; loss: 2.1565675735473633\n",
      "Training epoch 8980 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8980 ; accuracy: 0.74; loss: 2.1568233966827393\n",
      "Training epoch 8981 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8981 ; accuracy: 0.74; loss: 2.1570892333984375\n",
      "Training epoch 8982 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8982 ; accuracy: 0.74; loss: 2.157334089279175\n",
      "Training epoch 8983 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8983 ; accuracy: 0.74; loss: 2.157599925994873\n",
      "Training epoch 8984 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 8984 ; accuracy: 0.74; loss: 2.1579196453094482\n",
      "Training epoch 8985 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8985 ; accuracy: 0.74; loss: 2.1582157611846924\n",
      "Training epoch 8986 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8986 ; accuracy: 0.74; loss: 2.1584978103637695\n",
      "Training epoch 8987 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8987 ; accuracy: 0.74; loss: 2.1587655544281006\n",
      "Training epoch 8988 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8988 ; accuracy: 0.74; loss: 2.1590280532836914\n",
      "Training epoch 8989 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8989 ; accuracy: 0.74; loss: 2.1592812538146973\n",
      "Training epoch 8990 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8990 ; accuracy: 0.74; loss: 2.1595444679260254\n",
      "Training epoch 8991 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8991 ; accuracy: 0.74; loss: 2.1598072052001953\n",
      "Training epoch 8992 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8992 ; accuracy: 0.74; loss: 2.1600658893585205\n",
      "Training epoch 8993 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8993 ; accuracy: 0.74; loss: 2.160309076309204\n",
      "Training epoch 8994 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8994 ; accuracy: 0.74; loss: 2.1605424880981445\n",
      "Training epoch 8995 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8995 ; accuracy: 0.74; loss: 2.160769462585449\n",
      "Training epoch 8996 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8996 ; accuracy: 0.74; loss: 2.1609699726104736\n",
      "Training epoch 8997 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 8997 ; accuracy: 0.74; loss: 2.161195993423462\n",
      "Training epoch 8998 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 8998 ; accuracy: 0.74; loss: 2.1614158153533936\n",
      "Training epoch 8999 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 8999 ; accuracy: 0.74; loss: 2.1616268157958984\n",
      "Training epoch 9000 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9000 ; accuracy: 0.74; loss: 2.161860942840576\n",
      "Training epoch 9001 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9001 ; accuracy: 0.74; loss: 2.162095308303833\n",
      "Training epoch 9002 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9002 ; accuracy: 0.74; loss: 2.162323236465454\n",
      "Training epoch 9003 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9003 ; accuracy: 0.74; loss: 2.1625516414642334\n",
      "Training epoch 9004 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9004 ; accuracy: 0.74; loss: 2.162801742553711\n",
      "Training epoch 9005 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9005 ; accuracy: 0.74; loss: 2.1630337238311768\n",
      "Training epoch 9006 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9006 ; accuracy: 0.74; loss: 2.163259744644165\n",
      "Training epoch 9007 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9007 ; accuracy: 0.74; loss: 2.16351056098938\n",
      "Training epoch 9008 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9008 ; accuracy: 0.74; loss: 2.1637396812438965\n",
      "Training epoch 9009 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 9009 ; accuracy: 0.74; loss: 2.164062261581421\n",
      "Training epoch 9010 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9010 ; accuracy: 0.74; loss: 2.164354085922241\n",
      "Training epoch 9011 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9011 ; accuracy: 0.74; loss: 2.164668321609497\n",
      "Training epoch 9012 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9012 ; accuracy: 0.74; loss: 2.1649696826934814\n",
      "Training epoch 9013 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9013 ; accuracy: 0.74; loss: 2.165250062942505\n",
      "Training epoch 9014 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9014 ; accuracy: 0.74; loss: 2.165534734725952\n",
      "Training epoch 9015 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9015 ; accuracy: 0.74; loss: 2.1658263206481934\n",
      "Training epoch 9016 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9016 ; accuracy: 0.74; loss: 2.166118860244751\n",
      "Training epoch 9017 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9017 ; accuracy: 0.74; loss: 2.1664516925811768\n",
      "Training epoch 9018 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9018 ; accuracy: 0.74; loss: 2.166761875152588\n",
      "Training epoch 9019 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 9019 ; accuracy: 0.74; loss: 2.167098045349121\n",
      "Training epoch 9020 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9020 ; accuracy: 0.74; loss: 2.167417287826538\n",
      "Training epoch 9021 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9021 ; accuracy: 0.74; loss: 2.1676785945892334\n",
      "Training epoch 9022 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9022 ; accuracy: 0.74; loss: 2.1678965091705322\n",
      "Training epoch 9023 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9023 ; accuracy: 0.74; loss: 2.168109893798828\n",
      "Training epoch 9024 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9024 ; accuracy: 0.74; loss: 2.168304681777954\n",
      "Training epoch 9025 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9025 ; accuracy: 0.74; loss: 2.1684911251068115\n",
      "Training epoch 9026 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9026 ; accuracy: 0.74; loss: 2.1686782836914062\n",
      "Training epoch 9027 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9027 ; accuracy: 0.74; loss: 2.168887138366699\n",
      "Training epoch 9028 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9028 ; accuracy: 0.74; loss: 2.1690735816955566\n",
      "Training epoch 9029 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9029 ; accuracy: 0.74; loss: 2.1692569255828857\n",
      "Training epoch 9030 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9030 ; accuracy: 0.74; loss: 2.169464111328125\n",
      "Training epoch 9031 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9031 ; accuracy: 0.74; loss: 2.169686794281006\n",
      "Training epoch 9032 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9032 ; accuracy: 0.74; loss: 2.16989803314209\n",
      "Training epoch 9033 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9033 ; accuracy: 0.74; loss: 2.1701366901397705\n",
      "Training epoch 9034 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9034 ; accuracy: 0.74; loss: 2.170382022857666\n",
      "Training epoch 9035 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9035 ; accuracy: 0.74; loss: 2.1706080436706543\n",
      "Training epoch 9036 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9036 ; accuracy: 0.74; loss: 2.1708145141601562\n",
      "Training epoch 9037 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9037 ; accuracy: 0.74; loss: 2.171036958694458\n",
      "Training epoch 9038 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9038 ; accuracy: 0.74; loss: 2.1712870597839355\n",
      "Training epoch 9039 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9039 ; accuracy: 0.74; loss: 2.1715097427368164\n",
      "Training epoch 9040 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9040 ; accuracy: 0.74; loss: 2.1717400550842285\n",
      "Training epoch 9041 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9041 ; accuracy: 0.74; loss: 2.171954870223999\n",
      "Training epoch 9042 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9042 ; accuracy: 0.74; loss: 2.172189712524414\n",
      "Training epoch 9043 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9043 ; accuracy: 0.74; loss: 2.1724255084991455\n",
      "Training epoch 9044 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9044 ; accuracy: 0.74; loss: 2.1726622581481934\n",
      "Training epoch 9045 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9045 ; accuracy: 0.74; loss: 2.1728949546813965\n",
      "Training epoch 9046 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9046 ; accuracy: 0.74; loss: 2.173124074935913\n",
      "Training epoch 9047 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9047 ; accuracy: 0.74; loss: 2.1733558177948\n",
      "Training epoch 9048 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9048 ; accuracy: 0.74; loss: 2.173582077026367\n",
      "Training epoch 9049 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9049 ; accuracy: 0.74; loss: 2.1738080978393555\n",
      "Training epoch 9050 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9050 ; accuracy: 0.74; loss: 2.174015998840332\n",
      "Training epoch 9051 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9051 ; accuracy: 0.74; loss: 2.174241542816162\n",
      "Training epoch 9052 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9052 ; accuracy: 0.74; loss: 2.174398899078369\n",
      "Training epoch 9053 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9053 ; accuracy: 0.74; loss: 2.1745505332946777\n",
      "Training epoch 9054 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9054 ; accuracy: 0.74; loss: 2.174692392349243\n",
      "Training epoch 9055 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9055 ; accuracy: 0.74; loss: 2.174837827682495\n",
      "Training epoch 9056 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9056 ; accuracy: 0.74; loss: 2.1749846935272217\n",
      "Training epoch 9057 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9057 ; accuracy: 0.74; loss: 2.175126075744629\n",
      "Training epoch 9058 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9058 ; accuracy: 0.74; loss: 2.1752853393554688\n",
      "Training epoch 9059 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9059 ; accuracy: 0.74; loss: 2.1754262447357178\n",
      "Training epoch 9060 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9060 ; accuracy: 0.74; loss: 2.1755788326263428\n",
      "Training epoch 9061 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9061 ; accuracy: 0.74; loss: 2.175731897354126\n",
      "Training epoch 9062 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9062 ; accuracy: 0.74; loss: 2.1758904457092285\n",
      "Training epoch 9063 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9063 ; accuracy: 0.74; loss: 2.176035165786743\n",
      "Training epoch 9064 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9064 ; accuracy: 0.74; loss: 2.176213264465332\n",
      "Training epoch 9065 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9065 ; accuracy: 0.74; loss: 2.1764254570007324\n",
      "Training epoch 9066 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9066 ; accuracy: 0.74; loss: 2.1766459941864014\n",
      "Training epoch 9067 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9067 ; accuracy: 0.74; loss: 2.1768577098846436\n",
      "Training epoch 9068 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9068 ; accuracy: 0.74; loss: 2.17708420753479\n",
      "Training epoch 9069 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9069 ; accuracy: 0.74; loss: 2.177297830581665\n",
      "Training epoch 9070 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9070 ; accuracy: 0.74; loss: 2.1775059700012207\n",
      "Training epoch 9071 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9071 ; accuracy: 0.74; loss: 2.1776835918426514\n",
      "Training epoch 9072 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9072 ; accuracy: 0.74; loss: 2.177854299545288\n",
      "Training epoch 9073 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9073 ; accuracy: 0.74; loss: 2.1780714988708496\n",
      "Training epoch 9074 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9074 ; accuracy: 0.74; loss: 2.1782898902893066\n",
      "Training epoch 9075 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9075 ; accuracy: 0.74; loss: 2.1785738468170166\n",
      "Training epoch 9076 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9076 ; accuracy: 0.74; loss: 2.1788382530212402\n",
      "Training epoch 9077 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9077 ; accuracy: 0.74; loss: 2.1791062355041504\n",
      "Training epoch 9078 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9078 ; accuracy: 0.74; loss: 2.1793837547302246\n",
      "Training epoch 9079 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9079 ; accuracy: 0.74; loss: 2.1796536445617676\n",
      "Training epoch 9080 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9080 ; accuracy: 0.74; loss: 2.1799113750457764\n",
      "Training epoch 9081 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9081 ; accuracy: 0.74; loss: 2.180157423019409\n",
      "Training epoch 9082 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9082 ; accuracy: 0.74; loss: 2.1803934574127197\n",
      "Training epoch 9083 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9083 ; accuracy: 0.74; loss: 2.1806440353393555\n",
      "Training epoch 9084 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9084 ; accuracy: 0.74; loss: 2.1808719635009766\n",
      "Training epoch 9085 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9085 ; accuracy: 0.74; loss: 2.1810970306396484\n",
      "Training epoch 9086 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9086 ; accuracy: 0.74; loss: 2.1813175678253174\n",
      "Training epoch 9087 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9087 ; accuracy: 0.74; loss: 2.1815364360809326\n",
      "Training epoch 9088 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9088 ; accuracy: 0.74; loss: 2.1817431449890137\n",
      "Training epoch 9089 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9089 ; accuracy: 0.74; loss: 2.1819658279418945\n",
      "Training epoch 9090 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9090 ; accuracy: 0.74; loss: 2.182182788848877\n",
      "Training epoch 9091 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9091 ; accuracy: 0.74; loss: 2.182389259338379\n",
      "Training epoch 9092 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9092 ; accuracy: 0.74; loss: 2.1825790405273438\n",
      "Training epoch 9093 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9093 ; accuracy: 0.74; loss: 2.1827521324157715\n",
      "Training epoch 9094 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9094 ; accuracy: 0.74; loss: 2.182924747467041\n",
      "Training epoch 9095 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9095 ; accuracy: 0.74; loss: 2.183077335357666\n",
      "Training epoch 9096 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9096 ; accuracy: 0.74; loss: 2.183218240737915\n",
      "Training epoch 9097 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9097 ; accuracy: 0.74; loss: 2.183347463607788\n",
      "Training epoch 9098 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9098 ; accuracy: 0.74; loss: 2.1834774017333984\n",
      "Training epoch 9099 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9099 ; accuracy: 0.74; loss: 2.183624267578125\n",
      "Training epoch 9100 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9100 ; accuracy: 0.74; loss: 2.183756113052368\n",
      "Training epoch 9101 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9101 ; accuracy: 0.74; loss: 2.1838598251342773\n",
      "Training epoch 9102 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9102 ; accuracy: 0.74; loss: 2.1839661598205566\n",
      "Training epoch 9103 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9103 ; accuracy: 0.74; loss: 2.1840243339538574\n",
      "Training epoch 9104 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9104 ; accuracy: 0.74; loss: 2.184084892272949\n",
      "Training epoch 9105 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9105 ; accuracy: 0.74; loss: 2.184152126312256\n",
      "Training epoch 9106 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9106 ; accuracy: 0.74; loss: 2.1842198371887207\n",
      "Training epoch 9107 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9107 ; accuracy: 0.74; loss: 2.184302806854248\n",
      "Training epoch 9108 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9108 ; accuracy: 0.74; loss: 2.1843957901000977\n",
      "Training epoch 9109 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9109 ; accuracy: 0.74; loss: 2.184471368789673\n",
      "Training epoch 9110 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9110 ; accuracy: 0.74; loss: 2.184540271759033\n",
      "Training epoch 9111 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9111 ; accuracy: 0.74; loss: 2.184642791748047\n",
      "Training epoch 9112 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9112 ; accuracy: 0.74; loss: 2.1847665309906006\n",
      "Training epoch 9113 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9113 ; accuracy: 0.74; loss: 2.1848819255828857\n",
      "Training epoch 9114 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9114 ; accuracy: 0.74; loss: 2.1850032806396484\n",
      "Training epoch 9115 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9115 ; accuracy: 0.74; loss: 2.1851377487182617\n",
      "Training epoch 9116 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9116 ; accuracy: 0.74; loss: 2.185276508331299\n",
      "Training epoch 9117 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9117 ; accuracy: 0.74; loss: 2.1854076385498047\n",
      "Training epoch 9118 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9118 ; accuracy: 0.74; loss: 2.1855337619781494\n",
      "Training epoch 9119 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9119 ; accuracy: 0.74; loss: 2.185631036758423\n",
      "Training epoch 9120 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9120 ; accuracy: 0.74; loss: 2.1857051849365234\n",
      "Training epoch 9121 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9121 ; accuracy: 0.74; loss: 2.185784101486206\n",
      "Training epoch 9122 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9122 ; accuracy: 0.74; loss: 2.1858832836151123\n",
      "Training epoch 9123 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9123 ; accuracy: 0.74; loss: 2.185983657836914\n",
      "Training epoch 9124 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9124 ; accuracy: 0.74; loss: 2.1860783100128174\n",
      "Training epoch 9125 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9125 ; accuracy: 0.74; loss: 2.186173677444458\n",
      "Training epoch 9126 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9126 ; accuracy: 0.74; loss: 2.1862661838531494\n",
      "Training epoch 9127 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9127 ; accuracy: 0.74; loss: 2.1863856315612793\n",
      "Training epoch 9128 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9128 ; accuracy: 0.74; loss: 2.1865310668945312\n",
      "Training epoch 9129 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9129 ; accuracy: 0.74; loss: 2.1866776943206787\n",
      "Training epoch 9130 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9130 ; accuracy: 0.74; loss: 2.1868321895599365\n",
      "Training epoch 9131 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9131 ; accuracy: 0.74; loss: 2.1869654655456543\n",
      "Training epoch 9132 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9132 ; accuracy: 0.74; loss: 2.1871137619018555\n",
      "Training epoch 9133 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9133 ; accuracy: 0.74; loss: 2.1872880458831787\n",
      "Training epoch 9134 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9134 ; accuracy: 0.74; loss: 2.1874632835388184\n",
      "Training epoch 9135 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9135 ; accuracy: 0.74; loss: 2.187635898590088\n",
      "Training epoch 9136 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9136 ; accuracy: 0.74; loss: 2.187812566757202\n",
      "Training epoch 9137 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9137 ; accuracy: 0.74; loss: 2.1879818439483643\n",
      "Training epoch 9138 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9138 ; accuracy: 0.74; loss: 2.1881346702575684\n",
      "Training epoch 9139 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9139 ; accuracy: 0.74; loss: 2.188265323638916\n",
      "Training epoch 9140 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9140 ; accuracy: 0.74; loss: 2.1883955001831055\n",
      "Training epoch 9141 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9141 ; accuracy: 0.74; loss: 2.1885223388671875\n",
      "Training epoch 9142 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9142 ; accuracy: 0.74; loss: 2.1886467933654785\n",
      "Training epoch 9143 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9143 ; accuracy: 0.74; loss: 2.188767671585083\n",
      "Training epoch 9144 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9144 ; accuracy: 0.74; loss: 2.188873767852783\n",
      "Training epoch 9145 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9145 ; accuracy: 0.74; loss: 2.188981533050537\n",
      "Training epoch 9146 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9146 ; accuracy: 0.74; loss: 2.1890928745269775\n",
      "Training epoch 9147 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9147 ; accuracy: 0.74; loss: 2.1892125606536865\n",
      "Training epoch 9148 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9148 ; accuracy: 0.74; loss: 2.1893415451049805\n",
      "Training epoch 9149 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9149 ; accuracy: 0.74; loss: 2.1894876956939697\n",
      "Training epoch 9150 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9150 ; accuracy: 0.74; loss: 2.1896040439605713\n",
      "Training epoch 9151 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9151 ; accuracy: 0.74; loss: 2.189706325531006\n",
      "Training epoch 9152 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9152 ; accuracy: 0.74; loss: 2.189802408218384\n",
      "Training epoch 9153 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9153 ; accuracy: 0.74; loss: 2.1898891925811768\n",
      "Training epoch 9154 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9154 ; accuracy: 0.74; loss: 2.1899869441986084\n",
      "Training epoch 9155 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9155 ; accuracy: 0.74; loss: 2.190089225769043\n",
      "Training epoch 9156 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9156 ; accuracy: 0.74; loss: 2.190187454223633\n",
      "Training epoch 9157 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9157 ; accuracy: 0.74; loss: 2.1902642250061035\n",
      "Training epoch 9158 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9158 ; accuracy: 0.74; loss: 2.1903374195098877\n",
      "Training epoch 9159 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9159 ; accuracy: 0.74; loss: 2.1903998851776123\n",
      "Training epoch 9160 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9160 ; accuracy: 0.74; loss: 2.190467357635498\n",
      "Training epoch 9161 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9161 ; accuracy: 0.74; loss: 2.1905505657196045\n",
      "Training epoch 9162 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9162 ; accuracy: 0.74; loss: 2.1906607151031494\n",
      "Training epoch 9163 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9163 ; accuracy: 0.74; loss: 2.190763235092163\n",
      "Training epoch 9164 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9164 ; accuracy: 0.74; loss: 2.1908538341522217\n",
      "Training epoch 9165 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9165 ; accuracy: 0.74; loss: 2.1909408569335938\n",
      "Training epoch 9166 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9166 ; accuracy: 0.74; loss: 2.191047191619873\n",
      "Training epoch 9167 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9167 ; accuracy: 0.74; loss: 2.1911725997924805\n",
      "Training epoch 9168 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9168 ; accuracy: 0.74; loss: 2.191304922103882\n",
      "Training epoch 9169 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9169 ; accuracy: 0.74; loss: 2.1914398670196533\n",
      "Training epoch 9170 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9170 ; accuracy: 0.74; loss: 2.1915760040283203\n",
      "Training epoch 9171 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9171 ; accuracy: 0.74; loss: 2.1917502880096436\n",
      "Training epoch 9172 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9172 ; accuracy: 0.74; loss: 2.1918962001800537\n",
      "Training epoch 9173 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9173 ; accuracy: 0.74; loss: 2.1920316219329834\n",
      "Training epoch 9174 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9174 ; accuracy: 0.74; loss: 2.192135810852051\n",
      "Training epoch 9175 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9175 ; accuracy: 0.74; loss: 2.192239284515381\n",
      "Training epoch 9176 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9176 ; accuracy: 0.74; loss: 2.192359685897827\n",
      "Training epoch 9177 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9177 ; accuracy: 0.74; loss: 2.1925032138824463\n",
      "Training epoch 9178 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9178 ; accuracy: 0.74; loss: 2.192641258239746\n",
      "Training epoch 9179 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 9179 ; accuracy: 0.74; loss: 2.1928699016571045\n",
      "Training epoch 9180 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9180 ; accuracy: 0.74; loss: 2.1931004524230957\n",
      "Training epoch 9181 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9181 ; accuracy: 0.74; loss: 2.193326950073242\n",
      "Training epoch 9182 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9182 ; accuracy: 0.74; loss: 2.1935653686523438\n",
      "Training epoch 9183 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9183 ; accuracy: 0.74; loss: 2.193793296813965\n",
      "Training epoch 9184 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9184 ; accuracy: 0.74; loss: 2.194011926651001\n",
      "Training epoch 9185 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9185 ; accuracy: 0.74; loss: 2.1942298412323\n",
      "Training epoch 9186 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9186 ; accuracy: 0.74; loss: 2.1944611072540283\n",
      "Training epoch 9187 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9187 ; accuracy: 0.74; loss: 2.194681167602539\n",
      "Training epoch 9188 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9188 ; accuracy: 0.74; loss: 2.1948797702789307\n",
      "Training epoch 9189 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9189 ; accuracy: 0.74; loss: 2.195078134536743\n",
      "Training epoch 9190 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9190 ; accuracy: 0.74; loss: 2.195260524749756\n",
      "Training epoch 9191 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9191 ; accuracy: 0.74; loss: 2.195420503616333\n",
      "Training epoch 9192 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9192 ; accuracy: 0.74; loss: 2.195610523223877\n",
      "Training epoch 9193 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9193 ; accuracy: 0.74; loss: 2.1958045959472656\n",
      "Training epoch 9194 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9194 ; accuracy: 0.74; loss: 2.195976734161377\n",
      "Training epoch 9195 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9195 ; accuracy: 0.74; loss: 2.196152925491333\n",
      "Training epoch 9196 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9196 ; accuracy: 0.74; loss: 2.196303129196167\n",
      "Training epoch 9197 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9197 ; accuracy: 0.74; loss: 2.1964468955993652\n",
      "Training epoch 9198 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9198 ; accuracy: 0.74; loss: 2.196582078933716\n",
      "Training epoch 9199 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9199 ; accuracy: 0.74; loss: 2.196720838546753\n",
      "Training epoch 9200 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9200 ; accuracy: 0.74; loss: 2.196847438812256\n",
      "Training epoch 9201 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9201 ; accuracy: 0.74; loss: 2.19698429107666\n",
      "Training epoch 9202 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9202 ; accuracy: 0.74; loss: 2.197096347808838\n",
      "Training epoch 9203 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9203 ; accuracy: 0.74; loss: 2.1972012519836426\n",
      "Training epoch 9204 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9204 ; accuracy: 0.74; loss: 2.1973142623901367\n",
      "Training epoch 9205 ; accuracy: 0.9; loss: 0.19459106028079987\n",
      "Validation epoch 9205 ; accuracy: 0.74; loss: 2.197658061981201\n",
      "Training epoch 9206 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9206 ; accuracy: 0.74; loss: 2.1979706287384033\n",
      "Training epoch 9207 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9207 ; accuracy: 0.74; loss: 2.1982758045196533\n",
      "Training epoch 9208 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9208 ; accuracy: 0.74; loss: 2.198561906814575\n",
      "Training epoch 9209 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9209 ; accuracy: 0.74; loss: 2.198855400085449\n",
      "Training epoch 9210 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9210 ; accuracy: 0.74; loss: 2.199134588241577\n",
      "Training epoch 9211 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9211 ; accuracy: 0.74; loss: 2.199398994445801\n",
      "Training epoch 9212 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9212 ; accuracy: 0.74; loss: 2.199641704559326\n",
      "Training epoch 9213 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9213 ; accuracy: 0.74; loss: 2.1998724937438965\n",
      "Training epoch 9214 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9214 ; accuracy: 0.74; loss: 2.20009183883667\n",
      "Training epoch 9215 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9215 ; accuracy: 0.74; loss: 2.200303077697754\n",
      "Training epoch 9216 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9216 ; accuracy: 0.74; loss: 2.200517177581787\n",
      "Training epoch 9217 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9217 ; accuracy: 0.74; loss: 2.2007014751434326\n",
      "Training epoch 9218 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9218 ; accuracy: 0.74; loss: 2.200890064239502\n",
      "Training epoch 9219 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9219 ; accuracy: 0.74; loss: 2.201076030731201\n",
      "Training epoch 9220 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9220 ; accuracy: 0.74; loss: 2.2012696266174316\n",
      "Training epoch 9221 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9221 ; accuracy: 0.74; loss: 2.201418876647949\n",
      "Training epoch 9222 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9222 ; accuracy: 0.74; loss: 2.2015700340270996\n",
      "Training epoch 9223 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9223 ; accuracy: 0.74; loss: 2.2017250061035156\n",
      "Training epoch 9224 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9224 ; accuracy: 0.74; loss: 2.2018799781799316\n",
      "Training epoch 9225 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9225 ; accuracy: 0.74; loss: 2.202018976211548\n",
      "Training epoch 9226 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9226 ; accuracy: 0.74; loss: 2.2021498680114746\n",
      "Training epoch 9227 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9227 ; accuracy: 0.74; loss: 2.202286958694458\n",
      "Training epoch 9228 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9228 ; accuracy: 0.74; loss: 2.2024166584014893\n",
      "Training epoch 9229 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9229 ; accuracy: 0.74; loss: 2.20253324508667\n",
      "Training epoch 9230 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9230 ; accuracy: 0.74; loss: 2.2026376724243164\n",
      "Training epoch 9231 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9231 ; accuracy: 0.74; loss: 2.2027485370635986\n",
      "Training epoch 9232 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9232 ; accuracy: 0.74; loss: 2.2028489112854004\n",
      "Training epoch 9233 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9233 ; accuracy: 0.74; loss: 2.202948808670044\n",
      "Training epoch 9234 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9234 ; accuracy: 0.74; loss: 2.203052043914795\n",
      "Training epoch 9235 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9235 ; accuracy: 0.74; loss: 2.203150749206543\n",
      "Training epoch 9236 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9236 ; accuracy: 0.74; loss: 2.203249454498291\n",
      "Training epoch 9237 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9237 ; accuracy: 0.74; loss: 2.203354597091675\n",
      "Training epoch 9238 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9238 ; accuracy: 0.74; loss: 2.203453302383423\n",
      "Training epoch 9239 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9239 ; accuracy: 0.74; loss: 2.2035486698150635\n",
      "Training epoch 9240 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9240 ; accuracy: 0.74; loss: 2.203658103942871\n",
      "Training epoch 9241 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9241 ; accuracy: 0.74; loss: 2.2037456035614014\n",
      "Training epoch 9242 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9242 ; accuracy: 0.74; loss: 2.203829526901245\n",
      "Training epoch 9243 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9243 ; accuracy: 0.74; loss: 2.203902244567871\n",
      "Training epoch 9244 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9244 ; accuracy: 0.74; loss: 2.2039833068847656\n",
      "Training epoch 9245 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9245 ; accuracy: 0.74; loss: 2.204068899154663\n",
      "Training epoch 9246 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9246 ; accuracy: 0.74; loss: 2.2041382789611816\n",
      "Training epoch 9247 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9247 ; accuracy: 0.7433333333333333; loss: 2.2042057514190674\n",
      "Training epoch 9248 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9248 ; accuracy: 0.7433333333333333; loss: 2.2042741775512695\n",
      "Training epoch 9249 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9249 ; accuracy: 0.7433333333333333; loss: 2.2043612003326416\n",
      "Training epoch 9250 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9250 ; accuracy: 0.7433333333333333; loss: 2.204458236694336\n",
      "Training epoch 9251 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9251 ; accuracy: 0.7433333333333333; loss: 2.2045538425445557\n",
      "Training epoch 9252 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9252 ; accuracy: 0.7433333333333333; loss: 2.2047574520111084\n",
      "Training epoch 9253 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9253 ; accuracy: 0.7433333333333333; loss: 2.2049498558044434\n",
      "Training epoch 9254 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9254 ; accuracy: 0.7433333333333333; loss: 2.2051305770874023\n",
      "Training epoch 9255 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9255 ; accuracy: 0.7433333333333333; loss: 2.205291509628296\n",
      "Training epoch 9256 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9256 ; accuracy: 0.7433333333333333; loss: 2.2054362297058105\n",
      "Training epoch 9257 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9257 ; accuracy: 0.7433333333333333; loss: 2.205564260482788\n",
      "Training epoch 9258 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9258 ; accuracy: 0.7433333333333333; loss: 2.2056891918182373\n",
      "Training epoch 9259 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9259 ; accuracy: 0.7433333333333333; loss: 2.2058205604553223\n",
      "Training epoch 9260 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9260 ; accuracy: 0.7433333333333333; loss: 2.2059316635131836\n",
      "Training epoch 9261 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9261 ; accuracy: 0.7433333333333333; loss: 2.2060062885284424\n",
      "Training epoch 9262 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9262 ; accuracy: 0.7433333333333333; loss: 2.206063985824585\n",
      "Training epoch 9263 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9263 ; accuracy: 0.7433333333333333; loss: 2.2061100006103516\n",
      "Training epoch 9264 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9264 ; accuracy: 0.7433333333333333; loss: 2.206144332885742\n",
      "Training epoch 9265 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9265 ; accuracy: 0.7433333333333333; loss: 2.206158399581909\n",
      "Training epoch 9266 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9266 ; accuracy: 0.7433333333333333; loss: 2.2061710357666016\n",
      "Training epoch 9267 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9267 ; accuracy: 0.7433333333333333; loss: 2.206191301345825\n",
      "Training epoch 9268 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9268 ; accuracy: 0.7433333333333333; loss: 2.2062220573425293\n",
      "Training epoch 9269 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9269 ; accuracy: 0.7433333333333333; loss: 2.2062430381774902\n",
      "Training epoch 9270 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9270 ; accuracy: 0.7433333333333333; loss: 2.206256151199341\n",
      "Training epoch 9271 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9271 ; accuracy: 0.7433333333333333; loss: 2.206256866455078\n",
      "Training epoch 9272 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9272 ; accuracy: 0.7433333333333333; loss: 2.2062530517578125\n",
      "Training epoch 9273 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9273 ; accuracy: 0.7433333333333333; loss: 2.20623517036438\n",
      "Training epoch 9274 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9274 ; accuracy: 0.7433333333333333; loss: 2.2062270641326904\n",
      "Training epoch 9275 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9275 ; accuracy: 0.7433333333333333; loss: 2.2062184810638428\n",
      "Training epoch 9276 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9276 ; accuracy: 0.7433333333333333; loss: 2.2062108516693115\n",
      "Training epoch 9277 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9277 ; accuracy: 0.7433333333333333; loss: 2.206209659576416\n",
      "Training epoch 9278 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9278 ; accuracy: 0.7433333333333333; loss: 2.206212043762207\n",
      "Training epoch 9279 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9279 ; accuracy: 0.7433333333333333; loss: 2.206265687942505\n",
      "Training epoch 9280 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9280 ; accuracy: 0.7433333333333333; loss: 2.2063217163085938\n",
      "Training epoch 9281 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9281 ; accuracy: 0.7433333333333333; loss: 2.2063634395599365\n",
      "Training epoch 9282 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9282 ; accuracy: 0.7433333333333333; loss: 2.2063841819763184\n",
      "Training epoch 9283 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9283 ; accuracy: 0.7433333333333333; loss: 2.2064104080200195\n",
      "Training epoch 9284 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9284 ; accuracy: 0.7433333333333333; loss: 2.2064435482025146\n",
      "Training epoch 9285 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9285 ; accuracy: 0.7433333333333333; loss: 2.2064785957336426\n",
      "Training epoch 9286 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9286 ; accuracy: 0.7433333333333333; loss: 2.206517219543457\n",
      "Training epoch 9287 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9287 ; accuracy: 0.7433333333333333; loss: 2.2065534591674805\n",
      "Training epoch 9288 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9288 ; accuracy: 0.7433333333333333; loss: 2.206594944000244\n",
      "Training epoch 9289 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9289 ; accuracy: 0.7433333333333333; loss: 2.2066242694854736\n",
      "Training epoch 9290 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9290 ; accuracy: 0.7433333333333333; loss: 2.206660032272339\n",
      "Training epoch 9291 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9291 ; accuracy: 0.7433333333333333; loss: 2.206705331802368\n",
      "Training epoch 9292 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9292 ; accuracy: 0.7433333333333333; loss: 2.2067553997039795\n",
      "Training epoch 9293 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9293 ; accuracy: 0.7433333333333333; loss: 2.206801652908325\n",
      "Training epoch 9294 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9294 ; accuracy: 0.7433333333333333; loss: 2.206848382949829\n",
      "Training epoch 9295 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9295 ; accuracy: 0.7433333333333333; loss: 2.2069411277770996\n",
      "Training epoch 9296 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9296 ; accuracy: 0.7433333333333333; loss: 2.2070326805114746\n",
      "Training epoch 9297 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9297 ; accuracy: 0.7433333333333333; loss: 2.207141637802124\n",
      "Training epoch 9298 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9298 ; accuracy: 0.7433333333333333; loss: 2.207252025604248\n",
      "Training epoch 9299 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9299 ; accuracy: 0.7433333333333333; loss: 2.2073590755462646\n",
      "Training epoch 9300 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9300 ; accuracy: 0.7433333333333333; loss: 2.2074646949768066\n",
      "Training epoch 9301 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9301 ; accuracy: 0.7433333333333333; loss: 2.2075839042663574\n",
      "Training epoch 9302 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9302 ; accuracy: 0.7433333333333333; loss: 2.20770525932312\n",
      "Training epoch 9303 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9303 ; accuracy: 0.7433333333333333; loss: 2.2078282833099365\n",
      "Training epoch 9304 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9304 ; accuracy: 0.7433333333333333; loss: 2.207944393157959\n",
      "Training epoch 9305 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9305 ; accuracy: 0.7433333333333333; loss: 2.2080509662628174\n",
      "Training epoch 9306 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9306 ; accuracy: 0.7433333333333333; loss: 2.2081291675567627\n",
      "Training epoch 9307 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9307 ; accuracy: 0.7433333333333333; loss: 2.208193302154541\n",
      "Training epoch 9308 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9308 ; accuracy: 0.7433333333333333; loss: 2.208221673965454\n",
      "Training epoch 9309 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9309 ; accuracy: 0.7433333333333333; loss: 2.2082555294036865\n",
      "Training epoch 9310 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9310 ; accuracy: 0.7433333333333333; loss: 2.2082996368408203\n",
      "Training epoch 9311 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9311 ; accuracy: 0.7433333333333333; loss: 2.2083547115325928\n",
      "Training epoch 9312 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9312 ; accuracy: 0.7433333333333333; loss: 2.2084126472473145\n",
      "Training epoch 9313 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9313 ; accuracy: 0.7433333333333333; loss: 2.208456039428711\n",
      "Training epoch 9314 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9314 ; accuracy: 0.7433333333333333; loss: 2.208514928817749\n",
      "Training epoch 9315 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9315 ; accuracy: 0.7433333333333333; loss: 2.208571195602417\n",
      "Training epoch 9316 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9316 ; accuracy: 0.7433333333333333; loss: 2.2086143493652344\n",
      "Training epoch 9317 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9317 ; accuracy: 0.7433333333333333; loss: 2.208665370941162\n",
      "Training epoch 9318 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9318 ; accuracy: 0.7433333333333333; loss: 2.208726167678833\n",
      "Training epoch 9319 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9319 ; accuracy: 0.7433333333333333; loss: 2.20879864692688\n",
      "Training epoch 9320 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9320 ; accuracy: 0.7433333333333333; loss: 2.208867073059082\n",
      "Training epoch 9321 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9321 ; accuracy: 0.7433333333333333; loss: 2.2089202404022217\n",
      "Training epoch 9322 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9322 ; accuracy: 0.7433333333333333; loss: 2.208955764770508\n",
      "Training epoch 9323 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9323 ; accuracy: 0.7433333333333333; loss: 2.208989381790161\n",
      "Training epoch 9324 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9324 ; accuracy: 0.7433333333333333; loss: 2.2090184688568115\n",
      "Training epoch 9325 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9325 ; accuracy: 0.7433333333333333; loss: 2.209054946899414\n",
      "Training epoch 9326 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9326 ; accuracy: 0.7433333333333333; loss: 2.2091002464294434\n",
      "Training epoch 9327 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9327 ; accuracy: 0.7433333333333333; loss: 2.209139108657837\n",
      "Training epoch 9328 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9328 ; accuracy: 0.7433333333333333; loss: 2.2091848850250244\n",
      "Training epoch 9329 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9329 ; accuracy: 0.7433333333333333; loss: 2.2092254161834717\n",
      "Training epoch 9330 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9330 ; accuracy: 0.7433333333333333; loss: 2.209299325942993\n",
      "Training epoch 9331 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9331 ; accuracy: 0.7433333333333333; loss: 2.2093770503997803\n",
      "Training epoch 9332 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9332 ; accuracy: 0.7433333333333333; loss: 2.209451198577881\n",
      "Training epoch 9333 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9333 ; accuracy: 0.7433333333333333; loss: 2.2094647884368896\n",
      "Training epoch 9334 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9334 ; accuracy: 0.7433333333333333; loss: 2.2095067501068115\n",
      "Training epoch 9335 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9335 ; accuracy: 0.7433333333333333; loss: 2.2095677852630615\n",
      "Training epoch 9336 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9336 ; accuracy: 0.7433333333333333; loss: 2.2096033096313477\n",
      "Training epoch 9337 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9337 ; accuracy: 0.7433333333333333; loss: 2.2096328735351562\n",
      "Training epoch 9338 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9338 ; accuracy: 0.7433333333333333; loss: 2.2096683979034424\n",
      "Training epoch 9339 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9339 ; accuracy: 0.7433333333333333; loss: 2.2097105979919434\n",
      "Training epoch 9340 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9340 ; accuracy: 0.7433333333333333; loss: 2.2097206115722656\n",
      "Training epoch 9341 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9341 ; accuracy: 0.7433333333333333; loss: 2.209711790084839\n",
      "Training epoch 9342 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9342 ; accuracy: 0.7433333333333333; loss: 2.209728240966797\n",
      "Training epoch 9343 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9343 ; accuracy: 0.7433333333333333; loss: 2.2097702026367188\n",
      "Training epoch 9344 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9344 ; accuracy: 0.7433333333333333; loss: 2.2098166942596436\n",
      "Training epoch 9345 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9345 ; accuracy: 0.7433333333333333; loss: 2.209852695465088\n",
      "Training epoch 9346 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9346 ; accuracy: 0.7433333333333333; loss: 2.2099087238311768\n",
      "Training epoch 9347 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9347 ; accuracy: 0.7433333333333333; loss: 2.209965705871582\n",
      "Training epoch 9348 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9348 ; accuracy: 0.7433333333333333; loss: 2.2100601196289062\n",
      "Training epoch 9349 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9349 ; accuracy: 0.7433333333333333; loss: 2.210165023803711\n",
      "Training epoch 9350 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9350 ; accuracy: 0.7433333333333333; loss: 2.210284471511841\n",
      "Training epoch 9351 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9351 ; accuracy: 0.7433333333333333; loss: 2.2104008197784424\n",
      "Training epoch 9352 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9352 ; accuracy: 0.7433333333333333; loss: 2.2105302810668945\n",
      "Training epoch 9353 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9353 ; accuracy: 0.7433333333333333; loss: 2.2106645107269287\n",
      "Training epoch 9354 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9354 ; accuracy: 0.7433333333333333; loss: 2.210787534713745\n",
      "Training epoch 9355 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9355 ; accuracy: 0.7433333333333333; loss: 2.2109310626983643\n",
      "Training epoch 9356 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9356 ; accuracy: 0.7433333333333333; loss: 2.211060047149658\n",
      "Training epoch 9357 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9357 ; accuracy: 0.7433333333333333; loss: 2.2112035751342773\n",
      "Training epoch 9358 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9358 ; accuracy: 0.7433333333333333; loss: 2.2113609313964844\n",
      "Training epoch 9359 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9359 ; accuracy: 0.7433333333333333; loss: 2.211501359939575\n",
      "Training epoch 9360 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9360 ; accuracy: 0.7433333333333333; loss: 2.2116360664367676\n",
      "Training epoch 9361 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9361 ; accuracy: 0.7433333333333333; loss: 2.2117578983306885\n",
      "Training epoch 9362 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9362 ; accuracy: 0.7433333333333333; loss: 2.2118725776672363\n",
      "Training epoch 9363 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9363 ; accuracy: 0.7433333333333333; loss: 2.211979389190674\n",
      "Training epoch 9364 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9364 ; accuracy: 0.7433333333333333; loss: 2.212085485458374\n",
      "Training epoch 9365 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9365 ; accuracy: 0.7433333333333333; loss: 2.212190866470337\n",
      "Training epoch 9366 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9366 ; accuracy: 0.7433333333333333; loss: 2.2122879028320312\n",
      "Training epoch 9367 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9367 ; accuracy: 0.7433333333333333; loss: 2.2123756408691406\n",
      "Training epoch 9368 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9368 ; accuracy: 0.7433333333333333; loss: 2.212466239929199\n",
      "Training epoch 9369 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9369 ; accuracy: 0.7433333333333333; loss: 2.2125561237335205\n",
      "Training epoch 9370 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9370 ; accuracy: 0.7433333333333333; loss: 2.212643623352051\n",
      "Training epoch 9371 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9371 ; accuracy: 0.7433333333333333; loss: 2.212733507156372\n",
      "Training epoch 9372 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9372 ; accuracy: 0.7433333333333333; loss: 2.212820291519165\n",
      "Training epoch 9373 ; accuracy: 0.9; loss: 0.19459110498428345\n",
      "Validation epoch 9373 ; accuracy: 0.7433333333333333; loss: 2.213313102722168\n",
      "Training epoch 9374 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9374 ; accuracy: 0.7433333333333333; loss: 2.2137696743011475\n",
      "Training epoch 9375 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9375 ; accuracy: 0.7433333333333333; loss: 2.2141849994659424\n",
      "Training epoch 9376 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9376 ; accuracy: 0.7433333333333333; loss: 2.214550495147705\n",
      "Training epoch 9377 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9377 ; accuracy: 0.7433333333333333; loss: 2.2148749828338623\n",
      "Training epoch 9378 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9378 ; accuracy: 0.7433333333333333; loss: 2.215191602706909\n",
      "Training epoch 9379 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9379 ; accuracy: 0.7433333333333333; loss: 2.215496301651001\n",
      "Training epoch 9380 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9380 ; accuracy: 0.7433333333333333; loss: 2.2158002853393555\n",
      "Training epoch 9381 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9381 ; accuracy: 0.7433333333333333; loss: 2.2160873413085938\n",
      "Training epoch 9382 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9382 ; accuracy: 0.7433333333333333; loss: 2.216351270675659\n",
      "Training epoch 9383 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9383 ; accuracy: 0.7433333333333333; loss: 2.216611623764038\n",
      "Training epoch 9384 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9384 ; accuracy: 0.7433333333333333; loss: 2.216848134994507\n",
      "Training epoch 9385 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9385 ; accuracy: 0.7433333333333333; loss: 2.217053174972534\n",
      "Training epoch 9386 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9386 ; accuracy: 0.7433333333333333; loss: 2.2172462940216064\n",
      "Training epoch 9387 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9387 ; accuracy: 0.7433333333333333; loss: 2.217432975769043\n",
      "Training epoch 9388 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9388 ; accuracy: 0.7433333333333333; loss: 2.217588186264038\n",
      "Training epoch 9389 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9389 ; accuracy: 0.7433333333333333; loss: 2.2177393436431885\n",
      "Training epoch 9390 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9390 ; accuracy: 0.7433333333333333; loss: 2.2178900241851807\n",
      "Training epoch 9391 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9391 ; accuracy: 0.7433333333333333; loss: 2.218055248260498\n",
      "Training epoch 9392 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9392 ; accuracy: 0.7433333333333333; loss: 2.218219518661499\n",
      "Training epoch 9393 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9393 ; accuracy: 0.7433333333333333; loss: 2.2183613777160645\n",
      "Training epoch 9394 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9394 ; accuracy: 0.7433333333333333; loss: 2.2184791564941406\n",
      "Training epoch 9395 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9395 ; accuracy: 0.7433333333333333; loss: 2.2185897827148438\n",
      "Training epoch 9396 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9396 ; accuracy: 0.7433333333333333; loss: 2.218737840652466\n",
      "Training epoch 9397 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9397 ; accuracy: 0.7433333333333333; loss: 2.2188966274261475\n",
      "Training epoch 9398 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9398 ; accuracy: 0.7433333333333333; loss: 2.2190401554107666\n",
      "Training epoch 9399 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9399 ; accuracy: 0.7433333333333333; loss: 2.2191598415374756\n",
      "Training epoch 9400 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9400 ; accuracy: 0.7433333333333333; loss: 2.219265937805176\n",
      "Training epoch 9401 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9401 ; accuracy: 0.7433333333333333; loss: 2.219372272491455\n",
      "Training epoch 9402 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9402 ; accuracy: 0.7433333333333333; loss: 2.2194652557373047\n",
      "Training epoch 9403 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9403 ; accuracy: 0.7433333333333333; loss: 2.2195565700531006\n",
      "Training epoch 9404 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9404 ; accuracy: 0.7433333333333333; loss: 2.2196455001831055\n",
      "Training epoch 9405 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9405 ; accuracy: 0.7433333333333333; loss: 2.219735860824585\n",
      "Training epoch 9406 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9406 ; accuracy: 0.7433333333333333; loss: 2.219844102859497\n",
      "Training epoch 9407 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9407 ; accuracy: 0.7433333333333333; loss: 2.2199547290802\n",
      "Training epoch 9408 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9408 ; accuracy: 0.7433333333333333; loss: 2.220046281814575\n",
      "Training epoch 9409 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9409 ; accuracy: 0.7433333333333333; loss: 2.2201294898986816\n",
      "Training epoch 9410 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9410 ; accuracy: 0.7433333333333333; loss: 2.2202184200286865\n",
      "Training epoch 9411 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9411 ; accuracy: 0.7433333333333333; loss: 2.220303773880005\n",
      "Training epoch 9412 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9412 ; accuracy: 0.7433333333333333; loss: 2.22039532661438\n",
      "Training epoch 9413 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9413 ; accuracy: 0.7433333333333333; loss: 2.220482349395752\n",
      "Training epoch 9414 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9414 ; accuracy: 0.7433333333333333; loss: 2.220581293106079\n",
      "Training epoch 9415 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9415 ; accuracy: 0.7433333333333333; loss: 2.220670461654663\n",
      "Training epoch 9416 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9416 ; accuracy: 0.7433333333333333; loss: 2.220764636993408\n",
      "Training epoch 9417 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9417 ; accuracy: 0.7433333333333333; loss: 2.220850706100464\n",
      "Training epoch 9418 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9418 ; accuracy: 0.7433333333333333; loss: 2.220960855484009\n",
      "Training epoch 9419 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9419 ; accuracy: 0.7433333333333333; loss: 2.2210452556610107\n",
      "Training epoch 9420 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9420 ; accuracy: 0.7433333333333333; loss: 2.221132278442383\n",
      "Training epoch 9421 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9421 ; accuracy: 0.7433333333333333; loss: 2.221214771270752\n",
      "Training epoch 9422 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9422 ; accuracy: 0.7433333333333333; loss: 2.2213032245635986\n",
      "Training epoch 9423 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9423 ; accuracy: 0.7433333333333333; loss: 2.221392869949341\n",
      "Training epoch 9424 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9424 ; accuracy: 0.7433333333333333; loss: 2.2214839458465576\n",
      "Training epoch 9425 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9425 ; accuracy: 0.7433333333333333; loss: 2.221564531326294\n",
      "Training epoch 9426 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9426 ; accuracy: 0.7433333333333333; loss: 2.221651315689087\n",
      "Training epoch 9427 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9427 ; accuracy: 0.7433333333333333; loss: 2.221776008605957\n",
      "Training epoch 9428 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9428 ; accuracy: 0.7433333333333333; loss: 2.221897840499878\n",
      "Training epoch 9429 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9429 ; accuracy: 0.7433333333333333; loss: 2.2220113277435303\n",
      "Training epoch 9430 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9430 ; accuracy: 0.7433333333333333; loss: 2.222132682800293\n",
      "Training epoch 9431 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9431 ; accuracy: 0.7433333333333333; loss: 2.2222421169281006\n",
      "Training epoch 9432 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9432 ; accuracy: 0.7433333333333333; loss: 2.222283363342285\n",
      "Training epoch 9433 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9433 ; accuracy: 0.7433333333333333; loss: 2.222315788269043\n",
      "Training epoch 9434 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9434 ; accuracy: 0.7433333333333333; loss: 2.2223660945892334\n",
      "Training epoch 9435 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9435 ; accuracy: 0.7433333333333333; loss: 2.2224128246307373\n",
      "Training epoch 9436 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9436 ; accuracy: 0.7433333333333333; loss: 2.222461462020874\n",
      "Training epoch 9437 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9437 ; accuracy: 0.7433333333333333; loss: 2.222524642944336\n",
      "Training epoch 9438 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9438 ; accuracy: 0.7433333333333333; loss: 2.222574234008789\n",
      "Training epoch 9439 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9439 ; accuracy: 0.7433333333333333; loss: 2.222625732421875\n",
      "Training epoch 9440 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9440 ; accuracy: 0.7433333333333333; loss: 2.2226831912994385\n",
      "Training epoch 9441 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9441 ; accuracy: 0.7433333333333333; loss: 2.2227418422698975\n",
      "Training epoch 9442 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9442 ; accuracy: 0.7433333333333333; loss: 2.222794532775879\n",
      "Training epoch 9443 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9443 ; accuracy: 0.7433333333333333; loss: 2.2228453159332275\n",
      "Training epoch 9444 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9444 ; accuracy: 0.7433333333333333; loss: 2.2228991985321045\n",
      "Training epoch 9445 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9445 ; accuracy: 0.7433333333333333; loss: 2.2229387760162354\n",
      "Training epoch 9446 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9446 ; accuracy: 0.7433333333333333; loss: 2.2229483127593994\n",
      "Training epoch 9447 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9447 ; accuracy: 0.7433333333333333; loss: 2.2229678630828857\n",
      "Training epoch 9448 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9448 ; accuracy: 0.7433333333333333; loss: 2.222987174987793\n",
      "Training epoch 9449 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9449 ; accuracy: 0.7433333333333333; loss: 2.2230064868927\n",
      "Training epoch 9450 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9450 ; accuracy: 0.7433333333333333; loss: 2.2230169773101807\n",
      "Training epoch 9451 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9451 ; accuracy: 0.7433333333333333; loss: 2.2230453491210938\n",
      "Training epoch 9452 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9452 ; accuracy: 0.7433333333333333; loss: 2.2230889797210693\n",
      "Training epoch 9453 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9453 ; accuracy: 0.7433333333333333; loss: 2.2231481075286865\n",
      "Training epoch 9454 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9454 ; accuracy: 0.7433333333333333; loss: 2.223205804824829\n",
      "Training epoch 9455 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9455 ; accuracy: 0.7433333333333333; loss: 2.2232608795166016\n",
      "Training epoch 9456 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9456 ; accuracy: 0.7433333333333333; loss: 2.223308801651001\n",
      "Training epoch 9457 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9457 ; accuracy: 0.7433333333333333; loss: 2.22336483001709\n",
      "Training epoch 9458 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9458 ; accuracy: 0.7433333333333333; loss: 2.2234175205230713\n",
      "Training epoch 9459 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9459 ; accuracy: 0.7433333333333333; loss: 2.2234866619110107\n",
      "Training epoch 9460 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9460 ; accuracy: 0.7433333333333333; loss: 2.2235400676727295\n",
      "Training epoch 9461 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9461 ; accuracy: 0.7433333333333333; loss: 2.223573684692383\n",
      "Training epoch 9462 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9462 ; accuracy: 0.7433333333333333; loss: 2.223613977432251\n",
      "Training epoch 9463 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9463 ; accuracy: 0.7433333333333333; loss: 2.223672389984131\n",
      "Training epoch 9464 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9464 ; accuracy: 0.7433333333333333; loss: 2.2237627506256104\n",
      "Training epoch 9465 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9465 ; accuracy: 0.7433333333333333; loss: 2.2238433361053467\n",
      "Training epoch 9466 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9466 ; accuracy: 0.7433333333333333; loss: 2.2239162921905518\n",
      "Training epoch 9467 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9467 ; accuracy: 0.7433333333333333; loss: 2.223989725112915\n",
      "Training epoch 9468 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9468 ; accuracy: 0.7433333333333333; loss: 2.2240731716156006\n",
      "Training epoch 9469 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9469 ; accuracy: 0.7433333333333333; loss: 2.224172830581665\n",
      "Training epoch 9470 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9470 ; accuracy: 0.7433333333333333; loss: 2.2242650985717773\n",
      "Training epoch 9471 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9471 ; accuracy: 0.7433333333333333; loss: 2.224351167678833\n",
      "Training epoch 9472 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9472 ; accuracy: 0.7433333333333333; loss: 2.224404811859131\n",
      "Training epoch 9473 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9473 ; accuracy: 0.7433333333333333; loss: 2.224458694458008\n",
      "Training epoch 9474 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9474 ; accuracy: 0.7433333333333333; loss: 2.2245101928710938\n",
      "Training epoch 9475 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9475 ; accuracy: 0.7433333333333333; loss: 2.2245631217956543\n",
      "Training epoch 9476 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9476 ; accuracy: 0.7433333333333333; loss: 2.2246198654174805\n",
      "Training epoch 9477 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9477 ; accuracy: 0.7433333333333333; loss: 2.2246718406677246\n",
      "Training epoch 9478 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9478 ; accuracy: 0.7433333333333333; loss: 2.224712610244751\n",
      "Training epoch 9479 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9479 ; accuracy: 0.7433333333333333; loss: 2.2247695922851562\n",
      "Training epoch 9480 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9480 ; accuracy: 0.7433333333333333; loss: 2.2248361110687256\n",
      "Training epoch 9481 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9481 ; accuracy: 0.7433333333333333; loss: 2.224855661392212\n",
      "Training epoch 9482 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9482 ; accuracy: 0.7433333333333333; loss: 2.2248454093933105\n",
      "Training epoch 9483 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9483 ; accuracy: 0.7433333333333333; loss: 2.2248473167419434\n",
      "Training epoch 9484 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9484 ; accuracy: 0.7433333333333333; loss: 2.224871873855591\n",
      "Training epoch 9485 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9485 ; accuracy: 0.7433333333333333; loss: 2.224897861480713\n",
      "Training epoch 9486 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9486 ; accuracy: 0.7433333333333333; loss: 2.224926471710205\n",
      "Training epoch 9487 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9487 ; accuracy: 0.7433333333333333; loss: 2.2249739170074463\n",
      "Training epoch 9488 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9488 ; accuracy: 0.7433333333333333; loss: 2.225048303604126\n",
      "Training epoch 9489 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9489 ; accuracy: 0.7433333333333333; loss: 2.22514271736145\n",
      "Training epoch 9490 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9490 ; accuracy: 0.7433333333333333; loss: 2.225217580795288\n",
      "Training epoch 9491 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9491 ; accuracy: 0.7433333333333333; loss: 2.2252869606018066\n",
      "Training epoch 9492 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9492 ; accuracy: 0.7433333333333333; loss: 2.225360870361328\n",
      "Training epoch 9493 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9493 ; accuracy: 0.7433333333333333; loss: 2.225423574447632\n",
      "Training epoch 9494 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9494 ; accuracy: 0.7433333333333333; loss: 2.2254981994628906\n",
      "Training epoch 9495 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9495 ; accuracy: 0.7433333333333333; loss: 2.225560426712036\n",
      "Training epoch 9496 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9496 ; accuracy: 0.7433333333333333; loss: 2.2256016731262207\n",
      "Training epoch 9497 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9497 ; accuracy: 0.7433333333333333; loss: 2.2256510257720947\n",
      "Training epoch 9498 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9498 ; accuracy: 0.7433333333333333; loss: 2.2256932258605957\n",
      "Training epoch 9499 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9499 ; accuracy: 0.7433333333333333; loss: 2.2257487773895264\n",
      "Training epoch 9500 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9500 ; accuracy: 0.7433333333333333; loss: 2.225799322128296\n",
      "Training epoch 9501 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9501 ; accuracy: 0.7433333333333333; loss: 2.225850820541382\n",
      "Training epoch 9502 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9502 ; accuracy: 0.7433333333333333; loss: 2.225924253463745\n",
      "Training epoch 9503 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9503 ; accuracy: 0.7433333333333333; loss: 2.22598934173584\n",
      "Training epoch 9504 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9504 ; accuracy: 0.7433333333333333; loss: 2.2260425090789795\n",
      "Training epoch 9505 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9505 ; accuracy: 0.7433333333333333; loss: 2.2260963916778564\n",
      "Training epoch 9506 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9506 ; accuracy: 0.7433333333333333; loss: 2.2261552810668945\n",
      "Training epoch 9507 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9507 ; accuracy: 0.7433333333333333; loss: 2.226203680038452\n",
      "Training epoch 9508 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9508 ; accuracy: 0.7433333333333333; loss: 2.2262446880340576\n",
      "Training epoch 9509 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9509 ; accuracy: 0.7433333333333333; loss: 2.226292610168457\n",
      "Training epoch 9510 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9510 ; accuracy: 0.7433333333333333; loss: 2.2263376712799072\n",
      "Training epoch 9511 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9511 ; accuracy: 0.7433333333333333; loss: 2.2263901233673096\n",
      "Training epoch 9512 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9512 ; accuracy: 0.7433333333333333; loss: 2.2264678478240967\n",
      "Training epoch 9513 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9513 ; accuracy: 0.7433333333333333; loss: 2.2265477180480957\n",
      "Training epoch 9514 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9514 ; accuracy: 0.7433333333333333; loss: 2.2265942096710205\n",
      "Training epoch 9515 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9515 ; accuracy: 0.7433333333333333; loss: 2.2266345024108887\n",
      "Training epoch 9516 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9516 ; accuracy: 0.7433333333333333; loss: 2.2266716957092285\n",
      "Training epoch 9517 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9517 ; accuracy: 0.7433333333333333; loss: 2.2267005443573\n",
      "Training epoch 9518 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9518 ; accuracy: 0.7433333333333333; loss: 2.2267391681671143\n",
      "Training epoch 9519 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9519 ; accuracy: 0.7433333333333333; loss: 2.226794958114624\n",
      "Training epoch 9520 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9520 ; accuracy: 0.7433333333333333; loss: 2.2268619537353516\n",
      "Training epoch 9521 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9521 ; accuracy: 0.7433333333333333; loss: 2.226956844329834\n",
      "Training epoch 9522 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9522 ; accuracy: 0.7433333333333333; loss: 2.2270774841308594\n",
      "Training epoch 9523 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9523 ; accuracy: 0.7433333333333333; loss: 2.2271974086761475\n",
      "Training epoch 9524 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9524 ; accuracy: 0.7433333333333333; loss: 2.2273268699645996\n",
      "Training epoch 9525 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9525 ; accuracy: 0.7433333333333333; loss: 2.227450132369995\n",
      "Training epoch 9526 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9526 ; accuracy: 0.7433333333333333; loss: 2.227562665939331\n",
      "Training epoch 9527 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9527 ; accuracy: 0.7433333333333333; loss: 2.2276644706726074\n",
      "Training epoch 9528 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9528 ; accuracy: 0.7433333333333333; loss: 2.2277541160583496\n",
      "Training epoch 9529 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9529 ; accuracy: 0.7433333333333333; loss: 2.2278311252593994\n",
      "Training epoch 9530 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9530 ; accuracy: 0.7433333333333333; loss: 2.2279067039489746\n",
      "Training epoch 9531 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9531 ; accuracy: 0.7433333333333333; loss: 2.227969169616699\n",
      "Training epoch 9532 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9532 ; accuracy: 0.7433333333333333; loss: 2.228029489517212\n",
      "Training epoch 9533 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9533 ; accuracy: 0.7433333333333333; loss: 2.228081703186035\n",
      "Training epoch 9534 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9534 ; accuracy: 0.7433333333333333; loss: 2.228151559829712\n",
      "Training epoch 9535 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9535 ; accuracy: 0.7433333333333333; loss: 2.228217124938965\n",
      "Training epoch 9536 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9536 ; accuracy: 0.7433333333333333; loss: 2.228271722793579\n",
      "Training epoch 9537 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9537 ; accuracy: 0.7433333333333333; loss: 2.228304624557495\n",
      "Training epoch 9538 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9538 ; accuracy: 0.7433333333333333; loss: 2.228335380554199\n",
      "Training epoch 9539 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9539 ; accuracy: 0.7433333333333333; loss: 2.228360891342163\n",
      "Training epoch 9540 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9540 ; accuracy: 0.7433333333333333; loss: 2.228381872177124\n",
      "Training epoch 9541 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9541 ; accuracy: 0.7433333333333333; loss: 2.228409767150879\n",
      "Training epoch 9542 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9542 ; accuracy: 0.7433333333333333; loss: 2.2284750938415527\n",
      "Training epoch 9543 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9543 ; accuracy: 0.7433333333333333; loss: 2.2285454273223877\n",
      "Training epoch 9544 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9544 ; accuracy: 0.7433333333333333; loss: 2.2286314964294434\n",
      "Training epoch 9545 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9545 ; accuracy: 0.7433333333333333; loss: 2.228715658187866\n",
      "Training epoch 9546 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9546 ; accuracy: 0.7433333333333333; loss: 2.2287838459014893\n",
      "Training epoch 9547 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9547 ; accuracy: 0.7433333333333333; loss: 2.2288506031036377\n",
      "Training epoch 9548 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9548 ; accuracy: 0.7433333333333333; loss: 2.228912830352783\n",
      "Training epoch 9549 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9549 ; accuracy: 0.7433333333333333; loss: 2.228982448577881\n",
      "Training epoch 9550 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9550 ; accuracy: 0.7433333333333333; loss: 2.229050397872925\n",
      "Training epoch 9551 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9551 ; accuracy: 0.7433333333333333; loss: 2.229121685028076\n",
      "Training epoch 9552 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9552 ; accuracy: 0.7433333333333333; loss: 2.229198694229126\n",
      "Training epoch 9553 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9553 ; accuracy: 0.7433333333333333; loss: 2.229276657104492\n",
      "Training epoch 9554 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9554 ; accuracy: 0.7433333333333333; loss: 2.2293777465820312\n",
      "Training epoch 9555 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9555 ; accuracy: 0.7433333333333333; loss: 2.229485273361206\n",
      "Training epoch 9556 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9556 ; accuracy: 0.7433333333333333; loss: 2.229567766189575\n",
      "Training epoch 9557 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9557 ; accuracy: 0.7433333333333333; loss: 2.229649305343628\n",
      "Training epoch 9558 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9558 ; accuracy: 0.7433333333333333; loss: 2.2297234535217285\n",
      "Training epoch 9559 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9559 ; accuracy: 0.7433333333333333; loss: 2.229804515838623\n",
      "Training epoch 9560 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9560 ; accuracy: 0.7433333333333333; loss: 2.2298877239227295\n",
      "Training epoch 9561 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9561 ; accuracy: 0.7433333333333333; loss: 2.2299585342407227\n",
      "Training epoch 9562 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9562 ; accuracy: 0.7433333333333333; loss: 2.230018138885498\n",
      "Training epoch 9563 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9563 ; accuracy: 0.7433333333333333; loss: 2.2300806045532227\n",
      "Training epoch 9564 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9564 ; accuracy: 0.7433333333333333; loss: 2.2301275730133057\n",
      "Training epoch 9565 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9565 ; accuracy: 0.7433333333333333; loss: 2.23018479347229\n",
      "Training epoch 9566 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9566 ; accuracy: 0.7433333333333333; loss: 2.2302420139312744\n",
      "Training epoch 9567 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9567 ; accuracy: 0.7433333333333333; loss: 2.230297088623047\n",
      "Training epoch 9568 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9568 ; accuracy: 0.7433333333333333; loss: 2.2303576469421387\n",
      "Training epoch 9569 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9569 ; accuracy: 0.7433333333333333; loss: 2.2304136753082275\n",
      "Training epoch 9570 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9570 ; accuracy: 0.7433333333333333; loss: 2.230461359024048\n",
      "Training epoch 9571 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9571 ; accuracy: 0.7433333333333333; loss: 2.2305009365081787\n",
      "Training epoch 9572 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9572 ; accuracy: 0.7433333333333333; loss: 2.2303240299224854\n",
      "Training epoch 9573 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9573 ; accuracy: 0.7433333333333333; loss: 2.230168104171753\n",
      "Training epoch 9574 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9574 ; accuracy: 0.7433333333333333; loss: 2.2299892902374268\n",
      "Training epoch 9575 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9575 ; accuracy: 0.7433333333333333; loss: 2.229818820953369\n",
      "Training epoch 9576 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9576 ; accuracy: 0.7433333333333333; loss: 2.2296652793884277\n",
      "Training epoch 9577 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9577 ; accuracy: 0.7433333333333333; loss: 2.2295308113098145\n",
      "Training epoch 9578 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9578 ; accuracy: 0.7433333333333333; loss: 2.2294137477874756\n",
      "Training epoch 9579 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9579 ; accuracy: 0.7433333333333333; loss: 2.2293214797973633\n",
      "Training epoch 9580 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9580 ; accuracy: 0.7433333333333333; loss: 2.2292487621307373\n",
      "Training epoch 9581 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9581 ; accuracy: 0.7433333333333333; loss: 2.2291958332061768\n",
      "Training epoch 9582 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9582 ; accuracy: 0.7433333333333333; loss: 2.2291643619537354\n",
      "Training epoch 9583 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9583 ; accuracy: 0.7433333333333333; loss: 2.229135513305664\n",
      "Training epoch 9584 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9584 ; accuracy: 0.7433333333333333; loss: 2.2291128635406494\n",
      "Training epoch 9585 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9585 ; accuracy: 0.7433333333333333; loss: 2.2290945053100586\n",
      "Training epoch 9586 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9586 ; accuracy: 0.7433333333333333; loss: 2.2290618419647217\n",
      "Training epoch 9587 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9587 ; accuracy: 0.7433333333333333; loss: 2.2290396690368652\n",
      "Training epoch 9588 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9588 ; accuracy: 0.7433333333333333; loss: 2.2290399074554443\n",
      "Training epoch 9589 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9589 ; accuracy: 0.7433333333333333; loss: 2.229055404663086\n",
      "Training epoch 9590 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9590 ; accuracy: 0.7433333333333333; loss: 2.2290713787078857\n",
      "Training epoch 9591 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9591 ; accuracy: 0.7433333333333333; loss: 2.229081630706787\n",
      "Training epoch 9592 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9592 ; accuracy: 0.7433333333333333; loss: 2.229111671447754\n",
      "Training epoch 9593 ; accuracy: 0.9; loss: 0.19459104537963867\n",
      "Validation epoch 9593 ; accuracy: 0.7433333333333333; loss: 2.229191541671753\n",
      "Training epoch 9594 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9594 ; accuracy: 0.7433333333333333; loss: 2.2292754650115967\n",
      "Training epoch 9595 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9595 ; accuracy: 0.7433333333333333; loss: 2.229367971420288\n",
      "Training epoch 9596 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9596 ; accuracy: 0.7433333333333333; loss: 2.229464054107666\n",
      "Training epoch 9597 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9597 ; accuracy: 0.7433333333333333; loss: 2.2295594215393066\n",
      "Training epoch 9598 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9598 ; accuracy: 0.7433333333333333; loss: 2.2296366691589355\n",
      "Training epoch 9599 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9599 ; accuracy: 0.7433333333333333; loss: 2.2296996116638184\n",
      "Training epoch 9600 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9600 ; accuracy: 0.7433333333333333; loss: 2.229753255844116\n",
      "Training epoch 9601 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9601 ; accuracy: 0.7433333333333333; loss: 2.2298710346221924\n",
      "Training epoch 9602 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9602 ; accuracy: 0.7433333333333333; loss: 2.229973077774048\n",
      "Training epoch 9603 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9603 ; accuracy: 0.7433333333333333; loss: 2.2300782203674316\n",
      "Training epoch 9604 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9604 ; accuracy: 0.7433333333333333; loss: 2.2301740646362305\n",
      "Training epoch 9605 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9605 ; accuracy: 0.7433333333333333; loss: 2.230257749557495\n",
      "Training epoch 9606 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9606 ; accuracy: 0.7433333333333333; loss: 2.2303378582000732\n",
      "Training epoch 9607 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9607 ; accuracy: 0.7433333333333333; loss: 2.230412721633911\n",
      "Training epoch 9608 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9608 ; accuracy: 0.7433333333333333; loss: 2.2304937839508057\n",
      "Training epoch 9609 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9609 ; accuracy: 0.7433333333333333; loss: 2.2305526733398438\n",
      "Training epoch 9610 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9610 ; accuracy: 0.7433333333333333; loss: 2.2306180000305176\n",
      "Training epoch 9611 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9611 ; accuracy: 0.7433333333333333; loss: 2.230698585510254\n",
      "Training epoch 9612 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9612 ; accuracy: 0.7433333333333333; loss: 2.2307629585266113\n",
      "Training epoch 9613 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9613 ; accuracy: 0.7433333333333333; loss: 2.2308194637298584\n",
      "Training epoch 9614 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9614 ; accuracy: 0.7433333333333333; loss: 2.2308528423309326\n",
      "Training epoch 9615 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9615 ; accuracy: 0.7433333333333333; loss: 2.230889320373535\n",
      "Training epoch 9616 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9616 ; accuracy: 0.7433333333333333; loss: 2.2309138774871826\n",
      "Training epoch 9617 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9617 ; accuracy: 0.7433333333333333; loss: 2.2309420108795166\n",
      "Training epoch 9618 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9618 ; accuracy: 0.7433333333333333; loss: 2.2309763431549072\n",
      "Training epoch 9619 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9619 ; accuracy: 0.7433333333333333; loss: 2.231017827987671\n",
      "Training epoch 9620 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9620 ; accuracy: 0.7433333333333333; loss: 2.231053352355957\n",
      "Training epoch 9621 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9621 ; accuracy: 0.7433333333333333; loss: 2.2310895919799805\n",
      "Training epoch 9622 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9622 ; accuracy: 0.7433333333333333; loss: 2.231118679046631\n",
      "Training epoch 9623 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9623 ; accuracy: 0.7433333333333333; loss: 2.2311389446258545\n",
      "Training epoch 9624 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9624 ; accuracy: 0.7433333333333333; loss: 2.231156587600708\n",
      "Training epoch 9625 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9625 ; accuracy: 0.7433333333333333; loss: 2.2311837673187256\n",
      "Training epoch 9626 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9626 ; accuracy: 0.7433333333333333; loss: 2.2312073707580566\n",
      "Training epoch 9627 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9627 ; accuracy: 0.7433333333333333; loss: 2.231203317642212\n",
      "Training epoch 9628 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9628 ; accuracy: 0.7433333333333333; loss: 2.231210231781006\n",
      "Training epoch 9629 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9629 ; accuracy: 0.7433333333333333; loss: 2.23122239112854\n",
      "Training epoch 9630 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9630 ; accuracy: 0.7433333333333333; loss: 2.231222629547119\n",
      "Training epoch 9631 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9631 ; accuracy: 0.7433333333333333; loss: 2.231224536895752\n",
      "Training epoch 9632 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9632 ; accuracy: 0.7433333333333333; loss: 2.231227159500122\n",
      "Training epoch 9633 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9633 ; accuracy: 0.7433333333333333; loss: 2.2312252521514893\n",
      "Training epoch 9634 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9634 ; accuracy: 0.7433333333333333; loss: 2.2312371730804443\n",
      "Training epoch 9635 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9635 ; accuracy: 0.7433333333333333; loss: 2.23126482963562\n",
      "Training epoch 9636 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9636 ; accuracy: 0.7433333333333333; loss: 2.2312920093536377\n",
      "Training epoch 9637 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9637 ; accuracy: 0.7433333333333333; loss: 2.231288194656372\n",
      "Training epoch 9638 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9638 ; accuracy: 0.7433333333333333; loss: 2.231302499771118\n",
      "Training epoch 9639 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9639 ; accuracy: 0.7433333333333333; loss: 2.231316089630127\n",
      "Training epoch 9640 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9640 ; accuracy: 0.7433333333333333; loss: 2.2313268184661865\n",
      "Training epoch 9641 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9641 ; accuracy: 0.7433333333333333; loss: 2.231342077255249\n",
      "Training epoch 9642 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9642 ; accuracy: 0.7433333333333333; loss: 2.231362819671631\n",
      "Training epoch 9643 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9643 ; accuracy: 0.7433333333333333; loss: 2.2313880920410156\n",
      "Training epoch 9644 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9644 ; accuracy: 0.7433333333333333; loss: 2.2314112186431885\n",
      "Training epoch 9645 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9645 ; accuracy: 0.7433333333333333; loss: 2.231443166732788\n",
      "Training epoch 9646 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9646 ; accuracy: 0.7433333333333333; loss: 2.231459856033325\n",
      "Training epoch 9647 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9647 ; accuracy: 0.7433333333333333; loss: 2.231487989425659\n",
      "Training epoch 9648 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9648 ; accuracy: 0.7433333333333333; loss: 2.231513738632202\n",
      "Training epoch 9649 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9649 ; accuracy: 0.7433333333333333; loss: 2.2315561771392822\n",
      "Training epoch 9650 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9650 ; accuracy: 0.7433333333333333; loss: 2.231597900390625\n",
      "Training epoch 9651 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9651 ; accuracy: 0.7433333333333333; loss: 2.231642007827759\n",
      "Training epoch 9652 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9652 ; accuracy: 0.7433333333333333; loss: 2.2316572666168213\n",
      "Training epoch 9653 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9653 ; accuracy: 0.7433333333333333; loss: 2.2316651344299316\n",
      "Training epoch 9654 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9654 ; accuracy: 0.7433333333333333; loss: 2.231680393218994\n",
      "Training epoch 9655 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9655 ; accuracy: 0.7433333333333333; loss: 2.231691837310791\n",
      "Training epoch 9656 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9656 ; accuracy: 0.7433333333333333; loss: 2.2317068576812744\n",
      "Training epoch 9657 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9657 ; accuracy: 0.7433333333333333; loss: 2.231724739074707\n",
      "Training epoch 9658 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9658 ; accuracy: 0.7433333333333333; loss: 2.2317583560943604\n",
      "Training epoch 9659 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9659 ; accuracy: 0.7433333333333333; loss: 2.2317955493927\n",
      "Training epoch 9660 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9660 ; accuracy: 0.7433333333333333; loss: 2.231842279434204\n",
      "Training epoch 9661 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9661 ; accuracy: 0.7433333333333333; loss: 2.2318782806396484\n",
      "Training epoch 9662 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9662 ; accuracy: 0.7433333333333333; loss: 2.2319297790527344\n",
      "Training epoch 9663 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9663 ; accuracy: 0.7433333333333333; loss: 2.231996774673462\n",
      "Training epoch 9664 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9664 ; accuracy: 0.7433333333333333; loss: 2.2320587635040283\n",
      "Training epoch 9665 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9665 ; accuracy: 0.7433333333333333; loss: 2.2321174144744873\n",
      "Training epoch 9666 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9666 ; accuracy: 0.7433333333333333; loss: 2.232172966003418\n",
      "Training epoch 9667 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9667 ; accuracy: 0.7433333333333333; loss: 2.232219934463501\n",
      "Training epoch 9668 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9668 ; accuracy: 0.7433333333333333; loss: 2.2322351932525635\n",
      "Training epoch 9669 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9669 ; accuracy: 0.7433333333333333; loss: 2.232248544692993\n",
      "Training epoch 9670 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9670 ; accuracy: 0.7433333333333333; loss: 2.2322723865509033\n",
      "Training epoch 9671 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9671 ; accuracy: 0.7433333333333333; loss: 2.232304573059082\n",
      "Training epoch 9672 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9672 ; accuracy: 0.7433333333333333; loss: 2.2323005199432373\n",
      "Training epoch 9673 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9673 ; accuracy: 0.7433333333333333; loss: 2.2323131561279297\n",
      "Training epoch 9674 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9674 ; accuracy: 0.7433333333333333; loss: 2.2323319911956787\n",
      "Training epoch 9675 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9675 ; accuracy: 0.7433333333333333; loss: 2.2323546409606934\n",
      "Training epoch 9676 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9676 ; accuracy: 0.7433333333333333; loss: 2.2323758602142334\n",
      "Training epoch 9677 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9677 ; accuracy: 0.7433333333333333; loss: 2.2323644161224365\n",
      "Training epoch 9678 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9678 ; accuracy: 0.7433333333333333; loss: 2.2323670387268066\n",
      "Training epoch 9679 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9679 ; accuracy: 0.7433333333333333; loss: 2.2323765754699707\n",
      "Training epoch 9680 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9680 ; accuracy: 0.7433333333333333; loss: 2.2323954105377197\n",
      "Training epoch 9681 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9681 ; accuracy: 0.7433333333333333; loss: 2.2323930263519287\n",
      "Training epoch 9682 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9682 ; accuracy: 0.7433333333333333; loss: 2.2324023246765137\n",
      "Training epoch 9683 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9683 ; accuracy: 0.7433333333333333; loss: 2.232402801513672\n",
      "Training epoch 9684 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9684 ; accuracy: 0.7433333333333333; loss: 2.232419013977051\n",
      "Training epoch 9685 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9685 ; accuracy: 0.7433333333333333; loss: 2.2324323654174805\n",
      "Training epoch 9686 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9686 ; accuracy: 0.7433333333333333; loss: 2.232448101043701\n",
      "Training epoch 9687 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9687 ; accuracy: 0.7433333333333333; loss: 2.232487916946411\n",
      "Training epoch 9688 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9688 ; accuracy: 0.7433333333333333; loss: 2.232520818710327\n",
      "Training epoch 9689 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9689 ; accuracy: 0.7433333333333333; loss: 2.232553720474243\n",
      "Training epoch 9690 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9690 ; accuracy: 0.7433333333333333; loss: 2.232623338699341\n",
      "Training epoch 9691 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9691 ; accuracy: 0.7433333333333333; loss: 2.2326862812042236\n",
      "Training epoch 9692 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9692 ; accuracy: 0.7433333333333333; loss: 2.2327489852905273\n",
      "Training epoch 9693 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9693 ; accuracy: 0.7433333333333333; loss: 2.232827663421631\n",
      "Training epoch 9694 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9694 ; accuracy: 0.7433333333333333; loss: 2.2329139709472656\n",
      "Training epoch 9695 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9695 ; accuracy: 0.7433333333333333; loss: 2.23301100730896\n",
      "Training epoch 9696 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9696 ; accuracy: 0.7433333333333333; loss: 2.233102798461914\n",
      "Training epoch 9697 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9697 ; accuracy: 0.7433333333333333; loss: 2.2331783771514893\n",
      "Training epoch 9698 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9698 ; accuracy: 0.7433333333333333; loss: 2.2332520484924316\n",
      "Training epoch 9699 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9699 ; accuracy: 0.7433333333333333; loss: 2.2333216667175293\n",
      "Training epoch 9700 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9700 ; accuracy: 0.7433333333333333; loss: 2.2333645820617676\n",
      "Training epoch 9701 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9701 ; accuracy: 0.7433333333333333; loss: 2.2334089279174805\n",
      "Training epoch 9702 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9702 ; accuracy: 0.7433333333333333; loss: 2.2334558963775635\n",
      "Training epoch 9703 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9703 ; accuracy: 0.7433333333333333; loss: 2.233494758605957\n",
      "Training epoch 9704 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9704 ; accuracy: 0.7433333333333333; loss: 2.2335381507873535\n",
      "Training epoch 9705 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9705 ; accuracy: 0.7433333333333333; loss: 2.2335550785064697\n",
      "Training epoch 9706 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9706 ; accuracy: 0.7433333333333333; loss: 2.233583688735962\n",
      "Training epoch 9707 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9707 ; accuracy: 0.7433333333333333; loss: 2.2336015701293945\n",
      "Training epoch 9708 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9708 ; accuracy: 0.7433333333333333; loss: 2.2336297035217285\n",
      "Training epoch 9709 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9709 ; accuracy: 0.7433333333333333; loss: 2.2336626052856445\n",
      "Training epoch 9710 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9710 ; accuracy: 0.7433333333333333; loss: 2.2336928844451904\n",
      "Training epoch 9711 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9711 ; accuracy: 0.7433333333333333; loss: 2.233738422393799\n",
      "Training epoch 9712 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9712 ; accuracy: 0.7433333333333333; loss: 2.233781099319458\n",
      "Training epoch 9713 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9713 ; accuracy: 0.7433333333333333; loss: 2.233840227127075\n",
      "Training epoch 9714 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9714 ; accuracy: 0.7433333333333333; loss: 2.233898639678955\n",
      "Training epoch 9715 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9715 ; accuracy: 0.7433333333333333; loss: 2.233952522277832\n",
      "Training epoch 9716 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9716 ; accuracy: 0.7433333333333333; loss: 2.234020233154297\n",
      "Training epoch 9717 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9717 ; accuracy: 0.7433333333333333; loss: 2.2340919971466064\n",
      "Training epoch 9718 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9718 ; accuracy: 0.7433333333333333; loss: 2.234178066253662\n",
      "Training epoch 9719 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9719 ; accuracy: 0.7433333333333333; loss: 2.2342517375946045\n",
      "Training epoch 9720 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9720 ; accuracy: 0.7433333333333333; loss: 2.234330177307129\n",
      "Training epoch 9721 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9721 ; accuracy: 0.7433333333333333; loss: 2.2344119548797607\n",
      "Training epoch 9722 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9722 ; accuracy: 0.7433333333333333; loss: 2.2344822883605957\n",
      "Training epoch 9723 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9723 ; accuracy: 0.7433333333333333; loss: 2.234553337097168\n",
      "Training epoch 9724 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9724 ; accuracy: 0.7433333333333333; loss: 2.2346203327178955\n",
      "Training epoch 9725 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9725 ; accuracy: 0.7433333333333333; loss: 2.2346994876861572\n",
      "Training epoch 9726 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9726 ; accuracy: 0.7433333333333333; loss: 2.2347843647003174\n",
      "Training epoch 9727 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9727 ; accuracy: 0.7433333333333333; loss: 2.2348709106445312\n",
      "Training epoch 9728 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9728 ; accuracy: 0.7433333333333333; loss: 2.2349627017974854\n",
      "Training epoch 9729 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9729 ; accuracy: 0.7433333333333333; loss: 2.235048294067383\n",
      "Training epoch 9730 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9730 ; accuracy: 0.7433333333333333; loss: 2.235117197036743\n",
      "Training epoch 9731 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9731 ; accuracy: 0.7433333333333333; loss: 2.2352514266967773\n",
      "Training epoch 9732 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9732 ; accuracy: 0.7433333333333333; loss: 2.2353813648223877\n",
      "Training epoch 9733 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9733 ; accuracy: 0.7433333333333333; loss: 2.2355072498321533\n",
      "Training epoch 9734 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9734 ; accuracy: 0.7433333333333333; loss: 2.2356245517730713\n",
      "Training epoch 9735 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9735 ; accuracy: 0.7433333333333333; loss: 2.2357399463653564\n",
      "Training epoch 9736 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9736 ; accuracy: 0.7433333333333333; loss: 2.2358498573303223\n",
      "Training epoch 9737 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9737 ; accuracy: 0.7433333333333333; loss: 2.2359390258789062\n",
      "Training epoch 9738 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9738 ; accuracy: 0.7433333333333333; loss: 2.2359981536865234\n",
      "Training epoch 9739 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9739 ; accuracy: 0.7433333333333333; loss: 2.2360599040985107\n",
      "Training epoch 9740 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9740 ; accuracy: 0.7433333333333333; loss: 2.2361223697662354\n",
      "Training epoch 9741 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9741 ; accuracy: 0.7433333333333333; loss: 2.2361834049224854\n",
      "Training epoch 9742 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9742 ; accuracy: 0.7433333333333333; loss: 2.2362430095672607\n",
      "Training epoch 9743 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9743 ; accuracy: 0.7433333333333333; loss: 2.236311912536621\n",
      "Training epoch 9744 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9744 ; accuracy: 0.7433333333333333; loss: 2.2363483905792236\n",
      "Training epoch 9745 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9745 ; accuracy: 0.7433333333333333; loss: 2.236375093460083\n",
      "Training epoch 9746 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9746 ; accuracy: 0.7433333333333333; loss: 2.2363953590393066\n",
      "Training epoch 9747 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9747 ; accuracy: 0.7433333333333333; loss: 2.2364065647125244\n",
      "Training epoch 9748 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9748 ; accuracy: 0.7433333333333333; loss: 2.2364161014556885\n",
      "Training epoch 9749 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9749 ; accuracy: 0.7433333333333333; loss: 2.236433506011963\n",
      "Training epoch 9750 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9750 ; accuracy: 0.7433333333333333; loss: 2.236440896987915\n",
      "Training epoch 9751 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9751 ; accuracy: 0.7433333333333333; loss: 2.2364842891693115\n",
      "Training epoch 9752 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9752 ; accuracy: 0.7433333333333333; loss: 2.2365236282348633\n",
      "Training epoch 9753 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9753 ; accuracy: 0.7433333333333333; loss: 2.2365615367889404\n",
      "Training epoch 9754 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9754 ; accuracy: 0.7433333333333333; loss: 2.2365922927856445\n",
      "Training epoch 9755 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9755 ; accuracy: 0.7433333333333333; loss: 2.236626148223877\n",
      "Training epoch 9756 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9756 ; accuracy: 0.7433333333333333; loss: 2.236661434173584\n",
      "Training epoch 9757 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9757 ; accuracy: 0.7433333333333333; loss: 2.2367184162139893\n",
      "Training epoch 9758 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9758 ; accuracy: 0.7433333333333333; loss: 2.23677921295166\n",
      "Training epoch 9759 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9759 ; accuracy: 0.7433333333333333; loss: 2.2368338108062744\n",
      "Training epoch 9760 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9760 ; accuracy: 0.7433333333333333; loss: 2.236877679824829\n",
      "Training epoch 9761 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9761 ; accuracy: 0.7433333333333333; loss: 2.236926794052124\n",
      "Training epoch 9762 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9762 ; accuracy: 0.7433333333333333; loss: 2.236968755722046\n",
      "Training epoch 9763 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9763 ; accuracy: 0.7433333333333333; loss: 2.2370119094848633\n",
      "Training epoch 9764 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9764 ; accuracy: 0.7433333333333333; loss: 2.2370822429656982\n",
      "Training epoch 9765 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9765 ; accuracy: 0.7433333333333333; loss: 2.237138509750366\n",
      "Training epoch 9766 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9766 ; accuracy: 0.7433333333333333; loss: 2.237178325653076\n",
      "Training epoch 9767 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9767 ; accuracy: 0.7433333333333333; loss: 2.237220525741577\n",
      "Training epoch 9768 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9768 ; accuracy: 0.7433333333333333; loss: 2.237252712249756\n",
      "Training epoch 9769 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9769 ; accuracy: 0.7433333333333333; loss: 2.237290143966675\n",
      "Training epoch 9770 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9770 ; accuracy: 0.7433333333333333; loss: 2.2373385429382324\n",
      "Training epoch 9771 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9771 ; accuracy: 0.7433333333333333; loss: 2.23738431930542\n",
      "Training epoch 9772 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9772 ; accuracy: 0.7433333333333333; loss: 2.2374844551086426\n",
      "Training epoch 9773 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9773 ; accuracy: 0.7433333333333333; loss: 2.2376210689544678\n",
      "Training epoch 9774 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9774 ; accuracy: 0.7433333333333333; loss: 2.237750768661499\n",
      "Training epoch 9775 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9775 ; accuracy: 0.7433333333333333; loss: 2.2378697395324707\n",
      "Training epoch 9776 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9776 ; accuracy: 0.7433333333333333; loss: 2.237999200820923\n",
      "Training epoch 9777 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9777 ; accuracy: 0.7433333333333333; loss: 2.238116502761841\n",
      "Training epoch 9778 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9778 ; accuracy: 0.7433333333333333; loss: 2.2382078170776367\n",
      "Training epoch 9779 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9779 ; accuracy: 0.7433333333333333; loss: 2.238284111022949\n",
      "Training epoch 9780 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9780 ; accuracy: 0.7433333333333333; loss: 2.2383549213409424\n",
      "Training epoch 9781 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9781 ; accuracy: 0.7433333333333333; loss: 2.238429069519043\n",
      "Training epoch 9782 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9782 ; accuracy: 0.7433333333333333; loss: 2.238492250442505\n",
      "Training epoch 9783 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9783 ; accuracy: 0.7433333333333333; loss: 2.238555908203125\n",
      "Training epoch 9784 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9784 ; accuracy: 0.7433333333333333; loss: 2.238607406616211\n",
      "Training epoch 9785 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9785 ; accuracy: 0.7433333333333333; loss: 2.238646984100342\n",
      "Training epoch 9786 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9786 ; accuracy: 0.7433333333333333; loss: 2.2386817932128906\n",
      "Training epoch 9787 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9787 ; accuracy: 0.7433333333333333; loss: 2.2387070655822754\n",
      "Training epoch 9788 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9788 ; accuracy: 0.7433333333333333; loss: 2.2387337684631348\n",
      "Training epoch 9789 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9789 ; accuracy: 0.7433333333333333; loss: 2.238752603530884\n",
      "Training epoch 9790 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9790 ; accuracy: 0.7433333333333333; loss: 2.2387681007385254\n",
      "Training epoch 9791 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9791 ; accuracy: 0.7433333333333333; loss: 2.238797187805176\n",
      "Training epoch 9792 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9792 ; accuracy: 0.7433333333333333; loss: 2.2388312816619873\n",
      "Training epoch 9793 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9793 ; accuracy: 0.7433333333333333; loss: 2.238861560821533\n",
      "Training epoch 9794 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9794 ; accuracy: 0.7433333333333333; loss: 2.2389163970947266\n",
      "Training epoch 9795 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9795 ; accuracy: 0.7433333333333333; loss: 2.2389678955078125\n",
      "Training epoch 9796 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9796 ; accuracy: 0.7433333333333333; loss: 2.239023447036743\n",
      "Training epoch 9797 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9797 ; accuracy: 0.7433333333333333; loss: 2.2390713691711426\n",
      "Training epoch 9798 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9798 ; accuracy: 0.7433333333333333; loss: 2.2391016483306885\n",
      "Training epoch 9799 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9799 ; accuracy: 0.7433333333333333; loss: 2.2391176223754883\n",
      "Training epoch 9800 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9800 ; accuracy: 0.7433333333333333; loss: 2.2391281127929688\n",
      "Training epoch 9801 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9801 ; accuracy: 0.7433333333333333; loss: 2.2391445636749268\n",
      "Training epoch 9802 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9802 ; accuracy: 0.7433333333333333; loss: 2.2391622066497803\n",
      "Training epoch 9803 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9803 ; accuracy: 0.7433333333333333; loss: 2.2391884326934814\n",
      "Training epoch 9804 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9804 ; accuracy: 0.7433333333333333; loss: 2.239220142364502\n",
      "Training epoch 9805 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9805 ; accuracy: 0.7433333333333333; loss: 2.2392520904541016\n",
      "Training epoch 9806 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9806 ; accuracy: 0.7433333333333333; loss: 2.239280939102173\n",
      "Training epoch 9807 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9807 ; accuracy: 0.7433333333333333; loss: 2.2393126487731934\n",
      "Training epoch 9808 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9808 ; accuracy: 0.7433333333333333; loss: 2.2393598556518555\n",
      "Training epoch 9809 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9809 ; accuracy: 0.7433333333333333; loss: 2.2394018173217773\n",
      "Training epoch 9810 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9810 ; accuracy: 0.7433333333333333; loss: 2.2394349575042725\n",
      "Training epoch 9811 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9811 ; accuracy: 0.7433333333333333; loss: 2.2394843101501465\n",
      "Training epoch 9812 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9812 ; accuracy: 0.7433333333333333; loss: 2.2395265102386475\n",
      "Training epoch 9813 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9813 ; accuracy: 0.7433333333333333; loss: 2.2395899295806885\n",
      "Training epoch 9814 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9814 ; accuracy: 0.7433333333333333; loss: 2.2396597862243652\n",
      "Training epoch 9815 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9815 ; accuracy: 0.7433333333333333; loss: 2.2397348880767822\n",
      "Training epoch 9816 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9816 ; accuracy: 0.7433333333333333; loss: 2.239804267883301\n",
      "Training epoch 9817 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9817 ; accuracy: 0.7433333333333333; loss: 2.23986554145813\n",
      "Training epoch 9818 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9818 ; accuracy: 0.7433333333333333; loss: 2.239922285079956\n",
      "Training epoch 9819 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9819 ; accuracy: 0.7433333333333333; loss: 2.239975929260254\n",
      "Training epoch 9820 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9820 ; accuracy: 0.7433333333333333; loss: 2.2400214672088623\n",
      "Training epoch 9821 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9821 ; accuracy: 0.7433333333333333; loss: 2.240065574645996\n",
      "Training epoch 9822 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9822 ; accuracy: 0.7433333333333333; loss: 2.2401061058044434\n",
      "Training epoch 9823 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9823 ; accuracy: 0.7433333333333333; loss: 2.2401463985443115\n",
      "Training epoch 9824 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9824 ; accuracy: 0.7433333333333333; loss: 2.2401928901672363\n",
      "Training epoch 9825 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9825 ; accuracy: 0.7433333333333333; loss: 2.24023699760437\n",
      "Training epoch 9826 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9826 ; accuracy: 0.7433333333333333; loss: 2.240274667739868\n",
      "Training epoch 9827 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9827 ; accuracy: 0.7433333333333333; loss: 2.2403295040130615\n",
      "Training epoch 9828 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9828 ; accuracy: 0.7433333333333333; loss: 2.2404000759124756\n",
      "Training epoch 9829 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9829 ; accuracy: 0.7433333333333333; loss: 2.240455389022827\n",
      "Training epoch 9830 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9830 ; accuracy: 0.7433333333333333; loss: 2.2405166625976562\n",
      "Training epoch 9831 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9831 ; accuracy: 0.7433333333333333; loss: 2.240575075149536\n",
      "Training epoch 9832 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9832 ; accuracy: 0.7433333333333333; loss: 2.240671157836914\n",
      "Training epoch 9833 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9833 ; accuracy: 0.7433333333333333; loss: 2.240770101547241\n",
      "Training epoch 9834 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9834 ; accuracy: 0.7433333333333333; loss: 2.240852117538452\n",
      "Training epoch 9835 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9835 ; accuracy: 0.7433333333333333; loss: 2.240926742553711\n",
      "Training epoch 9836 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9836 ; accuracy: 0.7433333333333333; loss: 2.2410120964050293\n",
      "Training epoch 9837 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9837 ; accuracy: 0.7433333333333333; loss: 2.2411038875579834\n",
      "Training epoch 9838 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9838 ; accuracy: 0.7433333333333333; loss: 2.2411890029907227\n",
      "Training epoch 9839 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9839 ; accuracy: 0.7433333333333333; loss: 2.241295099258423\n",
      "Training epoch 9840 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9840 ; accuracy: 0.7433333333333333; loss: 2.241410493850708\n",
      "Training epoch 9841 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9841 ; accuracy: 0.7433333333333333; loss: 2.2415244579315186\n",
      "Training epoch 9842 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9842 ; accuracy: 0.7433333333333333; loss: 2.2416276931762695\n",
      "Training epoch 9843 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9843 ; accuracy: 0.7433333333333333; loss: 2.2417232990264893\n",
      "Training epoch 9844 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9844 ; accuracy: 0.7433333333333333; loss: 2.2418150901794434\n",
      "Training epoch 9845 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9845 ; accuracy: 0.7433333333333333; loss: 2.2419326305389404\n",
      "Training epoch 9846 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9846 ; accuracy: 0.7433333333333333; loss: 2.242051124572754\n",
      "Training epoch 9847 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9847 ; accuracy: 0.7433333333333333; loss: 2.242166042327881\n",
      "Training epoch 9848 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9848 ; accuracy: 0.7433333333333333; loss: 2.2422549724578857\n",
      "Training epoch 9849 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9849 ; accuracy: 0.7433333333333333; loss: 2.242335319519043\n",
      "Training epoch 9850 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9850 ; accuracy: 0.7433333333333333; loss: 2.242394208908081\n",
      "Training epoch 9851 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9851 ; accuracy: 0.7433333333333333; loss: 2.2424633502960205\n",
      "Training epoch 9852 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9852 ; accuracy: 0.7433333333333333; loss: 2.2425320148468018\n",
      "Training epoch 9853 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9853 ; accuracy: 0.7433333333333333; loss: 2.242586612701416\n",
      "Training epoch 9854 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9854 ; accuracy: 0.7433333333333333; loss: 2.242647409439087\n",
      "Training epoch 9855 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9855 ; accuracy: 0.7433333333333333; loss: 2.242701768875122\n",
      "Training epoch 9856 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9856 ; accuracy: 0.7433333333333333; loss: 2.2427330017089844\n",
      "Training epoch 9857 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9857 ; accuracy: 0.7433333333333333; loss: 2.24273419380188\n",
      "Training epoch 9858 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9858 ; accuracy: 0.7433333333333333; loss: 2.242737293243408\n",
      "Training epoch 9859 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9859 ; accuracy: 0.7433333333333333; loss: 2.242746591567993\n",
      "Training epoch 9860 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9860 ; accuracy: 0.7433333333333333; loss: 2.242739200592041\n",
      "Training epoch 9861 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9861 ; accuracy: 0.7433333333333333; loss: 2.242708206176758\n",
      "Training epoch 9862 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9862 ; accuracy: 0.7433333333333333; loss: 2.242664098739624\n",
      "Training epoch 9863 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9863 ; accuracy: 0.7433333333333333; loss: 2.24267840385437\n",
      "Training epoch 9864 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9864 ; accuracy: 0.7433333333333333; loss: 2.242703437805176\n",
      "Training epoch 9865 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9865 ; accuracy: 0.7433333333333333; loss: 2.2427332401275635\n",
      "Training epoch 9866 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9866 ; accuracy: 0.7433333333333333; loss: 2.2427685260772705\n",
      "Training epoch 9867 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9867 ; accuracy: 0.7433333333333333; loss: 2.2428171634674072\n",
      "Training epoch 9868 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9868 ; accuracy: 0.7433333333333333; loss: 2.242859363555908\n",
      "Training epoch 9869 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9869 ; accuracy: 0.7433333333333333; loss: 2.2429003715515137\n",
      "Training epoch 9870 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9870 ; accuracy: 0.7433333333333333; loss: 2.242946147918701\n",
      "Training epoch 9871 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9871 ; accuracy: 0.7433333333333333; loss: 2.2430038452148438\n",
      "Training epoch 9872 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9872 ; accuracy: 0.7433333333333333; loss: 2.2430758476257324\n",
      "Training epoch 9873 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9873 ; accuracy: 0.7433333333333333; loss: 2.2431397438049316\n",
      "Training epoch 9874 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9874 ; accuracy: 0.7433333333333333; loss: 2.243208885192871\n",
      "Training epoch 9875 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9875 ; accuracy: 0.7433333333333333; loss: 2.2432563304901123\n",
      "Training epoch 9876 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9876 ; accuracy: 0.7433333333333333; loss: 2.243300437927246\n",
      "Training epoch 9877 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9877 ; accuracy: 0.7433333333333333; loss: 2.2433009147644043\n",
      "Training epoch 9878 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9878 ; accuracy: 0.7433333333333333; loss: 2.243295907974243\n",
      "Training epoch 9879 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9879 ; accuracy: 0.7433333333333333; loss: 2.243298292160034\n",
      "Training epoch 9880 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9880 ; accuracy: 0.7433333333333333; loss: 2.243304967880249\n",
      "Training epoch 9881 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9881 ; accuracy: 0.7433333333333333; loss: 2.243274450302124\n",
      "Training epoch 9882 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9882 ; accuracy: 0.7433333333333333; loss: 2.243243932723999\n",
      "Training epoch 9883 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9883 ; accuracy: 0.7433333333333333; loss: 2.243222951889038\n",
      "Training epoch 9884 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9884 ; accuracy: 0.7433333333333333; loss: 2.2432119846343994\n",
      "Training epoch 9885 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9885 ; accuracy: 0.7433333333333333; loss: 2.24320912361145\n",
      "Training epoch 9886 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9886 ; accuracy: 0.7433333333333333; loss: 2.2432174682617188\n",
      "Training epoch 9887 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9887 ; accuracy: 0.7433333333333333; loss: 2.243250846862793\n",
      "Training epoch 9888 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9888 ; accuracy: 0.7433333333333333; loss: 2.2432892322540283\n",
      "Training epoch 9889 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9889 ; accuracy: 0.7433333333333333; loss: 2.2433300018310547\n",
      "Training epoch 9890 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9890 ; accuracy: 0.7433333333333333; loss: 2.243377447128296\n",
      "Training epoch 9891 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9891 ; accuracy: 0.7433333333333333; loss: 2.2434310913085938\n",
      "Training epoch 9892 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9892 ; accuracy: 0.7433333333333333; loss: 2.243460178375244\n",
      "Training epoch 9893 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9893 ; accuracy: 0.7433333333333333; loss: 2.243489980697632\n",
      "Training epoch 9894 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9894 ; accuracy: 0.7433333333333333; loss: 2.2435219287872314\n",
      "Training epoch 9895 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9895 ; accuracy: 0.7433333333333333; loss: 2.243546724319458\n",
      "Training epoch 9896 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9896 ; accuracy: 0.7433333333333333; loss: 2.2435669898986816\n",
      "Training epoch 9897 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9897 ; accuracy: 0.7433333333333333; loss: 2.243583917617798\n",
      "Training epoch 9898 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9898 ; accuracy: 0.7433333333333333; loss: 2.2436158657073975\n",
      "Training epoch 9899 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9899 ; accuracy: 0.7433333333333333; loss: 2.243640899658203\n",
      "Training epoch 9900 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9900 ; accuracy: 0.7433333333333333; loss: 2.243683338165283\n",
      "Training epoch 9901 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9901 ; accuracy: 0.7433333333333333; loss: 2.2437307834625244\n",
      "Training epoch 9902 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9902 ; accuracy: 0.7433333333333333; loss: 2.2437846660614014\n",
      "Training epoch 9903 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9903 ; accuracy: 0.7433333333333333; loss: 2.243826389312744\n",
      "Training epoch 9904 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9904 ; accuracy: 0.7433333333333333; loss: 2.2438645362854004\n",
      "Training epoch 9905 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9905 ; accuracy: 0.7433333333333333; loss: 2.243901252746582\n",
      "Training epoch 9906 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9906 ; accuracy: 0.7433333333333333; loss: 2.2439165115356445\n",
      "Training epoch 9907 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9907 ; accuracy: 0.7433333333333333; loss: 2.2439444065093994\n",
      "Training epoch 9908 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9908 ; accuracy: 0.7433333333333333; loss: 2.2439615726470947\n",
      "Training epoch 9909 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9909 ; accuracy: 0.7433333333333333; loss: 2.2439815998077393\n",
      "Training epoch 9910 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9910 ; accuracy: 0.7433333333333333; loss: 2.244004726409912\n",
      "Training epoch 9911 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9911 ; accuracy: 0.7433333333333333; loss: 2.244025707244873\n",
      "Training epoch 9912 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9912 ; accuracy: 0.7433333333333333; loss: 2.244049549102783\n",
      "Training epoch 9913 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9913 ; accuracy: 0.7433333333333333; loss: 2.244079113006592\n",
      "Training epoch 9914 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9914 ; accuracy: 0.7433333333333333; loss: 2.2441017627716064\n",
      "Training epoch 9915 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9915 ; accuracy: 0.7433333333333333; loss: 2.244109869003296\n",
      "Training epoch 9916 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9916 ; accuracy: 0.7433333333333333; loss: 2.2441158294677734\n",
      "Training epoch 9917 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9917 ; accuracy: 0.7433333333333333; loss: 2.244124412536621\n",
      "Training epoch 9918 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9918 ; accuracy: 0.7433333333333333; loss: 2.244117021560669\n",
      "Training epoch 9919 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9919 ; accuracy: 0.7433333333333333; loss: 2.244108200073242\n",
      "Training epoch 9920 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9920 ; accuracy: 0.7433333333333333; loss: 2.2441093921661377\n",
      "Training epoch 9921 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9921 ; accuracy: 0.7433333333333333; loss: 2.244117021560669\n",
      "Training epoch 9922 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9922 ; accuracy: 0.7433333333333333; loss: 2.2441210746765137\n",
      "Training epoch 9923 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9923 ; accuracy: 0.7433333333333333; loss: 2.2441234588623047\n",
      "Training epoch 9924 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9924 ; accuracy: 0.7433333333333333; loss: 2.244157314300537\n",
      "Training epoch 9925 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9925 ; accuracy: 0.7433333333333333; loss: 2.244194269180298\n",
      "Training epoch 9926 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9926 ; accuracy: 0.7433333333333333; loss: 2.2442216873168945\n",
      "Training epoch 9927 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9927 ; accuracy: 0.7433333333333333; loss: 2.2442641258239746\n",
      "Training epoch 9928 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9928 ; accuracy: 0.7433333333333333; loss: 2.244288921356201\n",
      "Training epoch 9929 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9929 ; accuracy: 0.7433333333333333; loss: 2.24430775642395\n",
      "Training epoch 9930 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9930 ; accuracy: 0.7433333333333333; loss: 2.244325876235962\n",
      "Training epoch 9931 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9931 ; accuracy: 0.7433333333333333; loss: 2.2443437576293945\n",
      "Training epoch 9932 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9932 ; accuracy: 0.7433333333333333; loss: 2.2443687915802\n",
      "Training epoch 9933 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9933 ; accuracy: 0.7433333333333333; loss: 2.2443854808807373\n",
      "Training epoch 9934 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9934 ; accuracy: 0.7433333333333333; loss: 2.244415044784546\n",
      "Training epoch 9935 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9935 ; accuracy: 0.7433333333333333; loss: 2.244448661804199\n",
      "Training epoch 9936 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9936 ; accuracy: 0.7433333333333333; loss: 2.244466543197632\n",
      "Training epoch 9937 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9937 ; accuracy: 0.7433333333333333; loss: 2.244457721710205\n",
      "Training epoch 9938 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9938 ; accuracy: 0.7433333333333333; loss: 2.2444405555725098\n",
      "Training epoch 9939 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9939 ; accuracy: 0.7433333333333333; loss: 2.2444217205047607\n",
      "Training epoch 9940 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9940 ; accuracy: 0.7433333333333333; loss: 2.2444021701812744\n",
      "Training epoch 9941 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9941 ; accuracy: 0.7433333333333333; loss: 2.2444241046905518\n",
      "Training epoch 9942 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9942 ; accuracy: 0.7433333333333333; loss: 2.2444305419921875\n",
      "Training epoch 9943 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9943 ; accuracy: 0.7433333333333333; loss: 2.244446277618408\n",
      "Training epoch 9944 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9944 ; accuracy: 0.7433333333333333; loss: 2.2444612979888916\n",
      "Training epoch 9945 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9945 ; accuracy: 0.7433333333333333; loss: 2.244460105895996\n",
      "Training epoch 9946 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9946 ; accuracy: 0.7433333333333333; loss: 2.244471311569214\n",
      "Training epoch 9947 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9947 ; accuracy: 0.7433333333333333; loss: 2.2444818019866943\n",
      "Training epoch 9948 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9948 ; accuracy: 0.7433333333333333; loss: 2.244495391845703\n",
      "Training epoch 9949 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9949 ; accuracy: 0.7433333333333333; loss: 2.244502067565918\n",
      "Training epoch 9950 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9950 ; accuracy: 0.7433333333333333; loss: 2.244499921798706\n",
      "Training epoch 9951 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9951 ; accuracy: 0.7433333333333333; loss: 2.2445037364959717\n",
      "Training epoch 9952 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9952 ; accuracy: 0.7433333333333333; loss: 2.244506597518921\n",
      "Training epoch 9953 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9953 ; accuracy: 0.7433333333333333; loss: 2.244506597518921\n",
      "Training epoch 9954 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9954 ; accuracy: 0.7433333333333333; loss: 2.2444980144500732\n",
      "Training epoch 9955 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9955 ; accuracy: 0.7433333333333333; loss: 2.244493007659912\n",
      "Training epoch 9956 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9956 ; accuracy: 0.7433333333333333; loss: 2.2444825172424316\n",
      "Training epoch 9957 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9957 ; accuracy: 0.7433333333333333; loss: 2.244502067565918\n",
      "Training epoch 9958 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9958 ; accuracy: 0.7433333333333333; loss: 2.2445201873779297\n",
      "Training epoch 9959 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9959 ; accuracy: 0.7433333333333333; loss: 2.2445268630981445\n",
      "Training epoch 9960 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9960 ; accuracy: 0.7433333333333333; loss: 2.2445309162139893\n",
      "Training epoch 9961 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9961 ; accuracy: 0.7433333333333333; loss: 2.2445361614227295\n",
      "Training epoch 9962 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9962 ; accuracy: 0.7433333333333333; loss: 2.2445576190948486\n",
      "Training epoch 9963 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9963 ; accuracy: 0.7433333333333333; loss: 2.2445905208587646\n",
      "Training epoch 9964 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9964 ; accuracy: 0.7433333333333333; loss: 2.244633674621582\n",
      "Training epoch 9965 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9965 ; accuracy: 0.7433333333333333; loss: 2.2446982860565186\n",
      "Training epoch 9966 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9966 ; accuracy: 0.7433333333333333; loss: 2.2447593212127686\n",
      "Training epoch 9967 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9967 ; accuracy: 0.7433333333333333; loss: 2.2448222637176514\n",
      "Training epoch 9968 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9968 ; accuracy: 0.7433333333333333; loss: 2.2448930740356445\n",
      "Training epoch 9969 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9969 ; accuracy: 0.7433333333333333; loss: 2.2449443340301514\n",
      "Training epoch 9970 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9970 ; accuracy: 0.7433333333333333; loss: 2.244971513748169\n",
      "Training epoch 9971 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9971 ; accuracy: 0.7433333333333333; loss: 2.2450110912323\n",
      "Training epoch 9972 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9972 ; accuracy: 0.7433333333333333; loss: 2.2450647354125977\n",
      "Training epoch 9973 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9973 ; accuracy: 0.7433333333333333; loss: 2.2451252937316895\n",
      "Training epoch 9974 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9974 ; accuracy: 0.7433333333333333; loss: 2.2451837062835693\n",
      "Training epoch 9975 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9975 ; accuracy: 0.7433333333333333; loss: 2.2452306747436523\n",
      "Training epoch 9976 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9976 ; accuracy: 0.7433333333333333; loss: 2.245288610458374\n",
      "Training epoch 9977 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9977 ; accuracy: 0.7433333333333333; loss: 2.2453465461730957\n",
      "Training epoch 9978 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9978 ; accuracy: 0.7433333333333333; loss: 2.245410680770874\n",
      "Training epoch 9979 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9979 ; accuracy: 0.7433333333333333; loss: 2.2454800605773926\n",
      "Training epoch 9980 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9980 ; accuracy: 0.7433333333333333; loss: 2.2455527782440186\n",
      "Training epoch 9981 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9981 ; accuracy: 0.7433333333333333; loss: 2.2456185817718506\n",
      "Training epoch 9982 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9982 ; accuracy: 0.7433333333333333; loss: 2.245678186416626\n",
      "Training epoch 9983 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9983 ; accuracy: 0.7433333333333333; loss: 2.2457449436187744\n",
      "Training epoch 9984 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9984 ; accuracy: 0.7433333333333333; loss: 2.245797872543335\n",
      "Training epoch 9985 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9985 ; accuracy: 0.7433333333333333; loss: 2.2458693981170654\n",
      "Training epoch 9986 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9986 ; accuracy: 0.7433333333333333; loss: 2.2459418773651123\n",
      "Training epoch 9987 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9987 ; accuracy: 0.7433333333333333; loss: 2.246013879776001\n",
      "Training epoch 9988 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9988 ; accuracy: 0.7433333333333333; loss: 2.246079921722412\n",
      "Training epoch 9989 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9989 ; accuracy: 0.7433333333333333; loss: 2.2461652755737305\n",
      "Training epoch 9990 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9990 ; accuracy: 0.7433333333333333; loss: 2.2462406158447266\n",
      "Training epoch 9991 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9991 ; accuracy: 0.7433333333333333; loss: 2.246314764022827\n",
      "Training epoch 9992 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9992 ; accuracy: 0.7433333333333333; loss: 2.2463886737823486\n",
      "Training epoch 9993 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9993 ; accuracy: 0.7433333333333333; loss: 2.246467351913452\n",
      "Training epoch 9994 ; accuracy: 0.9; loss: 0.19459103047847748\n",
      "Validation epoch 9994 ; accuracy: 0.7433333333333333; loss: 2.2465476989746094\n",
      "Training epoch 9995 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9995 ; accuracy: 0.7433333333333333; loss: 2.246616840362549\n",
      "Training epoch 9996 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9996 ; accuracy: 0.7433333333333333; loss: 2.246692419052124\n",
      "Training epoch 9997 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9997 ; accuracy: 0.7433333333333333; loss: 2.2467610836029053\n",
      "Training epoch 9998 ; accuracy: 0.9; loss: 0.1945910006761551\n",
      "Validation epoch 9998 ; accuracy: 0.7433333333333333; loss: 2.2468342781066895\n",
      "Training epoch 9999 ; accuracy: 0.9; loss: 0.19459101557731628\n",
      "Validation epoch 9999 ; accuracy: 0.7433333333333333; loss: 2.246901750564575\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs=10000\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_labels=labels[idx_train]\n",
    "    val_labels=labels[idx_val]\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_train],train_labels)\n",
    "    print(f\"Training epoch {epoch} ; accuracy: {accuracy(output[idx_train],train_labels)}; loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_val],val_labels)\n",
    "    print(f\"Validation epoch {epoch} ; accuracy: {accuracy(output[idx_val],val_labels)}; loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ; accuracy: 0.68; loss: 1.9731749296188354\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)\n",
    "loss=F.nll_loss(output[idx_test],test_labels)\n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ; accuracy: 0.72; loss: 0.8774661421775818\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)\n",
    "loss=F.nll_loss(output[idx_test],test_labels)\n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "366252978e52bb2df929d3934aeb3ff29dfa67e45e575a59a0b0194f7beef5a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
