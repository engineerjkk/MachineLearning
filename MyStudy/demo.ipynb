{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directory contains the a selection of the Cora dataset (www.research.whizbang.com/data).\n",
    "\n",
    "The Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:\n",
    "\t\tCase_Based\n",
    "\t\tGenetic_Algorithms\n",
    "\t\tNeural_Networks\n",
    "\t\tProbabilistic_Methods\n",
    "\t\tReinforcement_Learning\n",
    "\t\tRule_Learning\n",
    "\t\tTheory\n",
    "\n",
    "The papers were selected in a way such that in the final corpus every paper cites or is cited by atleast one other paper. There are 2708 papers in the whole corpus. \n",
    "\n",
    "After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. All words with document frequency less than 10 were removed.\n",
    "\n",
    "\n",
    "THE DIRECTORY CONTAINS TWO FILES:\n",
    "\n",
    "The .content file contains descriptions of the papers in the following format:\n",
    "\n",
    "\t\t<paper_id> <word_attributes>+ <class_label>\n",
    "\n",
    "The first entry in each line contains the unique string ID of the paper followed by binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper. Finally, the last entry in the line contains the class label of the paper.\n",
    "\n",
    "The .cites file contains the citation graph of the corpus. Each line describes a link in the following format:\n",
    "\n",
    "\t\t<ID of cited paper> <ID of citing paper>\n",
    "\n",
    "Each line contains two paper IDs. The first entry is the ID of the paper being cited and the second ID stands for the paper which contains the citation. The direction of the link is from right to left. If a line is represented by \"paper1 paper2\" then the link is \"paper2->paper1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def load_data(path, dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) #해당논문의 피쳐벡터이다.\n",
    "    labels = encode_onehot(idx_features_labels[:, -1]) #논문 아이디에 해당하는 라벨을 나타낸다.\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)#논문의 id 를 나타낸다. \n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test=load_data(path=\"./cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
       "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
       "       values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
       "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
      "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
      "       values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
      "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22358/3747784224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_adj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first."
     ]
    }
   ],
   "source": [
    "df_adj=pd.DataFrame(adj.numpy())\n",
    "df_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1423</th>\n",
       "      <th>1424</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1433 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3         4     5     6     7     8     9     ...  \\\n",
       "0      0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1      0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2      0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3      0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4      0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...    ...   ...   ...   ...       ...   ...   ...   ...   ...   ...  ...   \n",
       "2703   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2704   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2705   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2706   0.0   0.0   0.0   0.0  0.052632   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2707   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "          1423  1424      1425  1426  1427  1428  1429  1430  1431  1432  \n",
       "0     0.000000   0.0  0.000000  0.05   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.000000   0.0  0.058824  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...        ...   ...       ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2703  0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2704  0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2705  0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2706  0.052632   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2707  0.000000   0.0  0.000000  0.00   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[2708 rows x 1433 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=pd.DataFrame(features.numpy())\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     2\n",
       "1     4\n",
       "2     3\n",
       "3     3\n",
       "4     6\n",
       "...  ..\n",
       "2703  0\n",
       "2704  0\n",
       "2705  0\n",
       "2706  5\n",
       "2707  2\n",
       "\n",
       "[2708 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels=pd.DataFrame(labels.numpy())\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 3,  ..., 0, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      3\n",
       "4      4\n",
       "..   ...\n",
       "135  135\n",
       "136  136\n",
       "137  137\n",
       "138  138\n",
       "139  139\n",
       "\n",
       "[140 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_train=pd.DataFrame(idx_train.numpy())\n",
    "df_idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    200\n",
       "1    201\n",
       "2    202\n",
       "3    203\n",
       "4    204\n",
       "..   ...\n",
       "295  495\n",
       "296  496\n",
       "297  497\n",
       "298  498\n",
       "299  499\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_val=pd.DataFrame(idx_val.numpy())\n",
    "df_idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(idx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0     500\n",
       "1     501\n",
       "2     502\n",
       "3     503\n",
       "4     504\n",
       "..    ...\n",
       "995  1495\n",
       "996  1496\n",
       "997  1497\n",
       "998  1498\n",
       "999  1499\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_test=pd.DataFrame(idx_test.numpy())\n",
    "df_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])\n",
      "tensor([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
      "        256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
      "        396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
      "        410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
      "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
      "        438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
      "        452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
      "        466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479,\n",
      "        480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
      "        494, 495, 496, 497, 498, 499])\n",
      "tensor([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,  510,  511,\n",
      "         512,  513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
      "         524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,  535,\n",
      "         536,  537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
      "         548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,  559,\n",
      "         560,  561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
      "         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
      "         584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
      "         596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
      "         608,  609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
      "         620,  621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
      "         632,  633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
      "         644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
      "         656,  657,  658,  659,  660,  661,  662,  663,  664,  665,  666,  667,\n",
      "         668,  669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
      "         680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,  691,\n",
      "         692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
      "         704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
      "         716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
      "         728,  729,  730,  731,  732,  733,  734,  735,  736,  737,  738,  739,\n",
      "         740,  741,  742,  743,  744,  745,  746,  747,  748,  749,  750,  751,\n",
      "         752,  753,  754,  755,  756,  757,  758,  759,  760,  761,  762,  763,\n",
      "         764,  765,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
      "         776,  777,  778,  779,  780,  781,  782,  783,  784,  785,  786,  787,\n",
      "         788,  789,  790,  791,  792,  793,  794,  795,  796,  797,  798,  799,\n",
      "         800,  801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
      "         812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
      "         824,  825,  826,  827,  828,  829,  830,  831,  832,  833,  834,  835,\n",
      "         836,  837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
      "         848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
      "         860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,  871,\n",
      "         872,  873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "         884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
      "         896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
      "         908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
      "         920,  921,  922,  923,  924,  925,  926,  927,  928,  929,  930,  931,\n",
      "         932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
      "         944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,  955,\n",
      "         956,  957,  958,  959,  960,  961,  962,  963,  964,  965,  966,  967,\n",
      "         968,  969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
      "         980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,  991,\n",
      "         992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
      "        1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "        1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
      "        1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039,\n",
      "        1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
      "        1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
      "        1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
      "        1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087,\n",
      "        1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
      "        1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111,\n",
      "        1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
      "        1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
      "        1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "        1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
      "        1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171,\n",
      "        1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183,\n",
      "        1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
      "        1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
      "        1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219,\n",
      "        1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231,\n",
      "        1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243,\n",
      "        1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255,\n",
      "        1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267,\n",
      "        1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,\n",
      "        1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291,\n",
      "        1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "        1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315,\n",
      "        1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327,\n",
      "        1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339,\n",
      "        1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351,\n",
      "        1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363,\n",
      "        1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375,\n",
      "        1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387,\n",
      "        1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399,\n",
      "        1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411,\n",
      "        1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423,\n",
      "        1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "        1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
      "        1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
      "        1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
      "        1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483,\n",
      "        1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495,\n",
      "        1496, 1497, 1498, 1499])\n"
     ]
    }
   ],
   "source": [
    "print(idx_train)\n",
    "print(idx_val)\n",
    "print(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GCNN import NodeClassificationGCNN\n",
    "\n",
    "model = NodeClassificationGCNN(features.shape[1], 256, np.max(labels.detach().numpy())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1] #feature의 column수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(labels.detach().numpy())+1 #라벨수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,label):\n",
    "    oneHotCodded = out.max(1)[1].type_as(label)\n",
    "    return oneHotCodded.eq(label).double().sum()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ; accuracy: 0.1357142857142857; loss: 1.9460259675979614\n",
      "Validation epoch 0 ; accuracy: 0.6033333333333334; loss: 1.9014486074447632\n",
      "Training epoch 1 ; accuracy: 0.7142857142857143; loss: 1.8813461065292358\n",
      "Validation epoch 1 ; accuracy: 0.63; loss: 1.8361010551452637\n",
      "Training epoch 2 ; accuracy: 0.7642857142857142; loss: 1.7954338788986206\n",
      "Validation epoch 2 ; accuracy: 0.6666666666666666; loss: 1.7527246475219727\n",
      "Training epoch 3 ; accuracy: 0.7714285714285715; loss: 1.6912522315979004\n",
      "Validation epoch 3 ; accuracy: 0.68; loss: 1.663841724395752\n",
      "Training epoch 4 ; accuracy: 0.7928571428571428; loss: 1.5734896659851074\n",
      "Validation epoch 4 ; accuracy: 0.7033333333333334; loss: 1.5721203088760376\n",
      "Training epoch 5 ; accuracy: 0.8142857142857143; loss: 1.4523252248764038\n",
      "Validation epoch 5 ; accuracy: 0.7166666666666667; loss: 1.4773861169815063\n",
      "Training epoch 6 ; accuracy: 0.8428571428571429; loss: 1.325679063796997\n",
      "Validation epoch 6 ; accuracy: 0.72; loss: 1.3813127279281616\n",
      "Training epoch 7 ; accuracy: 0.8571428571428571; loss: 1.205795407295227\n",
      "Validation epoch 7 ; accuracy: 0.7333333333333333; loss: 1.286211609840393\n",
      "Training epoch 8 ; accuracy: 0.8642857142857143; loss: 1.081895351409912\n",
      "Validation epoch 8 ; accuracy: 0.76; loss: 1.1943285465240479\n",
      "Training epoch 9 ; accuracy: 0.8785714285714286; loss: 0.9563564658164978\n",
      "Validation epoch 9 ; accuracy: 0.7666666666666667; loss: 1.1071412563323975\n",
      "Training epoch 10 ; accuracy: 0.9; loss: 0.8459022045135498\n",
      "Validation epoch 10 ; accuracy: 0.78; loss: 1.0256807804107666\n",
      "Training epoch 11 ; accuracy: 0.9214285714285714; loss: 0.737967848777771\n",
      "Validation epoch 11 ; accuracy: 0.79; loss: 0.9506978392601013\n",
      "Training epoch 12 ; accuracy: 0.9642857142857143; loss: 0.6451830267906189\n",
      "Validation epoch 12 ; accuracy: 0.8166666666666667; loss: 0.8828777074813843\n",
      "Training epoch 13 ; accuracy: 0.9571428571428572; loss: 0.559817373752594\n",
      "Validation epoch 13 ; accuracy: 0.8266666666666667; loss: 0.8227068185806274\n",
      "Training epoch 14 ; accuracy: 0.9571428571428572; loss: 0.48038727045059204\n",
      "Validation epoch 14 ; accuracy: 0.8266666666666667; loss: 0.7703273892402649\n",
      "Training epoch 15 ; accuracy: 0.9642857142857143; loss: 0.41601690649986267\n",
      "Validation epoch 15 ; accuracy: 0.8266666666666667; loss: 0.7257176637649536\n",
      "Training epoch 16 ; accuracy: 0.9642857142857143; loss: 0.35504525899887085\n",
      "Validation epoch 16 ; accuracy: 0.8266666666666667; loss: 0.6887141466140747\n",
      "Training epoch 17 ; accuracy: 0.9714285714285714; loss: 0.31299450993537903\n",
      "Validation epoch 17 ; accuracy: 0.8166666666666667; loss: 0.6588507890701294\n",
      "Training epoch 18 ; accuracy: 0.9714285714285714; loss: 0.2687291204929352\n",
      "Validation epoch 18 ; accuracy: 0.82; loss: 0.6353627443313599\n",
      "Training epoch 19 ; accuracy: 0.9714285714285714; loss: 0.22840750217437744\n",
      "Validation epoch 19 ; accuracy: 0.82; loss: 0.6174148321151733\n",
      "Training epoch 20 ; accuracy: 0.9714285714285714; loss: 0.202955424785614\n",
      "Validation epoch 20 ; accuracy: 0.81; loss: 0.6042618751525879\n",
      "Training epoch 21 ; accuracy: 0.9785714285714285; loss: 0.1731550395488739\n",
      "Validation epoch 21 ; accuracy: 0.8133333333333334; loss: 0.5951313972473145\n",
      "Training epoch 22 ; accuracy: 0.9785714285714285; loss: 0.15396739542484283\n",
      "Validation epoch 22 ; accuracy: 0.8166666666666667; loss: 0.5895155072212219\n",
      "Training epoch 23 ; accuracy: 0.9785714285714285; loss: 0.13367876410484314\n",
      "Validation epoch 23 ; accuracy: 0.8166666666666667; loss: 0.5868353843688965\n",
      "Training epoch 24 ; accuracy: 0.9857142857142858; loss: 0.11781322956085205\n",
      "Validation epoch 24 ; accuracy: 0.8166666666666667; loss: 0.5865809917449951\n",
      "Training epoch 25 ; accuracy: 0.9857142857142858; loss: 0.10377505421638489\n",
      "Validation epoch 25 ; accuracy: 0.8166666666666667; loss: 0.5885685682296753\n",
      "Training epoch 26 ; accuracy: 0.9857142857142858; loss: 0.0914544016122818\n",
      "Validation epoch 26 ; accuracy: 0.8166666666666667; loss: 0.5920776128768921\n",
      "Training epoch 27 ; accuracy: 0.9857142857142858; loss: 0.08296477794647217\n",
      "Validation epoch 27 ; accuracy: 0.8266666666666667; loss: 0.5971744060516357\n",
      "Training epoch 28 ; accuracy: 0.9857142857142858; loss: 0.07185371220111847\n",
      "Validation epoch 28 ; accuracy: 0.8266666666666667; loss: 0.6032967567443848\n",
      "Training epoch 29 ; accuracy: 0.9857142857142858; loss: 0.06626863777637482\n",
      "Validation epoch 29 ; accuracy: 0.83; loss: 0.6101140379905701\n",
      "Training epoch 30 ; accuracy: 0.9857142857142858; loss: 0.058111149817705154\n",
      "Validation epoch 30 ; accuracy: 0.83; loss: 0.6168203949928284\n",
      "Training epoch 31 ; accuracy: 0.9928571428571429; loss: 0.051844943314790726\n",
      "Validation epoch 31 ; accuracy: 0.8333333333333334; loss: 0.6238356232643127\n",
      "Training epoch 32 ; accuracy: 0.9928571428571429; loss: 0.045853883028030396\n",
      "Validation epoch 32 ; accuracy: 0.8333333333333334; loss: 0.6311051249504089\n",
      "Training epoch 33 ; accuracy: 0.9928571428571429; loss: 0.04402727261185646\n",
      "Validation epoch 33 ; accuracy: 0.8333333333333334; loss: 0.6381310224533081\n",
      "Training epoch 34 ; accuracy: 1.0; loss: 0.036468807607889175\n",
      "Validation epoch 34 ; accuracy: 0.8366666666666667; loss: 0.6446784734725952\n",
      "Training epoch 35 ; accuracy: 1.0; loss: 0.032769590616226196\n",
      "Validation epoch 35 ; accuracy: 0.8333333333333334; loss: 0.6502912640571594\n",
      "Training epoch 36 ; accuracy: 1.0; loss: 0.03069901093840599\n",
      "Validation epoch 36 ; accuracy: 0.8333333333333334; loss: 0.6546212434768677\n",
      "Training epoch 37 ; accuracy: 1.0; loss: 0.02664051577448845\n",
      "Validation epoch 37 ; accuracy: 0.8333333333333334; loss: 0.6586357355117798\n",
      "Training epoch 38 ; accuracy: 1.0; loss: 0.02727729268372059\n",
      "Validation epoch 38 ; accuracy: 0.83; loss: 0.6617212891578674\n",
      "Training epoch 39 ; accuracy: 1.0; loss: 0.02150690369307995\n",
      "Validation epoch 39 ; accuracy: 0.8366666666666667; loss: 0.6647783517837524\n",
      "Training epoch 40 ; accuracy: 1.0; loss: 0.021466247737407684\n",
      "Validation epoch 40 ; accuracy: 0.8366666666666667; loss: 0.668034553527832\n",
      "Training epoch 41 ; accuracy: 1.0; loss: 0.020899349823594093\n",
      "Validation epoch 41 ; accuracy: 0.8366666666666667; loss: 0.6719110012054443\n",
      "Training epoch 42 ; accuracy: 1.0; loss: 0.017162758857011795\n",
      "Validation epoch 42 ; accuracy: 0.8366666666666667; loss: 0.675400972366333\n",
      "Training epoch 43 ; accuracy: 1.0; loss: 0.01637868955731392\n",
      "Validation epoch 43 ; accuracy: 0.8366666666666667; loss: 0.6791223883628845\n",
      "Training epoch 44 ; accuracy: 1.0; loss: 0.016177315264940262\n",
      "Validation epoch 44 ; accuracy: 0.8333333333333334; loss: 0.6834142208099365\n",
      "Training epoch 45 ; accuracy: 1.0; loss: 0.014228000305593014\n",
      "Validation epoch 45 ; accuracy: 0.8333333333333334; loss: 0.6881374716758728\n",
      "Training epoch 46 ; accuracy: 1.0; loss: 0.014531607739627361\n",
      "Validation epoch 46 ; accuracy: 0.83; loss: 0.6930396556854248\n",
      "Training epoch 47 ; accuracy: 1.0; loss: 0.01293090358376503\n",
      "Validation epoch 47 ; accuracy: 0.83; loss: 0.6976518034934998\n",
      "Training epoch 48 ; accuracy: 1.0; loss: 0.01184343732893467\n",
      "Validation epoch 48 ; accuracy: 0.83; loss: 0.702466607093811\n",
      "Training epoch 49 ; accuracy: 1.0; loss: 0.011595296673476696\n",
      "Validation epoch 49 ; accuracy: 0.83; loss: 0.7073429226875305\n",
      "Training epoch 50 ; accuracy: 1.0; loss: 0.010000033304095268\n",
      "Validation epoch 50 ; accuracy: 0.8333333333333334; loss: 0.7122706770896912\n",
      "Training epoch 51 ; accuracy: 1.0; loss: 0.00974592100828886\n",
      "Validation epoch 51 ; accuracy: 0.8333333333333334; loss: 0.7168241143226624\n",
      "Training epoch 52 ; accuracy: 1.0; loss: 0.008510342799127102\n",
      "Validation epoch 52 ; accuracy: 0.8333333333333334; loss: 0.7211682796478271\n",
      "Training epoch 53 ; accuracy: 1.0; loss: 0.009678727015852928\n",
      "Validation epoch 53 ; accuracy: 0.8333333333333334; loss: 0.7254819273948669\n",
      "Training epoch 54 ; accuracy: 1.0; loss: 0.00893526803702116\n",
      "Validation epoch 54 ; accuracy: 0.8333333333333334; loss: 0.7290476560592651\n",
      "Training epoch 55 ; accuracy: 1.0; loss: 0.007900328375399113\n",
      "Validation epoch 55 ; accuracy: 0.8333333333333334; loss: 0.7323930263519287\n",
      "Training epoch 56 ; accuracy: 1.0; loss: 0.007361764553934336\n",
      "Validation epoch 56 ; accuracy: 0.83; loss: 0.7352114915847778\n",
      "Training epoch 57 ; accuracy: 1.0; loss: 0.00739419786259532\n",
      "Validation epoch 57 ; accuracy: 0.83; loss: 0.7380671501159668\n",
      "Training epoch 58 ; accuracy: 1.0; loss: 0.007172608282417059\n",
      "Validation epoch 58 ; accuracy: 0.83; loss: 0.7406307458877563\n",
      "Training epoch 59 ; accuracy: 1.0; loss: 0.00656613614410162\n",
      "Validation epoch 59 ; accuracy: 0.8266666666666667; loss: 0.7432479858398438\n",
      "Training epoch 60 ; accuracy: 1.0; loss: 0.006054193712770939\n",
      "Validation epoch 60 ; accuracy: 0.8266666666666667; loss: 0.7456977367401123\n",
      "Training epoch 61 ; accuracy: 1.0; loss: 0.0061757853254675865\n",
      "Validation epoch 61 ; accuracy: 0.8266666666666667; loss: 0.7479216456413269\n",
      "Training epoch 62 ; accuracy: 1.0; loss: 0.005721846595406532\n",
      "Validation epoch 62 ; accuracy: 0.8266666666666667; loss: 0.7500872015953064\n",
      "Training epoch 63 ; accuracy: 1.0; loss: 0.005839600693434477\n",
      "Validation epoch 63 ; accuracy: 0.8266666666666667; loss: 0.7522262334823608\n",
      "Training epoch 64 ; accuracy: 1.0; loss: 0.005657479632645845\n",
      "Validation epoch 64 ; accuracy: 0.8266666666666667; loss: 0.7542532086372375\n",
      "Training epoch 65 ; accuracy: 1.0; loss: 0.005573124624788761\n",
      "Validation epoch 65 ; accuracy: 0.8266666666666667; loss: 0.7559632658958435\n",
      "Training epoch 66 ; accuracy: 1.0; loss: 0.005403734743595123\n",
      "Validation epoch 66 ; accuracy: 0.8266666666666667; loss: 0.7571951746940613\n",
      "Training epoch 67 ; accuracy: 1.0; loss: 0.004859170410782099\n",
      "Validation epoch 67 ; accuracy: 0.8266666666666667; loss: 0.7583112120628357\n",
      "Training epoch 68 ; accuracy: 1.0; loss: 0.0046909768134355545\n",
      "Validation epoch 68 ; accuracy: 0.8266666666666667; loss: 0.7597234845161438\n",
      "Training epoch 69 ; accuracy: 1.0; loss: 0.005070287734270096\n",
      "Validation epoch 69 ; accuracy: 0.8266666666666667; loss: 0.7610026597976685\n",
      "Training epoch 70 ; accuracy: 1.0; loss: 0.0044358884915709496\n",
      "Validation epoch 70 ; accuracy: 0.8266666666666667; loss: 0.7621227502822876\n",
      "Training epoch 71 ; accuracy: 1.0; loss: 0.004473024979233742\n",
      "Validation epoch 71 ; accuracy: 0.8266666666666667; loss: 0.7631315588951111\n",
      "Training epoch 72 ; accuracy: 1.0; loss: 0.0047606900334358215\n",
      "Validation epoch 72 ; accuracy: 0.8266666666666667; loss: 0.7642952799797058\n",
      "Training epoch 73 ; accuracy: 1.0; loss: 0.004475676920264959\n",
      "Validation epoch 73 ; accuracy: 0.8233333333333334; loss: 0.7656980156898499\n",
      "Training epoch 74 ; accuracy: 1.0; loss: 0.004195111338049173\n",
      "Validation epoch 74 ; accuracy: 0.8233333333333334; loss: 0.7673481106758118\n",
      "Training epoch 75 ; accuracy: 1.0; loss: 0.0037612570449709892\n",
      "Validation epoch 75 ; accuracy: 0.8233333333333334; loss: 0.7689989805221558\n",
      "Training epoch 76 ; accuracy: 1.0; loss: 0.004046057350933552\n",
      "Validation epoch 76 ; accuracy: 0.8233333333333334; loss: 0.7706596851348877\n",
      "Training epoch 77 ; accuracy: 1.0; loss: 0.004376125056296587\n",
      "Validation epoch 77 ; accuracy: 0.8233333333333334; loss: 0.7726320028305054\n",
      "Training epoch 78 ; accuracy: 1.0; loss: 0.003941484726965427\n",
      "Validation epoch 78 ; accuracy: 0.8233333333333334; loss: 0.7744940519332886\n",
      "Training epoch 79 ; accuracy: 1.0; loss: 0.003545662621036172\n",
      "Validation epoch 79 ; accuracy: 0.8233333333333334; loss: 0.7762390971183777\n",
      "Training epoch 80 ; accuracy: 1.0; loss: 0.0036910567432641983\n",
      "Validation epoch 80 ; accuracy: 0.8233333333333334; loss: 0.7777655720710754\n",
      "Training epoch 81 ; accuracy: 1.0; loss: 0.003451843513175845\n",
      "Validation epoch 81 ; accuracy: 0.8233333333333334; loss: 0.7794697284698486\n",
      "Training epoch 82 ; accuracy: 1.0; loss: 0.0038552863989025354\n",
      "Validation epoch 82 ; accuracy: 0.8233333333333334; loss: 0.781125009059906\n",
      "Training epoch 83 ; accuracy: 1.0; loss: 0.0036167888902127743\n",
      "Validation epoch 83 ; accuracy: 0.8233333333333334; loss: 0.7826989889144897\n",
      "Training epoch 84 ; accuracy: 1.0; loss: 0.003508006688207388\n",
      "Validation epoch 84 ; accuracy: 0.8233333333333334; loss: 0.7841905951499939\n",
      "Training epoch 85 ; accuracy: 1.0; loss: 0.0032318250741809607\n",
      "Validation epoch 85 ; accuracy: 0.8233333333333334; loss: 0.7857199311256409\n",
      "Training epoch 86 ; accuracy: 1.0; loss: 0.003258816199377179\n",
      "Validation epoch 86 ; accuracy: 0.8233333333333334; loss: 0.7872434258460999\n",
      "Training epoch 87 ; accuracy: 1.0; loss: 0.0030680187046527863\n",
      "Validation epoch 87 ; accuracy: 0.8233333333333334; loss: 0.7885985970497131\n",
      "Training epoch 88 ; accuracy: 1.0; loss: 0.00300807342864573\n",
      "Validation epoch 88 ; accuracy: 0.8233333333333334; loss: 0.7899917960166931\n",
      "Training epoch 89 ; accuracy: 1.0; loss: 0.003054940141737461\n",
      "Validation epoch 89 ; accuracy: 0.8233333333333334; loss: 0.791439950466156\n",
      "Training epoch 90 ; accuracy: 1.0; loss: 0.0029408789705485106\n",
      "Validation epoch 90 ; accuracy: 0.8266666666666667; loss: 0.7925571799278259\n",
      "Training epoch 91 ; accuracy: 1.0; loss: 0.003100699046626687\n",
      "Validation epoch 91 ; accuracy: 0.8266666666666667; loss: 0.7937960624694824\n",
      "Training epoch 92 ; accuracy: 1.0; loss: 0.0031617048662155867\n",
      "Validation epoch 92 ; accuracy: 0.8266666666666667; loss: 0.7949632406234741\n",
      "Training epoch 93 ; accuracy: 1.0; loss: 0.0029168713372200727\n",
      "Validation epoch 93 ; accuracy: 0.8266666666666667; loss: 0.7963252067565918\n",
      "Training epoch 94 ; accuracy: 1.0; loss: 0.0029328481759876013\n",
      "Validation epoch 94 ; accuracy: 0.8233333333333334; loss: 0.7976382374763489\n",
      "Training epoch 95 ; accuracy: 1.0; loss: 0.0030597711447626352\n",
      "Validation epoch 95 ; accuracy: 0.8233333333333334; loss: 0.7990686297416687\n",
      "Training epoch 96 ; accuracy: 1.0; loss: 0.0028022192418575287\n",
      "Validation epoch 96 ; accuracy: 0.8233333333333334; loss: 0.8004661202430725\n",
      "Training epoch 97 ; accuracy: 1.0; loss: 0.0026565897278487682\n",
      "Validation epoch 97 ; accuracy: 0.8233333333333334; loss: 0.8019102215766907\n",
      "Training epoch 98 ; accuracy: 1.0; loss: 0.0025460179895162582\n",
      "Validation epoch 98 ; accuracy: 0.8233333333333334; loss: 0.8031593561172485\n",
      "Training epoch 99 ; accuracy: 1.0; loss: 0.0028568552806973457\n",
      "Validation epoch 99 ; accuracy: 0.8233333333333334; loss: 0.8043340444564819\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs=100\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_labels=labels[idx_train]\n",
    "    val_labels=labels[idx_val]\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_train],train_labels)\n",
    "    print(f\"Training epoch {epoch} ; accuracy: {accuracy(output[idx_train],train_labels)}; loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_val],val_labels)\n",
    "    print(f\"Validation epoch {epoch} ; accuracy: {accuracy(output[idx_val],val_labels)}; loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 3,  ..., 0, 5, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # 2708개의 라벨이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 넣었을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ; accuracy: 0.8; loss: 0.6519570350646973\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)#모델에 피쳐와 인접행렬을 넣게되면 아웃풋이 나오게된다. \n",
    "loss=F.nll_loss(output[idx_test],test_labels)#모델에서 나온 테스트 아웃풋과 실제 라벨과 비교를 수행한다. \n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 7])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[idx_test].shape#1000개의 클래스의 아웃풋이 소프트맥스로 나오기때문에 7개의 컬럼이 생기게된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_idx_test=output[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.756758</td>\n",
       "      <td>-4.771767</td>\n",
       "      <td>-3.789002</td>\n",
       "      <td>-0.712534</td>\n",
       "      <td>-5.772954</td>\n",
       "      <td>-5.772954</td>\n",
       "      <td>-5.772954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.717550</td>\n",
       "      <td>-5.247421</td>\n",
       "      <td>-0.068866</td>\n",
       "      <td>-3.756852</td>\n",
       "      <td>-5.605240</td>\n",
       "      <td>-5.605240</td>\n",
       "      <td>-5.070476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.970730</td>\n",
       "      <td>-7.242230</td>\n",
       "      <td>-4.056234</td>\n",
       "      <td>-0.019694</td>\n",
       "      <td>-7.970730</td>\n",
       "      <td>-7.970730</td>\n",
       "      <td>-7.739547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.929094</td>\n",
       "      <td>-6.279648</td>\n",
       "      <td>-0.034319</td>\n",
       "      <td>-5.207578</td>\n",
       "      <td>-6.279852</td>\n",
       "      <td>-6.279852</td>\n",
       "      <td>-5.816270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.015274</td>\n",
       "      <td>-3.365618</td>\n",
       "      <td>-4.389576</td>\n",
       "      <td>-0.079661</td>\n",
       "      <td>-5.242544</td>\n",
       "      <td>-5.714655</td>\n",
       "      <td>-5.808599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.146374</td>\n",
       "      <td>-4.046483</td>\n",
       "      <td>-2.956736</td>\n",
       "      <td>-3.107183</td>\n",
       "      <td>-5.054242</td>\n",
       "      <td>-4.839193</td>\n",
       "      <td>-4.870347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-10.221483</td>\n",
       "      <td>-10.902170</td>\n",
       "      <td>-10.763740</td>\n",
       "      <td>-10.902170</td>\n",
       "      <td>-8.985662</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>-10.902170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-5.738677</td>\n",
       "      <td>-5.849688</td>\n",
       "      <td>-6.298367</td>\n",
       "      <td>-5.368475</td>\n",
       "      <td>-5.364305</td>\n",
       "      <td>-0.018634</td>\n",
       "      <td>-6.742230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-3.054274</td>\n",
       "      <td>-3.415375</td>\n",
       "      <td>-3.377723</td>\n",
       "      <td>-2.154646</td>\n",
       "      <td>-1.315938</td>\n",
       "      <td>-0.764018</td>\n",
       "      <td>-3.327133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-8.191581</td>\n",
       "      <td>-6.341682</td>\n",
       "      <td>-0.003478</td>\n",
       "      <td>-8.085120</td>\n",
       "      <td>-7.471849</td>\n",
       "      <td>-8.182422</td>\n",
       "      <td>-8.191581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3         4         5          6\n",
       "0    -0.756758  -4.771767  -3.789002  -0.712534 -5.772954 -5.772954  -5.772954\n",
       "1    -3.717550  -5.247421  -0.068866  -3.756852 -5.605240 -5.605240  -5.070476\n",
       "2    -7.970730  -7.242230  -4.056234  -0.019694 -7.970730 -7.970730  -7.739547\n",
       "3    -3.929094  -6.279648  -0.034319  -5.207578 -6.279852 -6.279852  -5.816270\n",
       "4    -4.015274  -3.365618  -4.389576  -0.079661 -5.242544 -5.714655  -5.808599\n",
       "..         ...        ...        ...        ...       ...       ...        ...\n",
       "995  -0.146374  -4.046483  -2.956736  -3.107183 -5.054242 -4.839193  -4.870347\n",
       "996 -10.221483 -10.902170 -10.763740 -10.902170 -8.985662 -0.000238 -10.902170\n",
       "997  -5.738677  -5.849688  -6.298367  -5.368475 -5.364305 -0.018634  -6.742230\n",
       "998  -3.054274  -3.415375  -3.377723  -2.154646 -1.315938 -0.764018  -3.327133\n",
       "999  -8.191581  -6.341682  -0.003478  -8.085120 -7.471849 -8.182422  -8.191581\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_idx_test=pd.DataFrame(output_idx_test.detach().numpy())\n",
    "df_output_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_output_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    3\n",
       "1    2\n",
       "2    3\n",
       "3    2\n",
       "4    2\n",
       "..  ..\n",
       "995  0\n",
       "996  5\n",
       "997  5\n",
       "998  5\n",
       "999  2\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labels=pd.DataFrame(test_labels.numpy())\n",
    "print(type(df_test_labels))\n",
    "df_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ; accuracy: 0.793; loss: 1.221176028251648\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)\n",
    "loss=F.nll_loss(output[idx_test],test_labels)\n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "366252978e52bb2df929d3934aeb3ff29dfa67e45e575a59a0b0194f7beef5a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
